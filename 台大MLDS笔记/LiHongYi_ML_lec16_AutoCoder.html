<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 14:55 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec-16 Auto-encoder</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec-16 Auto-encoder</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org68e5381">1. Auto-encoder</a>
<ul>
<li><a href="#org6792b87">1.1. auto-encoder 给非监督学习带来的曙光</a>
<ul>
<li><a href="#org258a4f1">1.1.1. 单层神经网络实现 PCA :auto-encoder</a></li>
<li><a href="#org3137e6c">1.1.2. 多层神经网络实现 PCA:deep auto-encoder</a></li>
</ul>
</li>
<li><a href="#org1473013">1.2. auto-encoder 应用：找到【意义】</a>
<ul>
<li><a href="#org706d0aa">1.2.1. auto-encoder 文档处理</a></li>
<li><a href="#orgdc5861e">1.2.2. auto-encoder 以图找图</a></li>
<li><a href="#org8e770b1">1.2.3. auto-encoder 用来 pre-training</a></li>
</ul>
</li>
<li><a href="#org731f0a5">1.3. 预训练步骤</a>
<ul>
<li><a href="#org19cf504">1.3.1. step1: 针对第一隐含层（1000）做 pre-traning</a></li>
<li><a href="#org4130647">1.3.2. step2: 保留（fix）预训练的权重 W1</a></li>
<li><a href="#orgb0de581">1.3.3. step3: 把所有样本按照 step2 提供的转换，转换为 1000 维度的样本</a></li>
<li><a href="#org8689991">1.3.4. step4: pre-training 下一层</a></li>
<li><a href="#orgfc8bf28">1.3.5. step5: fix W2, 所有样本再转换一次</a></li>
<li><a href="#orgdc33a14">1.3.6. step6：再 pre-training 下一个 layer</a></li>
<li><a href="#orge69c963">1.3.7. 预训练的实际意义</a></li>
<li><a href="#org839556b">1.3.8. 让 auto-encoder 做得更好</a></li>
</ul>
</li>
<li><a href="#org9ca30ec">1.4. auto-encoder for CNN</a>
<ul>
<li><a href="#org26d390e">1.4.1. Unpooling 是什么</a></li>
<li><a href="#orgc6d7204">1.4.2. Deconvolution 是什么</a></li>
</ul>
</li>
<li><a href="#org5834526">1.5. auto-decoder 生成模型</a>
<ul>
<li><a href="#org771d757">1.5.1. 方法改进</a></li>
</ul>
</li>
<li><a href="#org57e612b">1.6. 还有很多 non-linear dimension reduction</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="org68e5381" class="outline-2">
<h2 id="org68e5381"><span class="section-number-2">1</span> Auto-encoder</h2>
<div class="outline-text-2" id="text-1">
<p>
outline
</p>
<ul class="org-ul">
<li>starting from PCA</li>
<li>Deep auto-encoder</li>
<li>Auto-encoder, Text Retrieval</li>
<li>Auto-encoder, Similar Image Search</li>
<li>Auto-encoder, Pre-training DNN</li>
<li>De-noising auto-encoder</li>
<li>Learning Deep Belief Network</li>
<li>Auto-encoder for CNN</li>
<li>CNN unpooling</li>
<li>CNN deconvolution</li>
<li>Decoder</li>
</ul>
</div>
<div id="org6792b87" class="outline-3">
<h3 id="org6792b87"><span class="section-number-3">1.1</span> auto-encoder 给非监督学习带来的曙光</h3>
<div class="outline-text-3" id="text-1-1">
<p>
<img src="Auto-encoder/screenshot_2017-06-13_19-12-19.png" alt="screenshot_2017-06-13_19-12-19.png" />
Encoder 这里有类似【压缩】的效果
Compact representation of the input object
但是经过 encoder 之后，我们得到的 code 并没有参照物（y）无法衡量生成的好坏，也就无法得到最优的结果
&gt;&gt;&gt;不管這些，我们先逆向，找一个 decoder 然后生成回 input image
</p>

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_19-12-35.png" alt="screenshot_2017-06-13_19-12-35.png" />
</p>
</div>

<p>
我们可以把他们接起来【一齐训练】
</p>
<p>
<img src="Auto-encoder/screenshot_2017-06-13_19-13-58.png" alt="screenshot_2017-06-13_19-13-58.png" />
这样，我们可以用【解码的结果与自己比较】的方式实现【监督学习的效果】【然而整个过程却是非监督的】
</p>
</div>
<div id="org258a4f1" class="outline-4">
<h4 id="org258a4f1"><span class="section-number-4">1.1.1</span> 单层神经网络实现 PCA :auto-encoder</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
PCA 里面其实有非常类似的概念【注】通常 NN 拿到数据就要做 normalize,nromalize 之后，mean(x~) 就是 0,
所以这里省略了 [- x~] 的部分，
</p>

<p>
lec-13 的 ppt: <a href="LiHongYi_ML_lec13_Dimreduct.html#org326005f">用 Neural Network 来表示 PCA : Autoencoder</a>
</p>

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_19-26-06.png" alt="screenshot_2017-06-13_19-26-06.png" />
</p>
</div>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_19-36-59.png" alt="screenshot_2017-06-13_19-36-59.png" />
</p>
</div>
<blockquote>
<p>
把中间的 c 看做一个 hiden-layer 的话，他是 PCA 降维的正向结果，是维度较少的。所以是 bottleneck layer, PCA 那一讲给了两种算法，一个给出【数学解】一个给出【近似解】，近似解就是用 GD
</p>
</blockquote>
</div>
</div>

<div id="org3137e6c" class="outline-4">
<h4 id="org3137e6c"><span class="section-number-4">1.1.2</span> 多层神经网络实现 PCA:deep auto-encoder</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
按照以往的经验，一层 hiden-layer 能力是不足以做好的。所以我们考虑多层 hiden-layer
</p>

<p>
Reference: Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "Reducing the
dimensionality of data with neural networks." Science 313.5786 (2006): 504-507
</p>

<p>
<img src="Auto-encoder/screenshot_2017-06-13_20-51-23.png" alt="screenshot_2017-06-13_20-51-23.png" />
需要对每一层做 RBM layer-wise 的优化
</p>

<p>
【注】按照理论说，第一层权重 W1 应该与最后一层权重互为转置，但是实际做的时候可以证明，symmetric is not necessary. 亦即不需要转置对称。
</p>

<p>
上面那篇论文给出的结果，异常的好：
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_20-58-29.png" alt="screenshot_2017-06-13_20-58-29.png" />
</p>
</div>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_20-58-40.png" alt="screenshot_2017-06-13_20-58-40.png" />
</p>
</div>

<p>
如果是用 PCA 降到 2 维做 visualization
</p>

<p>
<img src="Auto-encoder/screenshot_2017-06-13_21-01-32.png" alt="screenshot_2017-06-13_21-01-32.png" />
和用 deep-autoencoder 降到 2 维做 visualization 效果：
</p>
<p>
<img src="Auto-encoder/screenshot_2017-06-13_21-01-40.png" alt="screenshot_2017-06-13_21-01-40.png" />
明显也是 deep-encoder 比较好
</p>
</div>
</div>
</div>

<div id="org1473013" class="outline-3">
<h3 id="org1473013"><span class="section-number-3">1.2</span> auto-encoder 应用：找到【意义】</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="org706d0aa" class="outline-4">
<h4 id="org706d0aa"><span class="section-number-4">1.2.1</span> auto-encoder 文档处理</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
想做相似文章搜寻， 把一篇文章表示成一个 vector Vector Space Model, 就是很多篇文章（向量）放在一起
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-20-18.png" alt="screenshot_2017-06-13_21-20-18.png" />
</p>
</div>

<p>
怎么把文章表示成 vector 呢？ 最简单的做法就是：bag of word Bag of word 就是把某种语言【所有的常用词】都做为【一个向量】的 features这样某一篇文章所有出现的词汇都去对比【这个向量】，并在向量对应位置增加'1',如此，这篇文章就表示成了【一个向量】，每一个 dimension 就是这个单词在这篇文章中出现得次数。
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-20-37.png" alt="screenshot_2017-06-13_21-20-37.png" />
</p>
</div>

<p>
如果还想做得更好，每一个向量的 dimension 上的次数还要乘以【inverse document
frequence】，即，不仅仅用词汇的出现次数，还要乘以这个词汇的【重要性】，这个重要性就是用 inverse document frequence 来表明的.
</p>

<p>
<b>丢失的语言【意义】</b> 但是这个模型（bag of word）很弱，因为他没有考虑任何【语义－－单词之间的关系】关的东西。也就是说【所有单词都是 indenpendent】。
</p>

<p>
<b>用 auto-encoder 寻找【语】义</b> The documents talking about the same thing
<b>will have close code</b>
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-25-02.png" alt="screenshot_2017-06-13_21-25-02.png" />
</p>
</div>

<p>
开始我把每一篇文章（训练集）都通过 auto-encoder 来转换成一个 2 维度向量整个训练集 visualization 之后，会散开成海星状（效果很好）。然后来了一个新的
query,我也用同样的 auto-encoder 去把他转换成一个 2 维度向量。按照 vector
space model 就可以得出谁是 query 的相似文章。
</p>

<p>
<b>auto-encoder 的效果，远远好于 LSA</b>
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-29-51.png" alt="screenshot_2017-06-13_21-29-51.png" />
</p>
</div>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-30-10.png" alt="screenshot_2017-06-13_21-30-10.png" />
</p>
</div>
</div>
</div>
<div id="orgdc5861e" class="outline-4">
<h4 id="orgdc5861e"><span class="section-number-4">1.2.2</span> auto-encoder 以图找图</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
普通的做法是：计算像素之间的相似度(用欧式距离代表相似度）。
Retrieved using Euclidean distance in pixel intensity space
如果仅仅如此做，效果很搞笑：
</p>

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-33-52.png" alt="screenshot_2017-06-13_21-33-52.png" />
</p>
</div>
<pre class="example">
   丢失的图片【意义】
</pre>
<p>
<b>用 auto-encoder 寻找【图】义</b>
<b>The documents talking about the same thing will have close code</b>
</p>

<p>
 还是用 auto-encoder,而且因为 auto-encoder 是非监督的，不需要 label
感觉跟【encoder-&gt;语义】的感觉是一样的，图片也有某种【图义】。
(自带 decoder,用自己学自己，reconstruction error),
所以你就可以写个爬虫随便从网上 download 一堆图片，然后丢进去让他学就可以了。
</p>

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-39-51.png" alt="screenshot_2017-06-13_21-39-51.png" />
</p>
</div>

<p>
如果不是用 pixel 上算相似度，而是在 code 上算相似度，得到的效果好很多
</p>

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-41-35.png" alt="screenshot_2017-06-13_21-41-35.png" />
</p>
</div>

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-13_21-41-45.png" alt="screenshot_2017-06-13_21-41-45.png" />
</p>
</div>
</div>
</div>
<div id="org8e770b1" class="outline-4">
<h4 id="org8e770b1"><span class="section-number-4">1.2.3</span> auto-encoder 用来 pre-training</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
训练深度神经网络的时候，经常面对的一个问题是 parameter initialization, 不同的初始值会对结果产生很大的影响
</p>

<pre class="example">
&gt;&gt;&gt; 什么是预训练？
------------------------------
pre-training :找一组好的参数初始值
------------------------------
</pre>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_09-59-22.png" alt="screenshot_2017-06-14_09-59-22.png" />
</p>
</div>

<blockquote>
<p>
这是我设计的一个 Network structure,input 是 28*28=784 的 image 输出是一个 10
维度向量，每一位都是 image 属于这个数字的概率
</p>
</blockquote>
<p>
下面我要做 pre-training 获得较合理的 parameter initial value每次预训练都是一个 auto-encoder ,层层预训练，层层推进
</p>
</div>
</div>
</div>
<div id="org731f0a5" class="outline-3">
<h3 id="org731f0a5"><span class="section-number-3">1.3</span> 预训练步骤</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="org19cf504" class="outline-4">
<h4 id="org19cf504"><span class="section-number-4">1.3.1</span> step1: 针对第一隐含层（1000）做 pre-traning</h4>
<div class="outline-text-4" id="text-1-3-1">

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_10-03-13.png" alt="screenshot_2017-06-14_10-03-13.png" />
</p>
</div>

<p>
注意这个待训练的目标层是不是比输出层维度大，如果大，需要强 regular
</p>
<blockquote>
<p>
注意，这里是 layer-wise 的 pre-training,也就是针对每一层都需要做一个
auto-encoder 但是在做这个的时候，要注意，我们一般的 auto-encoder 的
hiden-layer是要比 input 小，但是这里做 <span class="underline">layer-wise</span> 不能保证每一层都比前一层要小， 所以需要很强的 regularization, <span class="underline">很强的 regular</span> (比如 L1-regular) 可以让这个较高维度的hiden-layer(比如图中的 1000) 是比较 <span class="underline">sparse</span> 的，大概保证能学点东西，否则auto-encoder 可能什么都学不到，原封不动输出 input(图中 784 维度),
这是auto-encoder 的特性。
</p>
</blockquote>
</div>
</div>

<div id="org4130647" class="outline-4">
<h4 id="org4130647"><span class="section-number-4">1.3.2</span> step2: 保留（fix）预训练的权重 W1</h4>
<div class="outline-text-4" id="text-1-3-2">

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_10-10-12.png" alt="screenshot_2017-06-14_10-10-12.png" />
</p>
</div>
</div>
</div>

<div id="orgb0de581" class="outline-4">
<h4 id="orgb0de581"><span class="section-number-4">1.3.3</span> step3: 把所有样本按照 step2 提供的转换，转换为 1000 维度的样本</h4>
</div>

<div id="org8689991" class="outline-4">
<h4 id="org8689991"><span class="section-number-4">1.3.4</span> step4: pre-training 下一层</h4>
<div class="outline-text-4" id="text-1-3-4">

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_10-12-07.png" alt="screenshot_2017-06-14_10-12-07.png" />
</p>
</div>
</div>
</div>

<div id="orgfc8bf28" class="outline-4">
<h4 id="orgfc8bf28"><span class="section-number-4">1.3.5</span> step5: fix W2, 所有样本再转换一次</h4>
<div class="outline-text-4" id="text-1-3-5">

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_10-15-24.png" alt="screenshot_2017-06-14_10-15-24.png" />
</p>
</div>
</div>
</div>

<div id="orgdc33a14" class="outline-4">
<h4 id="orgdc33a14"><span class="section-number-4">1.3.6</span> step6：再 pre-training 下一个 layer</h4>
<div class="outline-text-4" id="text-1-3-6">

<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_10-15-56.png" alt="screenshot_2017-06-14_10-15-56.png" />
</p>
</div>

<p>
如此重复下去，直到最后一个隐含层的 parameter 训练好，fix 住的 W1,W2,W3 就是整个 DNN 的初始值, <span class="underline">通往 outpulayer 的权重不能预训练，需要给一个 random init</span>.
现在 W1,W2,W3,W4, 都有了，再用 backpropagation 做优化.
</p>
</div>
</div>

<div id="orge69c963" class="outline-4">
<h4 id="orge69c963"><span class="section-number-4">1.3.7</span> 预训练的实际意义</h4>
<div class="outline-text-4" id="text-1-3-7">
<p>
现在的 deep-learning 已经不需要做 pre-training 也可以做的很好，但是
</p>

<pre class="example">
&gt;&gt;&gt; 预训练的妙用
--------------------------------------------------------------
pre-traning 的妙用在于 如果有很多 unlabeled data 和少量 labeled data 你就
可以先利用 unlabelled data 预训练这个神经网络，然后利用 labeled data 对网络
做微调即可。

所以 pre-training 因为其 unsupervised 的特性，在缺少 labeled data 的时候
还是很有用的。
--------------------------------------------------------------
</pre>
</div>
</div>

<div id="org839556b" class="outline-4">
<h4 id="org839556b"><span class="section-number-4">1.3.8</span> 让 auto-encoder 做得更好</h4>
<div class="outline-text-4" id="text-1-3-8">
<pre class="example">
&gt;&gt;&gt; de-noising auto-encoder
----------------------------------------------------------------
Vincent, Pascal, et al. "Extracting and composing robust features
with denoising autoencoders." ICML, 2008.
----------------------------------------------------------------
</pre>



<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_10-29-59.png" alt="screenshot_2017-06-14_10-29-59.png" />
</p>
</div>
<ol class="org-ol">
<li>给原始训练集加入 noise</li>
<li>用 noise 训练集训练一个 auto-encoder and decoder</li>
<li>loss-fn 比较的是 decoder 结果与原始训练集的差距encoder 同时学到了【杂讯过滤】－－－ 所以叫做 de-noised auto-encoder</li>
<li>这样可以得到比较 robust 的 dimension reduction 函数</li>
</ol>
</div>
</div>
</div>
<div id="org9ca30ec" class="outline-3">
<h3 id="org9ca30ec"><span class="section-number-3">1.4</span> auto-encoder for CNN</h3>
<div class="outline-text-3" id="text-1-4">
<p>
auto-encoder 似乎长于优化，不论是之前的参数初始化，还是下面的优化 CNN
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-01-26.png" alt="screenshot_2017-06-14_11-01-26.png" />
</p>
</div>
</div>

<div id="org26d390e" class="outline-4">
<h4 id="org26d390e"><span class="section-number-4">1.4.1</span> Unpooling 是什么</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
unpooling 要记住我做 pooling 时候是从哪里取值的（比如 maxpooling 4 选 1,这个‘1’是在原来‘4’的哪个位置）记住 max locations
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-04-41.png" alt="screenshot_2017-06-14_11-04-41.png" />
</p>
</div>

<p>
unpooling 是把比较小的 matrix 变成原来的四倍，也就是按照记录的位置填好放大的
matrix ,其他补零。
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-06-53.png" alt="screenshot_2017-06-14_11-06-53.png" />
</p>
</div>

<p>
例如做完 unpooling 前后
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-07-21.png" alt="screenshot_2017-06-14_11-07-21.png" />
</p>
</div>

<p>
keras 这个机器学习框架是用另一种方法，他不记录位置，直接把小矩阵的每一个位置复制 4 份，形成大矩阵。
</p>
</div>
</div>

<div id="orgc6d7204" class="outline-4">
<h4 id="orgc6d7204"><span class="section-number-4">1.4.2</span> Deconvolution 是什么</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
actually, deconvolution is convolution deconvolution is just a padding
convolution convolution 图示是这样
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-14-54.png" alt="screenshot_2017-06-14_11-14-54.png" />
</p>
</div>

<p>
我们以为的 deconvolution 的图示是这样
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-15-20.png" alt="screenshot_2017-06-14_11-15-20.png" />
</p>
</div>

<p>
但其实，这个图示跟 padding convolution 的图示是一样的意思
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-16-24.png" alt="screenshot_2017-06-14_11-16-24.png" />
</p>
</div>

<p>
所以两者本质相同。
</p>
</div>
</div>
</div>

<div id="org5834526" class="outline-3">
<h3 id="org5834526"><span class="section-number-3">1.5</span> auto-decoder 生成模型</h3>
<div class="outline-text-3" id="text-1-5">
<pre class="example">
encoder:  image  ---  降维  --- code
decoder:  code   ---       --- image
</pre>

<p>
所以 decoder 可以用来生成图片，一个生成模型这个试验的过程是这样的，仍然是
MNIST 手写识别.
</p>

<ol class="org-ol">
<li><p>
首先 input 一大笔 image,训练 auto-encoder 和 decoder然后，把得到的 C,中间隐含 层的输出拿出来，他就是 code,我都映射到 2 维度，然后画其分布图
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-27-57.png" alt="screenshot_2017-06-14_11-27-57.png" />
</p>
</div></li>

<li><p>
然后，选取有数值分布的一块区域，均匀 sample code. 這些 code 中必然存在那些没有对应 image 的 code. 然后丢进 decoder 去生成图片
</p>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-29-59.png" alt="screenshot_2017-06-14_11-29-59.png" />
</p>
</div>

<p>
效果非常的好
</p></li>
</ol>
</div>

<div id="org771d757" class="outline-4">
<h4 id="org771d757"><span class="section-number-4">1.5.1</span> 方法改进</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
但是，真的有个任务让你去生成图片的时候，你还要先 encoder 然后visualization 然后再根据分布选取有点的位置 sampling code. 这个过程很耗时，所以，我可以用
regularization 改变 code 的点的分布（就是给整个 NN 加入 regularization 来改变降维的效果），比如 L2-regular 可以让分布比较集中并且接近 0 的附近(L2-regular
是 weight-decay 按比例缩小 weight) (L1-regular 是 按量缩小 weight,会让 code
分布较散) <span class="underline">这样之后，我不需要画图了，每次直接 sample 0 点附近的 code 点</span> <span class="underline">然后生成图片就可以了。</span>
</p>

<pre class="example">
&gt;&gt;&gt; regularization 的掌握
-----------------------------------------------------
liyongyi 老师真是有能力，我在这里肯定不敢加 regular,因为我会想
这么做会不会让模型的能力降低？以至于产生 underfitting.
-----------------------------------------------------
</pre>



<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-43-16.png" alt="screenshot_2017-06-14_11-43-16.png" />
</p>
</div>


<div class="figure">
<p><img src="Auto-encoder/screenshot_2017-06-14_11-43-32.png" alt="screenshot_2017-06-14_11-43-32.png" />
</p>
</div>
</div>
</div>
</div>

<div id="org57e612b" class="outline-3">
<h3 id="org57e612b"><span class="section-number-3">1.6</span> 还有很多 non-linear dimension reduction</h3>
<div class="outline-text-3" id="text-1-6">
<p>
RBM(这个人很多人误解，他其实不是神经网络) DBN(也不是神经网络) RBM 和 DBN 都是
graphical model, 不是神经网络 <a href="https://www.youtube.com/watch?v=Cdpfpy4bXPI">https://www.youtube.com/watch?v=Cdpfpy4bXPI</a>, 这里是介绍 graphical model 的教学视频。
</p>

<p>
Learning More
</p>
<ul class="org-ul">
<li>Restricted Boltzmann Machine
<ul class="org-ul">
<li>Neural networks [5.1] : Restricted Boltzmann machine – definition</li>
<li><a href="https://www.youtube.com/watch?v=p4Vh_zMw-HQ&amp;index=36&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">https://www.youtube.com/watch?v=p4Vh_zMw-HQ&amp;index=36&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH</a></li>
<li>Neural networks [5.2] : Restricted Boltzmann machine – inference</li>
<li><a href="https://www.youtube.com/watch?v=lekCh_i32iE&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=37">https://www.youtube.com/watch?v=lekCh_i32iE&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=37</a></li>
<li>Neural networks [5.3] : Restricted Boltzmann machine - free energy</li>
<li><a href="https://www.youtube.com/watch?v=e0Ts_7Y6hZU&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=38">https://www.youtube.com/watch?v=e0Ts_7Y6hZU&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=38</a></li>
</ul></li>
</ul>

<p>
Learning More
</p>
<ul class="org-ul">
<li>Deep Belief Network
<ul class="org-ul">
<li>Neural networks [7.7] : Deep learning - deep belief network</li>
<li><a href="https://www.youtube.com/watch?v=vkb6AWYXZ5I&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=57">https://www.youtube.com/watch?v=vkb6AWYXZ5I&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=57</a></li>
<li>Neural networks [7.8] : Deep learning - variational bound</li>
<li><a href="https://www.youtube.com/watch?v=pStDscJh2Wo&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=58">https://www.youtube.com/watch?v=pStDscJh2Wo&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=58</a></li>
<li>Neural networks [7.9] : Deep learning - DBN pre-training</li>
<li><a href="https://www.youtube.com/watch?v=35MUlYCColk&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=59">https://www.youtube.com/watch?v=35MUlYCColk&amp;list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH&amp;index=59</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-06</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 日 14:55</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
