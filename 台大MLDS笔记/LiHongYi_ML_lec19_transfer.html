<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 14:55 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec-19 Transfer Learning</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec-19 Transfer Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org366549d">1. Transfer Learning</a>
<ul>
<li><a href="#orgf2e1981">1.1. Transfer Learning 的四种可能</a></li>
</ul>
</li>
<li><a href="#org073d217">2. 如果 source 有 label, target 有 label</a>
<ul>
<li>
<ul>
<li><a href="#org4d757c8">2.0.1. Model Fine-tuning</a></li>
<li><a href="#org1c622f2">2.0.2. 技巧 1：Conservative Training</a></li>
<li><a href="#orgd40538f">2.0.3. 技巧 2：Layer Transfer</a></li>
<li><a href="#org414dd8b">2.0.4. Multitask learning</a></li>
<li><a href="#org1f8553b">2.0.5. Multitask Learning 的成功例子</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0f14d21">3. 如果 source 有 label, target 没有 label</a>
<ul>
<li>
<ul>
<li><a href="#org0a9e679">3.0.1. Domain-adversarial training</a></li>
<li><a href="#org662de03">3.0.2. 一个各怀鬼胎的神经网络</a></li>
<li><a href="#org3e5386f">3.0.3. zero-shot learning</a></li>
</ul>
</li>
<li><a href="#orge4bb65b">3.1. Attribute embedding</a>
<ul>
<li><a href="#org839440e">3.1.1. &gt;&gt;&gt; embedding tip</a></li>
<li><a href="#org01e8365">3.1.2. 借用 word-vector</a></li>
<li><a href="#orga74b24b">3.1.3. &gt;&gt;&gt; 区间控制：[类间大][类内小] tip</a></li>
<li><a href="#orgbdeef86">3.1.4. ConSE: Convex combination of semantic embedding</a></li>
<li><a href="#orgf4b0f0b">3.1.5. DeVISE</a></li>
<li><a href="#orgbadbd60">3.1.6. ConvNet. vs DeViSE. vs ConSE(10)</a></li>
<li><a href="#orga1d0e00">3.1.7. Example of zero-shot learning</a></li>
<li><a href="#orgccea602">3.1.8. More about Zero-shot learning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org9bdcb7d">4. 如果 source 没有 label</a>
<ul>
<li>
<ul>
<li><a href="#org9ca2451">4.0.1. 处理 unlabelled data 的思路</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="org366549d" class="outline-2">
<h2 id="org366549d"><span class="section-number-2">1</span> Transfer Learning</h2>
<div class="outline-text-2" id="text-1">
<p>
似乎 transfer 就是决定复用 哪些层，那些可以复用的层就叫做 transfer layer
</p>
<ul class="org-ul">
<li>Similar domain, different tasks</li>
<li>Different domains, same task</li>
</ul>

<p>
研究生该怎么做？可以参考漫画家的工作过程我们日常生活中，经常干这样的事情
</p>

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_14-20-16.png" alt="screenshot_2017-06-15_14-20-16.png" />
</p>
</div>
</div>
<div id="orgf2e1981" class="outline-3">
<h3 id="orgf2e1981"><span class="section-number-3">1.1</span> Transfer Learning 的四种可能</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>target data 是与我们的 task 直接相关</li>
<li>source data 是与我们的 task 无直接关系</li>
</ul>

<p>
[无直接关系的定义学界比较混乱，但大概可以理解一下]
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_14-26-02.png" alt="screenshot_2017-06-15_14-26-02.png" />
</p>
</div>
</div>
</div>
</div>
<div id="org073d217" class="outline-2">
<h2 id="org073d217"><span class="section-number-2">2</span> 如果 source 有 label, target 有 label</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="org4d757c8" class="outline-4">
<h4 id="org4d757c8"><span class="section-number-4">2.0.1</span> Model Fine-tuning</h4>
<div class="outline-text-4" id="text-2-0-1">
<p>
One-shot learning: only a few examples in target domain
</p>

<ul class="org-ul">
<li>Task description</li>
<li>Target data: x t , y t &#x2013;&gt; Very little</li>
<li>Source data: x s , y s &#x2013;&gt; A large amount</li>
<li>Example: (supervised) speaker adaption</li>
<li>Target data: audio data and its transcriptions of specific user</li>
<li>Source data: audio data and transcriptions from many speakers</li>
<li>Idea: training a model by source data, then fine-tune the model by target data</li>
<li>Challenge:</li>
</ul>
<p>
only limited target data, so be careful about overfitting
</p>

<p>
极端情况：One-shot learning: target data 少到只有很少的幾個就是 one-shot learning
</p>

<p>
什么叫做 fine-tune
就是用 source data train 一个模型，然后把这个模型 train 出来的参数当作初始值然后结合 target data 去 train 一个新的同样的模型。
</p>

<p>
如果 target data 少的好像 one-shot learning 一样，结果依然很差，怎么办？
</p>

<p>
有很多不同的技巧来解决这个问题：
</p>
</div>
</div>

<div id="org1c622f2" class="outline-4">
<h4 id="org1c622f2"><span class="section-number-4">2.0.2</span> 技巧 1：Conservative Training</h4>
<div class="outline-text-4" id="text-2-0-2">

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_14-40-25.png" alt="screenshot_2017-06-15_14-40-25.png" />
</p>
</div>

<pre class="example">
加一个 constrait,也就是一个 regular:新旧 model 在同一笔 data 上的结果不要太
大就可以防止 overfitting 的情形

[qqq]
-------------------------------------------------
这里为甚么是防止 overfitting 啊？
防止谁 overfitting? 是 source data 训练出来的模型
太厉害，远远超逾 target data 数量所能 hold 的模型复杂度？
-------------------------------------------------
</pre>
</div>
</div>

<div id="orgd40538f" class="outline-4">
<h4 id="orgd40538f"><span class="section-number-4">2.0.3</span> 技巧 2：Layer Transfer</h4>
<div class="outline-text-4" id="text-2-0-3">
<p>
source data train 好的 model, 从中直接 copy 一部分 layer到 target data 要训练的模型中去，然后 target data 只需要考虑那些没有被 copy 的 layer.
</p>

<p>
这样 target data 只需要考虑很少的 parameter, 所以可以避免 overfitting搞完之后呢，如果 target data 够的话，整体做一个 fine-tune
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_14-42-46.png" alt="screenshot_2017-06-15_14-42-46.png" />
</p>
</div>

<p>
但是哪些 layer 应该 copy 过去呢？
</p>

<p>
不同的 task 需要 copy 不同的 layer.
</p>

<p>
语音上，不同的人用同样的发音方式，因为口腔结构略有差异，所以得到的声音讯号是不一样的。speech 面向的 NN ,前几层是区分语者不同的发音方式，从发音方式到辨识结果是跟语者没有关系的这几层可以 copy.
</p>

<p>
图像识别上，要 copy 前面几层，因为前面几层就是 detect 有没有一些纹路，形状等等。所以这前几层是可以 被 transfer 到其他 target data 上的。因为这一部分大家是共用的。
</p>

<p>
似乎 transfer 就是决定复用 哪些层，那些可以复用的层就叫做 transfer layer
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_14-50-31.png" alt="screenshot_2017-06-15_14-50-31.png" />
</p>
</div>

<ul class="org-ul">
<li>横轴是 copy 前几个 layer</li>
<li>纵轴是 模型的误差</li>
</ul>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_14-55-24.png" alt="screenshot_2017-06-15_14-55-24.png" />
</p>
</div>

<p>
下面这张图，可以用来发现，两个不同领域的模型，哪些层做的事情是差不多的图片分类，sourcedata 是分动物，targetdata 是分家具，依次 copy前 1~7 层看看两者的差距从哪里开始拉开，这样可以验证从哪一层可以开始复用。以后用在分交通工具。
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_14-58-47.png" alt="screenshot_2017-06-15_14-58-47.png" />
</p>
</div>


<ul class="org-ul">
<li>Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson, “How transferable
are features in deep neural networks?”, NIPS, 2014</li>

<li>Fine-tune 关注的是 target-domain 做的好不好。</li>
<li>Multitask learning 同时关注 target and source domain</li>
</ul>
</div>
</div>

<div id="org414dd8b" class="outline-4">
<h4 id="org414dd8b"><span class="section-number-4">2.0.4</span> Multitask learning</h4>
<div class="outline-text-4" id="text-2-0-4">
<p>
Deep learning 特别适合做 multitask learning
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-11-34.png" alt="screenshot_2017-06-15_15-11-34.png" />
</p>
</div>

<ul class="org-ul">
<li>The multi-layer structure makes NN suitable for</li>
</ul>
<p>
multitask learning
</p>
<ol class="org-ol">
<li>共通的 feature</li>
</ol>

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-04-07.png" alt="screenshot_2017-06-15_15-04-07.png" />
</p>
</div>

<ol class="org-ol">
<li>不同的 feature</li>
</ol>

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-04-22.png" alt="screenshot_2017-06-15_15-04-22.png" />
</p>
</div>
</div>
</div>

<div id="org1f8553b" class="outline-4">
<h4 id="org1f8553b"><span class="section-number-4">2.0.5</span> Multitask Learning 的成功例子</h4>
<div class="outline-text-4" id="text-2-0-5">
<p>
不同语言下的语音辨识这个 model 可以同时辨识 5 种语言
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-05-47.png" alt="screenshot_2017-06-15_15-05-47.png" />
</p>
</div>



<p>
translation 也是一样的道理
</p>

<p>
Similar idea in translation: Daxiang Dong, Hua Wu, Wei He, Dianhai Yu and
Haifeng Wang, "Multi-task learning for multiple language translation.“, ACL 2015
</p>


<p>
经过最近几年，发现几乎所有的语言都可以互相 transfer
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-08-42.png" alt="screenshot_2017-06-15_15-08-42.png" />
</p>
</div>

<p>
Huang, Jui-Ting, et al. "Cross-language knowledge transfer using
multilingual deep neural network with shared hidden layers." ICASSP, 2013
</p>

<p>
Progressive Neural Networks
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-10-55.png" alt="screenshot_2017-06-15_15-10-55.png" />
</p>
</div>

<blockquote>
<p>
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer,
James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell,
“Progressive Neural Networks”, arXiv preprint 2016
</p>
</blockquote>
</div>
</div>
</div>

<div id="org0f14d21" class="outline-2">
<h2 id="org0f14d21"><span class="section-number-2">3</span> 如果 source 有 label, target 没有 label</h2>
<div class="outline-text-2" id="text-3">

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-11-56.png" alt="screenshot_2017-06-15_15-11-56.png" />
</p>
</div>
<ul class="org-ul">
<li>Source data: xs , ys &#x2014;&gt;  Training data</li>
<li>Target data: xt      &#x2014;&gt;  Testing data</li>
</ul>

<p>
两者非常的 mismatch
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-14-19.png" alt="screenshot_2017-06-15_15-14-19.png" />
</p>
</div>

<p>
如果我就直接用 source 去 train 一个 model 然后用到 target 上去。结果很差。通过之前的学习，我们知道 DNN 前面的几层基本就相当于一个 feature extractor 后面的几层基本就是一个 classifier
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-17-04.png" alt="screenshot_2017-06-15_15-17-04.png" />
</p>
</div>

<p>
如果把前面几层的结果拿出来 做一个 t-sne 做 visualization可以发现，target data
extract 出来的 features 跟用 source data extract 出来的 features 完全不在一个位置上。也就是说，
</p>

<blockquote>
<p>
前面的这些绿色层从 source data 和 target data 抽取的是不一样的 feature.
</p>
</blockquote>
</div>

<div id="org0a9e679" class="outline-4">
<h4 id="org0a9e679"><span class="section-number-4">3.0.1</span> Domain-adversarial training</h4>
<div class="outline-text-4" id="text-3-0-1">
<p>
原理跟 GAN 的 ‘A’ 相似: 所以我们希望，前面的 extractor 可以不那么【domain
specific】，我们希望想个办法把前面几层变的更【通用一些】，把 domain specific
的特性去掉。希望它可以 extract 之后的结果是【混在一齐】.
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-27-35.png" alt="screenshot_2017-06-15_15-27-35.png" />
</p>
</div>

<p>
从
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-23-34.png" alt="screenshot_2017-06-15_15-23-34.png" />
</p>
</div>

<p>
变成
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-23-48.png" alt="screenshot_2017-06-15_15-23-48.png" />
</p>
</div>

<p>
这里怎么做呢？加入一个 domain classifier 用来判断，抽取出的某个属性，到底是哪个 domain 的.
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-28-03.png" alt="screenshot_2017-06-15_15-28-03.png" />
</p>
</div>



<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-28-30.png" alt="screenshot_2017-06-15_15-28-30.png" />
</p>
</div>

<ul class="org-ul">
<li>feature extractor ==&gt; GAN 的 生成器</li>
<li>domain classifier ==&gt; GAN 的 鉴别器</li>
</ul>

<p>
这里要比 GAN 容易, 但是要给 feature extractor 添加一个任务：不但要【骗过】
domian classifier, 还要满足 label classifier 的要求。
</p>
</div>
</div>

<div id="org662de03" class="outline-4">
<h4 id="org662de03"><span class="section-number-4">3.0.2</span> 一个各怀鬼胎的神经网络</h4>
<div class="outline-text-4" id="text-3-0-2">

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-30-30.png" alt="screenshot_2017-06-15_15-30-30.png" />
</p>
</div>

<p>
可以发现 feature extractor 这一部分的一个目标是违反 domain classifier 的
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-32-40.png" alt="screenshot_2017-06-15_15-32-40.png" />
</p>
</div>

<ul class="org-ul">
<li>Yaroslav Ganin, Victor Lempitsky, Unsupervised Domain Adaptation by
Backpropagation, ICML, 2015</li>
<li>Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario
Marchand, Domain-Adversarial Training of Neural Networks, JMLR, 2016</li>
</ul>



<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-36-01.png" alt="screenshot_2017-06-15_15-36-01.png" />
</p>
</div>

<p>
此表 Proposed Approach 就是这里的方法。
</p>
</div>
</div>

<div id="org3e5386f" class="outline-4">
<h4 id="org3e5386f"><span class="section-number-4">3.0.3</span> zero-shot learning</h4>
<div class="outline-text-4" id="text-3-0-3">

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-36-54.png" alt="screenshot_2017-06-15_15-36-54.png" />
</p>
</div>

<p>
就是完全没有标签
</p>
<ul class="org-ul">
<li>Source data: x s , y s Training data</li>
<li>Target data: x t       Testing data</li>
</ul>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-38-43.png" alt="screenshot_2017-06-15_15-38-43.png" />
</p>
</div>

<p>
这里你让 <b>xt &#x2013;&gt; 草泥马</b>, 这个也太难了点了。
</p>

<p>
In speech recognition, we can not have all possible words in the source
(training) data. How we solve this problem in speech recognition?
</p>

<p>
但是这件事情在语音转文字是有解决方案的，英文单词每天都在产生，舶来词，网络词，等等，而這些词发音容易，但要计算机给出這些单词的拼写字幕，是非常难的。
</p>

<p>
降低分类单位, 不要直接把语音转成文字，而是把语音转成音标（phoneme）然后
phoneme 和 单词之间建立一个 table（字典）
</p>


<p>
那在草泥马这个图像识别上怎么做呢？
</p>

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-43-14.png" alt="screenshot_2017-06-15_15-43-14.png" />
</p>
</div>

<p>
phoneme &#x2014;&gt; attributes
attributes ===&gt; 毛茸茸，四肢脚，有尾巴，。。。
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_15-45-17.png" alt="screenshot_2017-06-15_15-45-17.png" />
</p>
</div>
</div>
</div>

<div id="orge4bb65b" class="outline-3">
<h3 id="orge4bb65b"><span class="section-number-3">3.1</span> Attribute embedding</h3>
<div class="outline-text-3" id="text-3-1">
<p>
如果 attribute 很复杂，可以做 embedding
</p>
</div>

<div id="org839440e" class="outline-4">
<h4 id="org839440e"><span class="section-number-4">3.1.1</span> &gt;&gt;&gt; embedding tip</h4>
<div class="outline-text-4" id="text-3-1-1">
<pre class="example">
-------------------------------------------------
看李宏毅老师是如何理解 embedding 的，embedding 也跟
regular一样被李老师【范 化】成了一个通用的工具，可
以任意的添加在某个模型里。
-------------------------------------------------
</pre>


<p>
<img src="Transfer Learning/screenshot_2017-06-15_15-54-55.png" alt="screenshot_2017-06-15_15-54-55.png" />
[勘误]: f(y3) ==&gt; f(x3)
也就是说，现在有一个 embedding 的 space, 然后把训练集数据 x 都透过一个 transform 转换成 embedding-space 的一个点，
x &#x2013;&gt; f(x)
然后把所有的 attribute 也都变成 embedding-space 上的一个点
y &#x2013;&gt; g(y)
这个 g 和 f 都可以是 NN
那么 training 的时候就希望 f(xn) ~~ g(yn) 越接近越好在做 testing  的时候就看这个点的 attribute 做 embedding 以后跟哪一个 attribute 最像，你就知道了他是什么样的 image
</p>

<p>
&gt;&gt;&gt; 学生问题这边把 attribute embedding 进去是什么意思啊？
&gt;&gt;&gt; 李老师回答
attribute 就是一个 vector 嘛，然后把这个 vector 乘以一个 transform,
然后把他丢到一个 NN 里面去。然后 NN 会 output 一个 vector 吧。也就把一个
vector 变成了另一个 low-dimension vector. 你可以想成是做【降维】的意思
&gt;&gt;&gt; 学生再问那所以就是这一个 embedding 的过程，他的 input 可以是一个 image 也可以是一个 attribute,但是要用不同的 transform,因为 image 和 attribute 是差很多的。然后我们就希望经过两个 NN 之后产生的结果，是非常接近的。
</p>

<p>
image 和 attribute 都可以描述成 vector,这里想要做的事情是把 image 和 attibute
同样的同一个空间里面， 可以理解成对 image-vector 和 attibute-vector 同时做降维降到相同的维度数。 所以把 image: x1,x2,.. 通过 f 转换到 embedding 上的点，把
attribute: y1,y2,&#x2026; 通过 g 也转换到 embedding 上的点，但是怎么找这个 f,g 呢？既然是函数是转换，我就可以用 NN. 我要做的其实就是通过两个 NN 找到 f,g 让他们在某个空间（embedding space）完成【配对】。配对就是重叠。（我发现 NN 有个能把所有不咋相关的事务‘相关化’的能力，image-vector 都是一些像素点组成的向量，attribute-vector 都是一些实体的特色组成的向量，两者在各自原来的空间中八竿子打不着，要把他们配对必须把他们拉到某一个相同的空间里）。现在假设 f,g 已经找到，新来一张 image,我要找他的【配偶】我就可以先通过 f 把他拉到这个 embedding 空间里。因为是新的图片，所以这个 embedding-space 里面是没有他的【配偶】的，但是我可以找一个离他最近的点，至少这个点应该【长得像】他的配偶。
</p>
</div>
</div>

<div id="org01e8365" class="outline-4">
<h4 id="org01e8365"><span class="section-number-4">3.1.2</span> 借用 word-vector</h4>
<div class="outline-text-4" id="text-3-1-2">
<p>
What if we dont have a database
</p>

<p>
如果我根本不知道每一个动物对应的 attribute 是什么，该怎么办呢？
</p>
<p>
<img src="Transfer Learning/screenshot_2017-06-15_16-35-49.png" alt="screenshot_2017-06-15_16-35-49.png" />
可以借用 word-vector
word-vector 的某个 dimension 就带表这个 word 的某种 attribute
所以你不一定需要一个 database
你就把 attribute 直接换成 word-vector,也做跟刚才一样的事情。
</p>

<p>
重新设计 loos-fn
[类间大][类内小]
刚才的思路只是在 最小化 一对夫妻的距离，但是【对跟对之间的距离没有考虑】
</p>
</div>
</div>

<div id="orga74b24b" class="outline-4">
<h4 id="orga74b24b"><span class="section-number-4">3.1.3</span> &gt;&gt;&gt; 区间控制：[类间大][类内小] tip</h4>
<div class="outline-text-4" id="text-3-1-3">
<hr />
<p>
注意：argmin<sub>fg</sub> Σ||fx - gx ||
是没有考虑 [类间大] 的。看看李老师是如何改进这个 loss-fn 的
argmin<sub>fg</sub> Σ max(0, k - f*g<sub>m</sub>=n + f*g<sub>m</sub>≠n)
[类间大]这个间距似乎就是 svm 的强项我们 hold 住了 loss-fn 的最小值，lossfn 最小为 0
丈夫跟自己的老婆的距离有多近呢？
k - f*g<sub>m</sub>=n + f*g<sub>m</sub>≠n &lt; 0
=&gt; f*g<sub>m</sub>=n - f*g<sub>m</sub>≠n &gt; k
丈夫跟不是自己老婆的所有女人中关系最近的哪一个的关系，比跟自己老婆的关系都要远一个 k
</p>

<p>
这个函数经典，张弛有度：首先这个函数是要越小越好，所以比较大的都会被干掉，比如
k - f*g<sub>m</sub>=n + f*g<sub>m</sub>≠n &gt; 0, 说明【跟配偶之外的异性关系暧昧】越暧昧这个值越大就越会被干掉。其次，f*g<sub>m</sub>=n - f*g<sub>m</sub>≠n 这个值不是越大越好么，最好无限大，‘水至清则无鱼’有可能一个点都找不到。所以设置了一个阈值，只要比这个阈值大就是可以接受的。所以想要 hold 住一个【区间】就是用这个函数：
max(0, 阈值－距离)
</p>

<hr />
</div>
</div>

<div id="orgbdeef86" class="outline-4">
<h4 id="orgbdeef86"><span class="section-number-4">3.1.4</span> ConSE: Convex combination of semantic embedding</h4>
<div class="outline-text-4" id="text-3-1-4">
<p>
<img src="Transfer Learning/screenshot_2017-06-15_17-20-12.png" alt="screenshot_2017-06-15_17-20-12.png" />
这个就是借用，你从网络上 download
一个已经 train 好的 off-the-shelf 图像辨识系统和一个已经 train 好的 off-the-shelf work2vec
</p>
<ol class="org-ol">
<li>一张图丢进 NN 他可能输出 0.5 狮子，0.5 老虎</li>
<li>找 lion tiger 的 word-vector,用刚才的比例混合</li>
<li>找一个 word-vector 跟按比例混合之后的结果最接近</li>
</ol>

<p>
这里你不需要任何 training, 只要两个现成的模型就可以做图像 &#x2013;&gt; 文字这种识别了。
</p>
</div>
</div>

<div id="orgf4b0f0b" class="outline-4">
<h4 id="orgf4b0f0b"><span class="section-number-4">3.1.5</span> DeVISE</h4>
<div class="outline-text-4" id="text-3-1-5">
<p>
把 word-vector 和 NN 都 project 到同一个 embedding space
</p>
</div>
</div>

<div id="orgbadbd60" class="outline-4">
<h4 id="orgbadbd60"><span class="section-number-4">3.1.6</span> ConvNet. vs DeViSE. vs ConSE(10)</h4>
<div class="outline-text-4" id="text-3-1-6">

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_17-31-50.png" alt="screenshot_2017-06-15_17-31-50.png" />
</p>
</div>
</div>
</div>

<div id="orga1d0e00" class="outline-4">
<h4 id="orga1d0e00"><span class="section-number-4">3.1.7</span> Example of zero-shot learning</h4>
<div class="outline-text-4" id="text-3-1-7">
<p>
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu,
Zhifeng Chen, Nikhil Thorat. Google’s Multilingual Neural Machine
Translation System: Enabling Zero-Shot Translation, arXiv preprint 2016
</p>


<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_17-37-47.png" alt="screenshot_2017-06-15_17-37-47.png" />
</p>
</div>

<p>
这个翻译机，从来没看过韩语到日语的翻译，但他看过其他种类的各种翻译。但是学习之后，他可以很好的把韩语翻译成日语。
</p>

<p>
把不同语言的不同句子 project 到同一个 embedding space 上面而这个 embedding space 是 language-independent 的。这个 embedding space 上的位置只跟这个句子的语义有关，跟具体的语言无关。
</p>



<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_17-41-07.png" alt="screenshot_2017-06-15_17-41-07.png" />
</p>
</div>

<p>
根据 learn 好的 translator, translator 有一个 encoder,
他会把 input 的句子变成 vector, dicoder 根据这个 vector
解成一个句子，这就是翻译的过程。
</p>

<p>
如果把很多语言的同一个意思的句子通过 encoder 映射到这个 embedding
space 中，可以发现他们处于很相近的位置上。图中同一个颜色代表同样的语义但是他们可以来自不同的语言。可以把这个 embedding space 看成是一种 [新的语言]
</p>
</div>
</div>

<div id="orgccea602" class="outline-4">
<h4 id="orgccea602"><span class="section-number-4">3.1.8</span> More about Zero-shot learning</h4>
<div class="outline-text-4" id="text-3-1-8">
<ul class="org-ul">
<li>Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton, Tom M. Mitchell,
“Zero-shot Learning with Semantic Output Codes”, NIPS 2009</li>
<li>Zeynep Akata, Florent Perronnin, Zaid Harchaoui and Cordelia Schmid,
“Label-Embedding for Attribute-Based Classification”, CVPR 2013</li>
<li>Andrea Frome, Greg S. Corrado, Jon Shlens, Samy Bengio, Jeff Dean,
Marc'Aurelio Ranzato, Tomas Mikolov, “DeViSE: A Deep Visual-Semantic
Embedding Model”, NIPS 2013</li>
<li>Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon
Shlens, Andrea Frome, Greg S. Corrado, Jeffrey Dean, “Zero-Shot Learning
by Convex Combination of Semantic Embeddings”, arXiv preprint 2013</li>
<li>Subhashini Venugopalan, Lisa Anne Hendricks, Marcus Rohrbach, Raymond
Mooney, Trevor Darrell, Kate Saenko, “Captioning Images with Diverse
Objects”, arXiv preprint 2016</li>
</ul>
</div>
</div>
</div>
</div>

<div id="org9bdcb7d" class="outline-2">
<h2 id="org9bdcb7d"><span class="section-number-2">4</span> 如果 source 没有 label</h2>
<div class="outline-text-2" id="text-4">

<div class="figure">
<p><img src="Transfer Learning/screenshot_2017-06-15_17-45-33.png" alt="screenshot_2017-06-15_17-45-33.png" />
</p>
</div>

<p>
Self-taught learning
</p>

<p>
Rajat Raina , Alexis Battle , Honglak Lee , Benjamin Packer , Andrew Y. Ng,
Self-taught learning: transfer learning from unlabeled data, ICML, 2007
</p>

<p>
Self-taught Clustering
</p>

<p>
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, Yong Yu, "Self- taught clustering", ICML
2008
</p>

<p>
注意，之前也学过，semi-supervised learning. 也是有 labelled and unlabelled
data. 但是两者有本质的区别，semi 中的 labelled 和 unlabelled 还是有很多关系的，只是没有标签。
</p>

<p>
而这里是说，两者完全来自不同的 domain,是几乎没有什么关系的。
</p>

<p>
Self-taught learning
</p>
<ul class="org-ul">
<li>Learning to extract better representation from the source data (unsupervised approach)</li>
<li>Extracting better representation for target data</li>
</ul>
</div>

<div id="org9ca2451" class="outline-4">
<h4 id="org9ca2451"><span class="section-number-4">4.0.1</span> 处理 unlabelled data 的思路</h4>
<div class="outline-text-4" id="text-4-0-1">
<p>
总之如果 source data 是 unlabelled data可以 learn 一个 feature extractor. 你可以用 auto-encoder 来 learn 这个 feature extractor. 或者 learn 一个好的
representation.
</p>

<p>
然后用这个 feature extractor or good representation 去 target data 上去抽
feature.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-06</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 日 14:55</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
