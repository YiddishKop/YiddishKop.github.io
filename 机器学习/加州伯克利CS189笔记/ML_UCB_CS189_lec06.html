<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 05:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 06 Decision theory</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 06 Decision theory</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org3329a45">lec 06 Decision theory</a>
<ul>
<li><a href="#org7c5c167">DECISION THEORY</a>
<ul>
<li><a href="#orgb43eed4">Bayes' Theorem:</a></li>
</ul>
</li>
<li><a href="#orge983034">3 WAYS TO BUILD CLASSIFIERS</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="org3329a45" class="outline-2">
<h2 id="org3329a45">lec 06 Decision theory</h2>
<div class="outline-text-2" id="text-org3329a45">
</div>
<div id="org7c5c167" class="outline-3">
<h3 id="org7c5c167">DECISION THEORY</h3>
<div class="outline-text-3" id="text-org7c5c167">
<p>
<code>=============</code>
[Today I'm going to talk about a style of classifier very different from SVMs.
 The classifiers we'll cover in the next few weeks are based on probability,
 because sometimes a point in feature space doesn't have just one class.]
</p>

<pre class="example">
[Suppose one borrower with income $30,000 and debt $15,000 defaults.
     another    "      "     "       "     "   "      "    doesn't default.
 So in your feature space, you have two samples at the _same point_ with
 _different classes_.  Obviously, in that case, you can't draw a decision
 boundary that classifies all points with 100% accuracy.]

</pre>

<p>
Multiple samples with different classes could lie at same point:
we want a <b>probabilistic classifier</b>.
</p>

<pre class="example">
Suppose 10% of population has cancer, 90% doesn't.              [caps here
Probability distributions for calorie intake, P(X | Y):          mean random
                                                                 variables,
   calories  (X)       |  &lt; 1,200  | 1,200--1,600 |  &gt; 1,600     not matrices.]
   --------------------+-----------+--------------+----------
   cancer    (Y =  1)  |    20%    |      50%     |    30%
   no cancer (Y = -1)  |     1%    |      10%     |    89%

[I made these numbers up.  Please don't take them as medical advice.]

Recall:  P(X) = P(X | Y = 1) P(Y = 1) + P(X | Y = -1) P(Y = -1)
         P(1,200 &lt;= X &lt;= 1,600) = 0.5 * 0.1 + 0.1 * 0.9 = 0.14

</pre>

<p>
You meet guy eating x = 1,400 calories/day.  Guess whether he has cancer?
</p>

<p>
[If you're in a hurry, you might see that 50% of people with cancer eat
 1,400 calories, but only 10% of people with no cancer do, and conclude that
 someone who eats 1,400 calories probably has cancer.  But that would be wrong,
 because that reasoning fails to take the prior probabilities into account.]
</p>
</div>

<div id="orgb43eed4" class="outline-4">
<h4 id="orgb43eed4">Bayes' Theorem:</h4>
<div class="outline-text-4" id="text-orgb43eed4">
<pre class="example">
| posterior probability      | prior prob.   | for 1,200 &lt;= X &lt;= 1,600
|                            v               v
v                P(X | Y = 1) P(Y = 1)    0.05        \
P(Y = 1 | X)  =  ---------------------  = ----         |
                          P(X)            0.14         |
                                                        &gt; sum is 1
                P(X | Y = -1) P(Y = -1)   0.09         |
P(Y = -1 | X) = ----------------------- = ----         |
                          P(X)            0.14        /

P(cancer | X = 1,400 cals) = 5/14 ~ 36%.

[So we probably shouldn't diagnose cancer.]
</pre>

<p>
[However, we've been assuming that we want to maximize the chance of a correct
 prediction.  But that's not always the right assumption.  If you're developing
 a cheap screening test for cancer, you'd rather have more false positives and
 fewer false negatives.  A false negative might mean somebody misses an early
 diagnosis and dies of a cancer that could have been treated if caught early.
 A false positive just means that you spend more money on more accurate tests.]
</p>

<p>
A <span class="underline">loss_function</span> L(z, y) specifies badness if true class is y,
classifier predicts z.
</p>

<pre class="example">
                /  1      if z = 1, y = -1         false positive is bad
E.g., L(z, y) = |  5      if z = -1, y = 1         false negative is BAAAAAD
                \  0      if z = y
</pre>

<p>
A 36% probability of loss 5 is worse than a 64% prob. of loss 1,
so we recommend further cancer screening.
</p>

<p>
Defs:  loss fn above is <span class="underline">asymmetrical</span>.
       The <span class="underline">0-1_loss_function</span> is 1 for incorrect predictions,  [symmetrical]
                                  0 for correct.
</p>

<p>
[Another example where you want a very asymmetrical loss function is for spam
 detection.  Putting a good email in the spam folder is much worse than putting
 spam in your inbox.]
</p>

<p>
Let r : R^d -&gt; +-1 be a <span class="underline">decision_rule</span>, aka <span class="underline">classifier</span>:  a fn that maps
a feature vector x to 1 ("in class") or -1 ("not in class").
</p>

<p>
The <span class="underline">risk</span> for r is the expected loss over all values of x, y:
</p>

<pre class="example">
R(r) = E[L(r(X), Y)]
     = sum (L(r(x), 1) P(Y = 1 | X = x) + L(r(x), -1) P(Y = -1 | X = x)) P(x)
        x
     = P(Y = 1) sum L(r(x), 1) P(X = x | Y = 1) +
                 x
       P(Y = -1) sum L(r(x), -1) P(X = x | Y = -1)
                  x
</pre>

<p>
The <span class="underline">Bayes_decision_rule</span> aka <span class="underline">Bayes_classifier</span> is the r that minimizes R(r);
call it r*.  Assuming L(z, y) = 0 for z = y:
</p>

<pre class="example">
        /   1      if L(-1, 1) P(Y = 1 | X = x) &gt; L(1, -1) P(Y = -1 | X = x),
r*(x) = |
        \  -1      otherwise
</pre>

<p>
In cancer example, r* = 1 for intakes &lt;= 1,600; r* = -1 for intakes &gt; 1,600.
</p>

<p>
The <span class="underline">Bayes_risk</span>, aka <span class="underline">optimal_risk</span>, is the risk of the Bayes classifier.
[In our cancer example, the last expression for risk gives:]
</p>

<p>
R(r*) = 0.1 (5 * 0.3) + 0.9 (1 * 0.01 + 1 * 0.1) = 0.249
</p>

<p>
[It is interesting that, if we really know all these probabilities, we really
 can construct an ideal probabilistic classifier.  But in real applications,
 we rarely know these probabilities; the best we can do is use statistical
 methods to estimate them.]
</p>

<p>

Suppose X has a continuous probability density fn (PDF).
</p>

<p>
Review:  [Go back to your CS 70 or stats notes if you don't remember this.]
</p>

<pre class="example">
                                                                 / x_2
^ P(x)    ====        prob. that random variable X in [x , x ] = |    P(x) dx
|       ==  |.==      [shaded area]                     1   2    / x_1
|      =    |..|=
|     =     |..| ==                                              / inf
|   ==      |..|   =====            area under whole curve = 1 = |    P(x) dx
| ==        |..|        =========                                / -inf
+-----------+--+----------------------&gt; x
            x  x                                            / inf
             1  2      _expected_ value of f(X):  E[f(X)] = |    f(x) P(x) dx
                                                            / -inf
                   / inf
_mean_ mu = E[X] = |    x P(x) dx
                   / -inf

                2             2       2      2
_variance_ sigma  = E[(X - mu) ] = E[X ] - mu
</pre>

<p>
[Perhaps our cancer statistics look like this:]
</p>

<pre class="example">
^             ====
|           ==    == P(X | Y = 1)                [area under each curve is 1]
|          =        =
|         =          =
|        =            =
|        =            =
|       =             +=++++++++++++++
|       =         ++++  =             ++++  P(X | Y = -1)
|      =        ++       =                ++
|     =       ++          ==                ++++
|   ==    ++++              ======              ++++++
| ==++++++                        ===============     +++++
+-----------------------------------------------------------&gt; x

</pre>

<p>
[Let's go back to the 0-1 loss function for a moment.  In other words, you want
 a classifier that maximizes the chance of a correct prediction.  The wrong
 answer would be to look where these two curves cross and make that be the
 decision boundary.  As before, it's wrong because it doesn't take into account
 the prior probabilities.]
</p>

<p>
Suppose P(Y = 1) = 1/3, P(Y = -1) = 2/3, 0-1 loss:
</p>

<pre class="example">
^  P(X | Y = 1) P(Y = 1)
|             ====        ++++++++
|           ==    ==  ++++        ++++
|          =        =+                ++  P(X | Y = -1) P(Y = -1)
|         =       ++|=                  ++
|        =       +..|.=                   +
|        =      +...|.=                    +
|       =      +....|..=                    ++
|       =     +.....|...=                     ++
|      =    ++......|....=                      ++
|     =   ++........|.....==                      ++++
|   ==++++..........|.......======                    +++++
| ==++..............|.............===============
+-------------------+----------------------------==========&gt; x
                    |
     Bayes optimal decision boundary

</pre>
<p>

[To maximize the chance you'll predict correctly whether somebody has cancer,
 the Bayes decision rule looks up x on this chart and picks the curve with the
 highest probability.  In this example, that means you pick cancer when x is
 left of the optimal decision boundary, and no cancer when x is to the right.]
</p>

<p>
Define <span class="underline">risk</span> as before, replacing summations with integrals.
</p>

<pre class="example">
R(r) = E[L(r(X), Y)]
                /
     = P(Y = 1) | L(r(x), 1) P(X = x | Y = 1) dx +
                /
                 /
       P(Y = -1) | L(r(x), -1) P(X = x | Y = -1) dx
                 /
</pre>

<pre class="example">
If L is 0-1 loss,          [the risk has a particularly nice interpretation]
  R(r) = P(r(x) is wrong)  [which makes sense, because R is the expected loss.]
</pre>

<p>
For Bayes decision rule, Bayes Risk is the area under minimum of functions
above (shaded).  Assuming L(z, y) = 0 for z = y:
</p>

<pre class="example">
        /
R(r*) = |  min  L(-y, y) P(X = x | Y = y) P(Y = y) dx
        / y=+-1
</pre>

<p>
[If you want to use an asymmetrical loss function, just scale the vertical
 reach of each curve accordingly in the figure above.]
</p>

<pre class="example">
_Bayes_optimal_decision_boundary_:  {x : P(Y = 1 | X = x) = 0.5}
                                         \______________/  \___/
                                           predictor fn   isovalue
</pre>

<p>
[Show figure of 2D Gaussians with decision boundary.]
</p>

<p>
[Obviously, the accuracy of the probabilities is most important near the
 decision boundary.  Far away from the decision boundary, a bit of error in
 the probabilities probably wouldn't change the classification.]
</p>

<p>
[You can also have multi-class classifiers, where each sample is in one class
 among many.  The Bayesian approach is a particularly convenient way to
 generate multi-class classifiers, because you can simply choose whichever
 class has the greatest posterior probability.  Then the decision boundary lies
 wherever two or more classes are tied for the highest probability.]
</p>

<p>

</p>
</div>
</div>
</div>
<div id="orge983034" class="outline-3">
<h3 id="orge983034">3 WAYS TO BUILD CLASSIFIERS</h3>
<div class="outline-text-3" id="text-orge983034">
<p>
<code>=========================</code>
(1)  Generative models (e.g. LDA)
</p>

<pre class="example">
- Assume samples come from probability distributions,
  different for each class.
- Guess form of distributions
- For each class C, fit distribution parameters to class C samples,
  giving P(X | Y = C)
- For each C, estimate P(Y = C)
- Bayes' Theorem gives P(Y | X)
- If 0-1 loss, pick class C that maximizes P(Y = C | X = x)    [posterior]
                   equivalently, maximizes P(X = x | Y = C) P(Y = C)
</pre>

<p>
(2)  Discriminative models (e.g. logistic regression)
</p>

<ul class="org-ul">
<li>Model P(Y | X) directly</li>
</ul>

<p>
(3)  Find decision boundary (e.g. SVM)
</p>

<ul class="org-ul">
<li>Model r(x) directly (no posterior)</li>
</ul>

<p>
Advantage of (1 &amp; 2):  P(Y | X) tells you probability your guess is wrong
                       [This is something SVMs don't do.]
Advantage of (1):  you can diagnose outliers:  P(X) is very small
Disadvantages of (1):  often hard to estimate distributions accurately;
                       real distributions rarely match standard ones.
</p>

<p>
[What I've written here doesn't actually define the phrases "generative model"
 or "discriminative model".  The proper definitions accord with the way
 statisticians think about models.  A <span class="underline">generative_model</span> is a full
 probabilistic model of all variables, whereas a <span class="underline">discriminative_model</span>
 provides a model only for the target variables.]
</p>

<p>
[It's important to remember that we rarely know precisely the value of any of
 these probabilities.  There is usually error in all of these P's, and in
 a generative model those errors can get compounded when we apply Bayes'
 Theorem to estimate P(Y | X).  In practice, generative models are most
 popular when you have phenomena that are really well fitted by the normal
 distribution.]
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 五</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 日 05:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
