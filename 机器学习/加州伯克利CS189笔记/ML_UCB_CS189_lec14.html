<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 æ—¥ 05:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 14 Kernel Perceptrons</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 14 Kernel Perceptrons</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org79dd461">lec 14 Kernel Perceptrons</a>
<ul>
<li><a href="#org82d5fb1">Kernel Logistic Regression</a></li>
<li><a href="#orgde401ba">The Gaussian Kernel</a></li>
</ul>
</li>
<li><a href="#org944a9f2">lec 14.2 SUBSET SELECTION</a></li>
<li><a href="#orgf44d876">lec 14.3 LASSO (Tibshirani, 1996)</a></li>
</ul>
</div>
</div>
<div id="org79dd461" class="outline-2">
<h2 id="org79dd461">lec 14 Kernel Perceptrons</h2>
<div class="outline-text-2" id="text-org79dd461">
<hr />
<pre class="example">
Note:  Everywhere below, we can replace X  with Phi(X )
                                         i           i

</pre>
<p>
Recall perceptron alg:
</p>

<pre class="example">
  while some y  X  . w &lt; 0
              i  i
    w &lt;- w + epsilon y  X
                      i  i
  for each test pt z
             T
    f(z) &lt;- w z
                    T                 T
Kernelize with w = X a:   X  . w = (XX a)  = (Ka)
                           i             i       i

</pre>

<p>
Dual perceptron alg:
</p>

<pre class="example">
                    T
a = [y_1, 0, ..., 0]            [starting point is arbitrary, but can't be 0]
K   = k(X , X )   for all i, j  &lt;= O(n^2 d) time (kernel trick)
 ij      i   j
while some y  (Ka)  &lt; 0
            i     i
  a  &lt;- a  + epsilon y          &lt;= optimization:  can update Ka in O(n) time
   i     i            i
for each test pt z
           n
  f(z) &lt;- sum a  k(X , z)       &lt;= O(nd) time
          j=1  j    j

</pre>

<p>
[The d's above do not increase if we replace X_i with a kernelized Phi(X_i)!]
</p>

<pre class="example">
OR we can compute w = X^T a once in O(nd') time
   &amp; evaluate test pts in O(d') time/pt, where d' is length of Phi(.)

Interpretation:  a_i reflects how many times we have added epsilon X
                 (or -epsilon X ) to w.                             i
                               i
</pre>
</div>
<div id="org82d5fb1" class="outline-3">
<h3 id="org82d5fb1">Kernel Logistic Regression</h3>
<div class="outline-text-3" id="text-org82d5fb1">
<hr />
<p>
[The stochastic gradient ascent step for logistic regression is just a small
 modification of the step for perceptrons.  However, remember that we're no
 longer looking for misclassified samples; instead, we apply the gradient
 ascent rule to weights in a stochastic, random order--or, alternatively, to
 all the weights at once.]
</p>

<pre class="example">
Stochastic gradient ascent step:
  a_i &lt;- a_i + epsilon (y  - s((Ka) ))       [where s is the logistic function]
                         i         i
[Just like with perceptrons, every time you update one weight a_i, if you're
 clever you can update Ka in O(n) time so you don't have to compute it from
 scratch on the next iteration.]

Gradient ascent step:
  a &lt;- a + epsilon (y - s(Ka))        &lt;= applying s component-wise to vector Ka

for each test pt z:
             n
  h(z) &lt;- s(sum a  k(X , z) )
            j=1  j    j
</pre>

<p>
[or, if you're using logistic regression as a classifier and you don't care
about the probability, you can skip the logistic fn and just compute the sum.]
</p>

<p>

</p>
</div>
</div>
<div id="orgde401ba" class="outline-3">
<h3 id="orgde401ba">The Gaussian Kernel</h3>
<div class="outline-text-3" id="text-orgde401ba">
<hr />
<p>
[...But I think our next act is even more mind-blowing.  Since we can now do
 fast computations in spaces with exponentially large dimension, why don't we
 go all the way and do computations in infinite-dimensional space?]
</p>

<p>
<span class="underline">Gaussian_kernel</span>, aka <span class="underline">radial_basis_fn_kernel</span>:
There exists a Phi(x) such that
</p>

<pre class="example">
                            2
                     |x - z|
  k(x, z) = exp ( - --------- )
                    2 sigma^2

[In case you're curious, here's the feature vector that gives you this kernel,
 for the case where you have only one input feature per sample.]

e.g. for d = 1,         2                                  2
                       x                x                 x               T
  Phi(x) = exp ( - --------- ) [1, --------------, ----------------, ... ]
                   2 sigma^2       sigma sqrt(1!)  sigma^2 sqrt(2!)
</pre>

<p>
[This is an infinite vector, and Phi(x)^T Phi(z) is a converging series.
 Nobody actually uses this value of Phi(x) directly, or even cares about it;
 they just use the kernel function k(.,.).]
[At this point, it's best <b>not</b> to think of points in a high-dimensional space.
 Instead, think of the kernel k as a measure of how similar or close together
 two samples are to each other.]
</p>

<pre class="example">
                                     n
Key observation:  hypothesis h(z) = sum a  k(X , z) is a linear combo of
                                    j=1  j    j
                  Gaussians centered at samples.
                  [The dual weights are the coefficients of the linear combo.]
                  [Think of the Gaussians as a basis for the hypothesis.]
                  [Show example hypothesis (gausskernel.pdf)]
</pre>

<p>
Very popular in practice!  Why?
</p>
<ul class="org-ul">
<li>Gives very smooth h</li>
<li>Behaves somewhat like k-nearest neighbor, but smoother</li>
<li>Oscillates less than polynomials (depending on sigma)</li>
<li>k(x, z) can be interpreted as a "similarity measure".
Gaussian is maximum when x = z, goes to 0 as distance increases.</li>
<li>Samples "vote" for value at z, but closer samples get weighter vote.</li>
</ul>

<p>
[The "standard" kernel x . y assigns high value to vectors that point in
 roughly the same direction.  The Gaussian kernel assigns high value to points
 near each other.]
</p>

<p>
sigma trades off bias vs. variance:
  larger sigma -&gt; wider Gaussians, smoother h -&gt; more bias, less variance
Choose by (cross-)validation.
</p>

<p>
[Show decision boundary of SVM with Gauss kernel (gausskernelsvm.pdf)]
</p>

<p>
[By the way, there are many other kernels like this that are defined directly
 as kernel functions without worrying about Phi.  But not every function can be
 a kernel function.  A function is qualified only if it always generates
 a positive semidefinite kernel matrix, no matter what the samples are.]
</p>

<p>

</p>
</div>
</div>
</div>
<div id="org944a9f2" class="outline-2">
<h2 id="org944a9f2">lec 14.2 SUBSET SELECTION</h2>
<div class="outline-text-2" id="text-org944a9f2">
<p>
<code>==============</code>
</p>
<pre class="example">
All features increase variance, but not all features reduce bias (much).
Idea:  Identify poorly predictive features, ignore them (weight zero).
       Less overfitting, lower test errors.
2nd motivation:  Inference.  Simpler models convey interpretable wisdom.
Useful in all classification &amp; regression methods.
Sometimes it's hard:  Different features can partly encode same information.
                      Combinatorially hard to choose best feature subset.

</pre>

<pre class="example">
Alg:  Best subset selection.  Try all 2^d - 1 nonempty subsets of features.
      Choose the best model by (cross-)validation.  Slow.
</pre>

<p>
[Obviously, best subset selection isn't tractible if we have a lot of features.
 But it gives us an "ideal" algorithm to compare practical algorithms with.
 If d is large, there is no algorithm that's both guaranteed to find the best
 subset and that runs in acceptable time.  But heuristics often work well.]
</p>

<p>
Heuristic:  Forward stepwise selection.
Start with <span class="underline">null_model</span> (0 features); repeatedly add best feature until
test errors start increasing (due to overfitting) instead of decreasing.
At each outer iteration, inner loop tries every feature and chooses the best
by cross-validation.  Requires training O(d^2) models instead of O(2^d).
Not perfect:  Won't find the best 2-feature model if neither of those features
              yields the best 1-feature model.
</p>

<p>
Heuristic:  Backward stepwise selection.
Start with all d features; repeatedly remove feature whose removal gives best
reduction in test errors.  Also trains O(d^2) models.
Additional heuristic:  Only try to remove features with small weights.
</p>
<pre class="example">
Q:  small relative to what?
                                                              2   T   -1
Recall:  variance of least-squ. regr. is proportional to sigma  (X  X)
                                     w_i
_z-score_ of weight w  is z  = ---------------
                     i     i   sigma sqrt(v_j)
                                     T   -1
where v_j is jth diagonal entry of (X  X)  .
Small z-score hints "true" w_i could be zero.

</pre>

<p>
[Forward stepwise selection is a better choice when you suspect only a few
 features will be good predictors.  Backward stepwise is better when you
 suspect most of the features will be necessary.  If you're lucky, you'll stop
 early.]
</p>

<p>

</p>
</div>
</div>
<div id="orgf44d876" class="outline-2">
<h2 id="orgf44d876">lec 14.3 LASSO (Tibshirani, 1996)</h2>
<div class="outline-text-2" id="text-orgf44d876">
<p>
<code>===</code>
Regression w/regularization:  (1) + (A) + l_1 penalized mean loss (e).
"Least absolute shrinkage and selection operator"
[This is a regularized regression method similar to ridge regression, but it
 has the advantage that it often naturally sets some of the weights to zero.]
</p>

<pre class="example">
  --------------------------------------------------
  |                               2                |
  | Find w that minimizes |Xw - y|  + lambda |w'|  |
  |                                              1 |
  --------------------------------------------------
               n
where |w'|  = sum |w |                                  (Don't penalize alpha.)
          1   i=1   i
                                             2
Recall ridge regr.:  isosurfaces of |w'|  are hyperspheres.
                                             2
The isosurfaces of |w'|  are _cross-polytopes_.
                       1
The unit cross-polytope is the convex hull of all the positive and negative
unit coordinate vectors.

               ^
              /|\
             / | \
      2D:   &lt;--+--&gt;           3D:  [Draw 3D cross-polytope]
             \ | /
              \|/
               v

</pre>
<p>
[You get other cross-polytope isosurfaces by scaling this.]
[Show plot of isocontours for the two objective fn terms (lassoridge.pdf)]
[Recall that beta is the least-squares solution, and the red ellipses are the
 isocontours of |Xw - y|^2.  The isocontours of |w'|_1 are diamonds centered at
 the origin (blue).  The solution lies where a red isocontour just touches
 a blue diamond.  What's interesting here is that in this example, the red
 isocontour touches just the tip of the diamond.  So one of the weights gets
 set to zero.  That's what we want to happen to weights that don't have enough
 influence.  This doesn't always happen--for instance, the red isosurface could
 touch a side of the diamond instead of a tip of the diamond.]
[When you go to higher dimensions, you might have several weights set to zero.
 For example, in 3D, if the red isosurface touches a sharp corner of the
 cross-polytope, two of the three weights get set to zero.  If it touches
 a sharp edge of the cross-polytope, one weight gets set to zero.  If it
 touches a flat side of the cross-polytope, no weight is zero.]
[Show plot of weights as a function of lambda (lassoweights.pdf)]
[This shows the weights for a typical linear regression problem with about 10
 variables.  You can see that as lambda increases, more and more of the weights
 become zero.  Only four of the weights are really useful for prediction;
 they're in color.  Statisticians used to choose lambda by looking at a chart
 like this and trying to eyeball a spot where there aren't too many predictors
 and the weights aren't changing too fast.  But nowadays they prefer
 cross-validation, as always.]
</p>

<p>
Sometimes sets some weights to zero, especially for large lambda.
Algs:  subgradient descent, least-angle regression (LARS), forward stagewise
[Lasso's objective function is not smooth, and that makes it tricky to
 optimize.  I'm not going to teach you an optimization method for Lasso.  If
 you need one, look up the last two of these names; LARS is built into the
 R Programming Language for statistics.]
[As with ridge regression, you should probably normalize the features first
 before applying this method.]
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 äº”</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 æ—¥ 05:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
