<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 æ—¥ 05:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 09 ANISOTROPIC GAUSSIANS</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 09 ANISOTROPIC GAUSSIANS</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orga3bc17d">lec 09 ANISOTROPIC GAUSSIANS</a>
<ul>
<li><a href="#org1d883bc">Maximum likelihood estimation for anisotropic Gaussians</a></li>
<li><a href="#orgae088c3">QDA</a></li>
<li><a href="#org11b03aa">LDA</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="orga3bc17d" class="outline-2">
<h2 id="orga3bc17d">lec 09 ANISOTROPIC GAUSSIANS</h2>
<div class="outline-text-2" id="text-orga3bc17d">
<p>
<code>===================</code>
</p>
<pre class="example">
  X ~ N(mu, Sigma)                     &lt;==  X is a random d-vector with mean mu

                     1                      1         T      -1
  P(x) = -------------------------- exp ( - - (x - mu)  Sigma   (x - mu) )
         sqrt(2 pi)^d sqrt(|Sigma|)         2
                           ^ determinant of Sigma

Sigma is the d-by-d SPD _covariance_matrix_.
Sigma^-1 is the d-by-d SPD _precision_matrix_; serves as metric tensor.

                                           T      -1
Write P(x) = n(q(x)), where q(x) = (x - mu)  Sigma   (x - mu)
             ^              ^
     R-&gt;R, exponential   R^d -&gt; R, quadratic

</pre>

<p>
[Now q(x) is a function we understand--it's just a quadratic bowl centered at
 mu whose curvature is represented by the metric tensor Sigma^-1.  q(x) is the
 squared distance from mu to x under this metric.  The other function n(.) is
 like a 1D Gaussian with a different normalization factor.  It is helpful to
 understand that this mapping n(.) does not change the isosurfaces.]
</p>

<pre class="example">
Principle:  given f:R -&gt; R, isosurfaces of f(q(x)) are same as q(x)
            (different isovalues), except that some might be "combined"
            [if f maps them to the same value]
</pre>

<p>
[Show example of paraboloid q(x) and bivariate Gaussian n(q(x)), w/univariate
 Gaussian n(.) between them.]
</p>

<pre class="example">
                                                  T        T          T
_covariance_:  Cov(X, Y) = E[(X - E[X]) (Y - E[Y]) ] = E[XY ] - mu  mu
               Var(X) = Cov(X, X)                                 X   Y
               [These two definitions hold for both vectors and scalars.]

</pre>

<p>
For a Gaussian, one can show Var(X) = Sigma.
[...by integrating the expectation in anisotropic spherical coordinates.]
Hence
</p>

<pre class="example">
        -                                                        -
        |   Var(X_1)       Cov(X_1, X_2)    ...    Cov(X_1, X_d) |
Sigma = | Cov(X_2, X_1)      Var(X_2)              Cov(X_2, X_d) |
        |      ...                                      ...      |
        | Cov(X_d, X_1)    Cov(X_d, X_2)    ...      Var(X_d)    |
        -                                                        -

</pre>

<p>
[An important point is that statisticians didn't just arbitrarily decide to
 call this thing a covariance matrix.  Rather, statisticians discovered that
 if you find the covariance of the normal distribution by integration, it turns
 out that each covariance happens to be an entry in the matrix inverse to the
 metric tensor.  This is a happy discovery; it's rather elegant.]
</p>

<p>
[Observe that Sigma is symmetric, as Cov(X_i, X_j) = Cov(X_j, X_i).]
</p>

<pre class="example">
X_i, X_j independent   ==&gt;   Cov(X_i, X_j) = 0
Cov(X_i, X_j) = 0  AND  joint normal distribution   ==&gt;   X_i, X_j independent
all features pairwise independent   ==&gt;   Sigma is diagonal
Sigma is diagonal   &lt;==&gt;   axis-aligned Gaussian; squared radii on the diagonal
                    &lt;==&gt;   P(X) = P(X_1) P(X_2) ... P(X_d)
                           \__/   \______________________/
                     multivariate     each univariate

</pre>

<p>
[So when the features are independent, you can write the multivariate Gaussian
 as a product of univariate Gaussians.  When they aren't, you can do a change
 of coordinates to the eigenvector coordinate system, and write it as a product
 of univariate Gaussians in those coordinates.]
</p>

<p>
[It's tricky to keep track of the relationships between the matrices, so here's
 a handy chart.]
</p>

<pre class="example">
        covariance                 precision
          matrix                    matrix
                                   (metric)
                       inverse              -1
 -----&gt;   Sigma     &lt;-----------&gt;  M = Sigma
/          ^                          ^
|          | |                        | |
|   square | |                 square | |
|          | | root                   | | root
|          | |                        | |
|            v                          v
|             1/2      inverse              -1/2
|        Sigma      &lt;-----------&gt;  A = Sigma
|   (maps spheres to             (maps ellipsoids
|      ellipsoids)                  to spheres)
|
|          ^                     1/2
|          | eigenvalues of Sigma    are ellipsoid radii
|            (standard deviations along the eigenvectors) ---\
|                                                            |
eigenvalues of Sigma are variances along the eigenvectors    |
                            |                                v
                            v     T                1/2           1/2  T
Diagonalizing:  Sigma = V Lambda V            Sigma    = V Lambda    V
</pre>

<p>
[Remember that all four of these matrices have the same eigenvectors V.
 Remember that when you take the inverse or square or square root of an SPD
 matrix, you do the same to its eigenvalues.  So the ellipsoid radii, being the
 eigenvalues of Sigma^1/2, are the square roots of the eigenvalues of Sigma;
 moreover, they are the inverse square roots of the precision matrix (metric).]
</p>

<p>
[Keep this chart handy when you do Homework 3.]
</p>

<p>

</p>
</div>
<div id="org1d883bc" class="outline-3">
<h3 id="org1d883bc">Maximum likelihood estimation for anisotropic Gaussians</h3>
<div class="outline-text-3" id="text-org1d883bc">
<hr />
<p>
Given samples x_1, ..., x_n and classes y_1, ..., y_n, find best-fit Gaussians.
</p>

<p>
[Once again, we want to fit the Gaussian that maximizes the likelihood of
 generating the samples in a specified class.  This time I won't derive the
 maximum-likelihood Gaussian; I'll just tell you the answer.]
</p>

<pre class="example">
For QDA:

    ^      1                                  T
  Sigma  = -     sum     (x  - mu ) (x  - mu )     &lt;== _conditional_covariance_
       C   n  {i:y  = C}   i     C    i     C          for samples in class C
            C     i      \____________________/
                          outer product matrix       [Show example maxlike.jpg]

</pre>

<pre class="example">
Priors pi_C, means mu_C:  same as before
[Priors are class samples / total samples; means are per-class sample means]
  ^
Sigma_C is positive semidefinite, but not always definite!
If some zero eigenvalues, must eliminate the zero-variance dimensions
(eigenvectors).

</pre>

<p>
[I'm not going to discuss how to do that today, but it involves projecting the
 samples onto a subspace along the eigenvectors with eigenvalue zero.]
</p>

<pre class="example">
For LDA:

    ^      1                                     T
  Sigma  = - sum    sum     (x  - mu ) (x  - mu )     &lt;== _pooled_within-class_
           n  C  {i:y  = C}   i     C    i     C          _covariance_matrix_
                     i

</pre>

<p>
[Notice that although we're computing one covariance matrix for all the data,
 each sample contributes with respect to <b>its*own*class's*mean</b>.  This gives
 a very different result than if you simply compute one covariance matrix for
 all the samples using the global mean!  In the former case, the variances are
 typically smaller.]
</p>

<p>

[Let's revisit QDA and LDA and see what has changed now that we know
 anisotropic Gaussians.  The short answer is "not much has changed".
 By the way, capital X is once again a random variable.]
</p>
</div>
</div>

<div id="orgae088c3" class="outline-3">
<h3 id="orgae088c3">QDA</h3>
<div class="outline-text-3" id="text-orgae088c3">
<p>
---
</p>
<pre class="example">
pi , mu , Sigma  may be different for each class C.
  C    C       C

Choosing C that maximizes P(X = x | Y = C) pi  is equivalent to maximizing
the _quadratic_discriminant_fn_              C
                        d               1         1
  Q (x) = ln (sqrt(2 pi)  P(x) pi ) = - - q (x) - - ln |Sigma | + ln pi
   C                      ^      C      2  C      2          C         C
                          |
                    Gaussian for C                 [works for any # of classes]

2 classes:  Prediction fn Q (x) - Q (x) is quadratic, but may be indefinite
                           C       D
            ==&gt;  Bayes decision boundary is a quadric.

            Posterior is P(Y = C | X = x) = s(Q (x) - Q (x))
                                               C       D
            where s(.) is logistic fn


</pre>
<p>
[Show example where decision boundary is a hyperbola:  two anisotropic
 Gaussians, Q_C - Q_D, logistic fn, posterior.  Show anisotropic Voronoi
 diagram.  Separate "whiteboard".]
</p>
</div>
</div>

<div id="org11b03aa" class="outline-3">
<h3 id="org11b03aa">LDA</h3>
<div class="outline-text-3" id="text-org11b03aa">
<p>
---
One Sigma for all classes.
[Once again, the quadratic terms cancel each other out so the predictor
 function is linear and the decision boundary is a hyperplane.]
</p>

<pre class="example">
Q (x) - Q (x) =
 C       D
                           T       -1          T       -1
           T      -1     mu_C Sigma   mu_C - mu_D Sigma   mu_D
(mu  - mu )  Sigma   x - ------------------------------------- + ln pi  - ln pi
   C     D                                 2                          C        D
\____________________/ \_______________________________________________________/
          T
         w x          +                          alpha

Choose class C that maximizes the _linear_discriminant_fn_

    T      -1     1   T      -1
  mu  Sigma   x - - mu  Sigma   mu  + ln pi        [works for any # of classes]
    C             2   C           C        C

                                  T
2 classes:  Decision boundary is w x + alpha = 0
                                               T
            Posterior is P(Y = C | X = x) = s(w x + alpha)

</pre>

<p>
[Show example where decision boundary is a line:  two anisotropic Gaussians,
 Q_C - Q_D, logistic fn, posterior.  Show ESL, Figure 4.11 (LDAdata.pdf):
 example of LDA with messy data.  Separate "whiteboard".]
</p>

<pre class="example">
Notes:

- Changing prior pi_C (or loss) is easy:  adjust alpha.
- LDA often interpreted as projecting samples onto normal vector w;
  cutting the line in half.
  [Show projection onto normal vector (project.png)]
- For 2 classes, LDA has d+1 parameters (w, alpha);
                 QDA has d(d+3)/2 + 1 parameters;
                 QDA more likely to overfit.
                 [Show comparison (ldaqda.pdf)]
- With features, LDA can give nonlinear boundaries; QDA can give nonquadratic.
- We don't get *true* optimum Bayes classifier
  -- estimate distributions from finite data
  -- real-world data not perfectly Gaussian
- Posterior gives decision boundaries for 10% confidence, 50%, 90%, etc.
  -- Choosing isovalue = probability p is same as choosing asymmetrical loss
     p for false positive, (1 - p) for false negative.
</pre>


<p>
[LDA &amp; QDA are best in practice for many applications.  In STATLOG project, LDA
 or QDA were in the top three classifiers for 10 out of 22 datasets.  But it's
 not because all those datasets are Gaussian.  LDA &amp; QDA work well when the
 data can only support simple decision boundaries such as linear or quadratic,
 because Gaussian models provide stable estimates.]  [ESL, Section 4.3]
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 äº”</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 æ—¥ 05:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
