<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 05:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 11.1 WEIGHTED LEAST-SQUARES REGRESSION</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 11.1 WEIGHTED LEAST-SQUARES REGRESSION</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5eaca96">lec 11.1 WEIGHTED LEAST-SQUARES REGRESSION</a></li>
<li><a href="#org91cfe3b">lec 11.2 NEWTON'S METHOD</a></li>
<li><a href="#org68399fa">lec 11.3 LOGISTIC REGRESSSION (continued)</a>
<ul>
<li><a href="#org0c88784">LDA vs. logistic regression</a></li>
</ul>
</li>
<li><a href="#org089aa23">lec 11.4 ROC CURVES (for test sets)</a></li>
<li><a href="#org88468f8">lec 11.5 LEAST-SQUARES POLYNOMIAL REGRESSION</a></li>
</ul>
</div>
</div>
<div id="org5eaca96" class="outline-2">
<h2 id="org5eaca96">lec 11.1 WEIGHTED LEAST-SQUARES REGRESSION</h2>
<div class="outline-text-2" id="text-org5eaca96">
<p>
<code>===============================</code>
linear regression fn (1) + squared loss fn (A) + cost fn (c).
</p>

<p>
[The idea of weighted least-squares is that some samples might be more trusted
 than others, or there might be certain samples you want to fit particularly
 well.  So you assign those more trusted samples a higher weight.  If you
 suspect some samples of being outliers, you can assign them a lower weight.]
</p>

<pre class="example">
Assign each sample a weight omega_i; collect them in nxn diagonal matrix Omega.
                                            ^                          ^
Greater omega_i -&gt; work harder to minimize |y_i - y_i|        recall:  y = Xw

  --------------------------------------------------
  |                               T                |    n                     2
  | Find w that minimizes (Xw - y)  Omega (Xw - y) | = sum omega  ((Xw)  - y )
  --------------------------------------------------   i=1      i      i    i

                                   T             T
Solve for w in normal equations:  X  Omega Xw = X  Omega y

[Once again, you can interpret it as a projection:]
            1/2 ^                                  1/2
Note:  Omega    y is orthogonal projection of Omega    y onto
                                           1/2
       subspace spanned by columns of Omega    X.
</pre>

<p>
[If you stretch the n-dimensional space by applying the linear transformation
 Omega^1/2, it's an orthogonal projection in that stretched space.]
</p>
</div>
</div>

<div id="org91cfe3b" class="outline-2">
<h2 id="org91cfe3b">lec 11.2 NEWTON'S METHOD</h2>
<div class="outline-text-2" id="text-org91cfe3b">
<p>
<code>=============</code>
Iterative optimization method for smooth fn J(w).
Often much faster than gradient descent.  [We'll use for logistic regression.]
</p>

<p>
Idea:  You're at point v.  Approximate J(w) near v by quadratic fn.
       Jump to its unique critical pt.  Repeat until bored.
</p>

<p>
[Show method in 1D (newton1.pdf--newton3.pdf); in 2D (newton2D.png).]
</p>

<pre class="example">
Taylor series about v:
                               2                          2
  grad J(w) = grad J(v) + (grad  J(v)) (w - v) + O(|w - v| )

  where grad^2 J(v) is the _Hessian_matrix_ of J(w) at v.

Find critical pt w by setting grad J(w) = 0:

               2      -1
  w = v - (grad  J(v))   grad J(v)
</pre>
<p>
[This is an iterative update rule you can repeat until it converges to
 a solution.  As usual, we don't really want to compute a matrix inverse.]
</p>

<pre class="example">
Newton's method:

  pick starting point w
  repeat until convergence              2
    e &lt;- solution to linear system (grad  J(w)) e = - grad J(w)
    w &lt;- w + e

</pre>
<p>

Warning:  Doesn't know difference between minima, maxima, saddle points.
          Starting point must be "close enough" to desired solution.
</p>

<p>
[If the objective function J is actually quadratic, Newton's method needs only
 one step to find the correct answer.  The closer J is to quadratic, the faster
 Newton's method tends to converge.]
[Newton's method is superior to blind gradient descent for some optimization
 problems for several reasons.  First, it tries to find the right step length
 to reach the minimum, rather than just walking an arbitrary distance downhill.
 Second, rather than follow the direction of steepest descent, it tries to
 optimize all directions at once.]
[Nevertheless, it has some major disadvantages.  The biggest one is that
 computing the Hessian can be quite expensive, and it has to be recomputed
 every iteration.  It can work well for low-dimensional feature spaces, but
 you would never use it for a neural net, because there are too many weights.
 Newton's method also doesn't work for most nonsmooth functions.  It
 particularly fails for the perceptron risk function, whose Hessian is zero,
 except where it's not even defined.]
</p>
</div>
</div>

<div id="org68399fa" class="outline-2">
<h2 id="org68399fa">lec 11.3 LOGISTIC REGRESSSION (continued)</h2>
<div class="outline-text-2" id="text-org68399fa">
<p>
<code>==================</code>
</p>
<pre class="example">
Recall:  s'(gamma) = s(gamma) (1 - s(gamma))          s  = s(X  . w)
                                                       i      i
                       n                  T
         grad  J(w) = sum (y  - s ) X  = X  (y - s)
             w        i=1   i    i   i
where s is n-vector with components s_i
[Now we work out the Hessian too, so we can use Newton's method.]

      2           n                  T      T
  grad  J(w) = - sum s  (1 - s ) X  X  = - X  Omega X
      w          i=1  i       i   i  i

where Omega is diagonal matrix with components s  (1 - s )
                                                i       i
                                     T
Omega is +ve definite for all w, so X  Omega X is +ve semidefinite,
so -J(w) is convex.
[Therefore, Newton's method finds an optimal point if it converges at all.]

Newton's method:
                                    T               T
Solve for e in normal equations:  (X  Omega X) e = X  (y - s)
w &lt;- w + e                              ^                  ^
repeat until "convergence"         Remember:  Omega, s are fns of w

</pre>

<p>
[Notice that this looks a lot like weighted least squares, but the weight
 matrix Omega and the right-hand-side vector y-s change every iteration.]
An example of <span class="underline">iteratively_reweighted_least_squares</span>.
Omega prioritizes samples with s_i near 0.5; tunes out samples near 0/1.
[In other words, samples near the decision boundary have the biggest effect on
 the iterations.  Meanwhile, the iterations move the decision boundary; in
 turn, that movement may change which samples have the most influence.  In the
 end, only the samples near the decision boundary make a big contribution to
 the logistic fit.]
</p>

<p>
Idea:  If n very large, save time by using a random subsample of the samples
per iteration.  Increase sample size as you go.
</p>

<p>
[The principle is that the first iteration isn't going to take you all the way
 to the optimal point, so why waste time looking at all the samples?  Whereas
 the last iteration should be the most accurate one.]
</p>
</div>

<div id="org0c88784" class="outline-3">
<h3 id="org0c88784">LDA vs. logistic regression</h3>
<div class="outline-text-3" id="text-org0c88784">
<hr />
<p>
Advantages of LDA:
</p>
<ul class="org-ul">
<li>For well-separated classes, LDA stable; log. reg. surprisingly unstable</li>
<li>&gt; 2 classes easy &amp; elegant, log. reg. needs modifying ("softmax regression")</li>
<li>Slightly more accurate when classes nearly normal, especially if n is small</li>
</ul>
<p>
Advantages of log. reg.:
</p>
<ul class="org-ul">
<li>More emphasis on decision boundary [though not as much as SVMs]
[Show figure of log.reg. vs. LDA for tight boundary (logregvsLDAauni.pdf)]</li>
<li>Hence less sensitive to outliers</li>
<li>Easy &amp; elegant treatment of "partial" class membership; LDA is all-or-nothing</li>
<li>More robust on some non-Gaussian distributions (e.g. large skew)</li>
</ul>
</div>
</div>
</div>

<div id="org089aa23" class="outline-2">
<h2 id="org089aa23">lec 11.4 ROC CURVES (for test sets)</h2>
<div class="outline-text-2" id="text-org089aa23">
<p>
<code>========</code>
[Show ROC curve (ROC.pdf).]
[This is a <span class="underline">ROC_curve</span>.  That stands for <span class="underline">receiver_operating_characteristics</span>,
 which is an awful name but we're stuck with it for historical reasons.
 It is made by running a classifier on the test set or validation set.
 It shows the rate of false positives vs. true positives for a range of
 settings.
 We assume there is a knob we can turn to trade off these two types of error;
 in this case, that knob is the probability threshold for Gaussian discriminant
 analysis or linear regression.
 However, neither axis is the probability threshold.]
x-axis:  "false positive rate = % of -ve classified as +ve"
y-axis:  "true positive rate = % of +ve classified as +ve  aka <span class="underline">sensitivity</span>"
"false negative rate":  vertical distance from curve to top
"_specificity_":  horizontal distance from curve to right
[You generate this curve by trying <b>every</b> probability threshold; for each
 threshold, measure the false positive &amp; true positive rates and plot a point.]
upper right corner:  "always classify +ve (Pr &gt;= 0)"
lower left corner:  "always classify -ve (Pr &gt; 1)"
diagonal:  "random classifiers"
[A rough measure of a classifier's effectiveness is the area under the curve.
 For a classifier that is always correct and never assigns a probability
 between 0 and 1, the area under the curve is one.
</p>

<p>
[IMPORTANT:  In practice, the trade-off between false negatives and false
positives is usually negotiated by choosing a point on this plot, based on real
test data, and NOT by taking the choice of threshold that's best in theory.]
</p>
</div>
</div>

<div id="org88468f8" class="outline-2">
<h2 id="org88468f8">lec 11.5 LEAST-SQUARES POLYNOMIAL REGRESSION</h2>
<div class="outline-text-2" id="text-org88468f8">
<p>
<code>=================================</code>
</p>
<pre class="example">
Replace each X  with feature vector Phi(X ) with all terms of degree 1...p
              i                          i
                  2               2                   T
e.g. Phi(X ) = [ X     X   X     X     X     X     1 ]
          i       i1    i1  i2    i2    i1    i2
</pre>

<p>
[Notice that we've added the fictitious dimension "1" here, so we don't need
 to add it again.  This basis covers all polynomials quadratic in X_i1, X_i2.]
</p>

<p>
Can also use non-polynomial features (e.g. edge detectors).
Otherwise just like linear or logistic regression.
</p>

<p>
Log. reg. + quadratic features = same logistic posteriors as QDA.
</p>

<p>
Very easy to overfit!      [Show overfits (overunder.png; UScensusquartic.png)]
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 五</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 日 05:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
