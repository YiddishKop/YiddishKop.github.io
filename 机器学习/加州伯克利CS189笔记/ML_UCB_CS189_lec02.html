<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 05:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 02 Classifiers</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 02 Classifiers</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgba9e9e3">lec 02 Classifiers</a>
<ul>
<li><a href="#org671a49b">Classifiers</a>
<ul>
<li><a href="#orgc009cd2">Sample, Feature(dimension), Class&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></a></li>
<li><a href="#orgf8d7ce8">Representation of an example</a></li>
<li><a href="#org402684f">basic concepts of ML&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></a></li>
<li><a href="#orga4959d6">an example of 2 feature classifier</a></li>
<li><a href="#org0ed41ba">Decision Boundary&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></a></li>
<li><a href="#orga2aaf0f">Predictor/Decision/Discriminant Function&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></a></li>
<li><a href="#orgc4efe36">Isosurface,Isovalue,Isocontour&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></a></li>
<li><a href="#orge8a81b5">Linear Classifier&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></a></li>
<li><a href="#org8c38985">Overfitting&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></a></li>
</ul>
</li>
<li><a href="#org488ea0e">Math Review</a>
<ul>
<li><a href="#org5f292a4">Vectors:</a></li>
<li><a href="#org5c2c446">Conventions of symbol</a></li>
<li><a href="#org7f8257f">Inner product: Linear fn</a></li>
<li><a href="#orgd3e4153">Inner product: Euclidean norm, Normalize</a></li>
<li><a href="#org327eb6b">Inner product: Compute angle</a></li>
<li><a href="#org543cc07">Predictor fn: Hyperplane,</a></li>
<li><a href="#org42e75fa">Hyperplane: Normal vector</a></li>
<li><a href="#org1ece4c7">Normal vector: Signed Distance</a></li>
<li><a href="#orgf5536cc">Signed Distance: Weight</a></li>
</ul>
</li>
<li><a href="#org5ca24c6">A Stupid Classifier</a>
<ul>
<li><a href="#org6dbb873">Centroid method</a></li>
</ul>
</li>
<li><a href="#org93af924">Perceptron Algorithm</a>
<ul>
<li><a href="#org56ad49b">Key questions of Numerical Optimization</a></li>
<li><a href="#orgec1e83f">Represent a sample: a row of features</a></li>
<li><a href="#org1ecba94">Represent Label</a></li>
<li><a href="#org5a48fdb">Goal:  find weights w</a></li>
<li><a href="#org360bcb5">Get constraint for optimization</a></li>
<li><a href="#org1efeacc">Define Loss function</a></li>
<li><a href="#org1bd0703">Define risk function</a></li>
<li><a href="#org4b5ff26">Summarize</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="orgba9e9e3" class="outline-2">
<h2 id="orgba9e9e3">lec 02 Classifiers</h2>
<div class="outline-text-2" id="text-orgba9e9e3">
<p>
Linear classifiers.
Predictor functions and decision boundaries.
The centroid method.
Perceptrons.
Read parts of the Wikipedia <a href="https://www.wikiwand.com/en/Perceptron">Perceptron</a> page.
Read ESL, Section 4.5–4.5.1
</p>
</div>

<div id="org671a49b" class="outline-3">
<h3 id="org671a49b">Classifiers</h3>
<div class="outline-text-3" id="text-org671a49b">
</div>
<div id="orgc009cd2" class="outline-4">
<h4 id="orgc009cd2">Sample, Feature(dimension), Class&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></h4>
<div class="outline-text-4" id="text-orgc009cd2">
<p>
You are given a data set of n <span class="underline">samples</span>, each with d <span class="underline">features</span>.
d features = d dimensional space.
we're going to represent each sample as a point in <span class="underline">d dimensional space</span>.
Some samples belong to <span class="underline">class</span> O; some do not.
</p>
</div>
</div>

<div id="orgf8d7ce8" class="outline-4">
<h4 id="orgf8d7ce8">Representation of an example</h4>
<div class="outline-text-4" id="text-orgf8d7ce8">
<p>
Example:  Samples are bank loans
          Features are income &amp; age (d = 2)
          Some are in class "defaulted", some are not
</p>

<p>
Goal:  Predict whether future borrowers will default,
       based on their income &amp; age.
</p>
</div>
</div>
<div id="org402684f" class="outline-4">
<h4 id="org402684f">basic concepts of ML&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></h4>
<div class="outline-text-4" id="text-org402684f">
<p>
Sample, Point, Feature_vector, Predictor, Independent_var
Represent each <span class="underline">sample</span> as a <span class="underline">point</span> in d-dimensional space,
called a <span class="underline">feature_vector</span> (aka <span class="underline">predictors</span>, <span class="underline">independent_variables</span>).
</p>
</div>
</div>

<div id="orga4959d6" class="outline-4">
<h4 id="orga4959d6">an example of 2 feature classifier</h4>
<div class="outline-text-4" id="text-orga4959d6">

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-04-30_22-09-06.png" alt="screenshot_2017-04-30_22-09-06.png" />
</p>
</div>

<pre class="example">
        ^                        ^                        ^    _overfitting_
        |  X     X X             |  X     X O             |  X  O  O X
        |    X   X               |    X   X               |    X   X
        | O       X              | O       O              | X       O
income  |O     X         income  |O     X         income  |O     X
        |    O   X               |    O X O               |    O   X
        | O    O   X             | O    O   O             | X    O   X
        | O       O              | O       O              | O       O
        +-----------&gt;            +-----------&gt;            +-----------&gt;
        age                      age                      age
</pre>

<p>
[Draw lines/curves separating O's from X's.  Use those curves to predict
which future borrowers will default.]
</p>
</div>
</div>

<div id="org0ed41ba" class="outline-4">
<h4 id="org0ed41ba">Decision Boundary&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></h4>
<div class="outline-text-4" id="text-org0ed41ba">
<p>
<span class="underline">decision boundary</span>:  the boundary chosen by our classifier to separate
  items in the class from those not.
</p>

<p>
[By the way, when I underline a word or a short phrase, usually that is a
 <b>definition</b>.  If you want to do well in this course, you should <b>memorize</b>
 all the definitions I write down.]
</p>
</div>
</div>

<div id="orga2aaf0f" class="outline-4">
<h4 id="orga2aaf0f">Predictor/Decision/Discriminant Function&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></h4>
<div class="outline-text-4" id="text-orga2aaf0f">
<p>
Some (not all) classifiers work by computing a
</p>
<pre class="example">
_predictor function_:  A function f(x) that maps a sample point x to

a scalar such that
f(x) &gt;  0     if x is in class O;
f(x) &lt;= 0     if x not in class O.
Aka _decision_function_ or _discriminant_function_.
</pre>

<p>
For these classifiers, the <span class="underline">decision boundary</span> is {x ∈ R^d : f(x) = 0}
[That is, the set of all points where the prediction function is zero.]
Usually, this set is <span class="underline">a (d - 1) dimensional surface</span> in R^d space.
</p>
</div>
</div>

<div id="orgc4efe36" class="outline-4">
<h4 id="orgc4efe36">Isosurface,Isovalue,Isocontour&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></h4>
<div class="outline-text-4" id="text-orgc4efe36">
<p>
{x : f(x) = 0} is also called an <span class="underline">isosurface</span> of f for the <span class="underline">isovalue</span> 0.
</p>

<p>
f has other isosurfaces for other isovalues, e.g. {x : f(x) = 1}.
</p>

<p>
[Show plot &amp; <b>isocontours</b> of sqrt(x^2 + y^2) - 3.
Imagine a function in R^d, and imagine its (d - 1)-dimensional isosurface.]
</p>
<p>
<img src="Machine Learning/screenshot_2017-04-30_22-55-32.png" alt="screenshot_2017-04-30_22-55-32.png" />
left is sample space of 3-dimensional space
right picture is a set of circular 'cake' which means 2-dimension isosurface with different radius
但是更多时候，我们会有很多 feature，也就是很多维度的向量，each feature 1 dimension.
比如处理图像，一张 4pixel × 4pixel 图像就是 16-dimension feature-vector(有一个 15-dimensional surface),
each pixel may have 6 bytes space to represent different color. it is huge dimensional space.
</p>


<div class="figure">
<p><img src="Machine Learning/screenshot_2017-04-30_23-07-59.png" alt="screenshot_2017-04-30_23-07-59.png" />
</p>
</div>

<p>
this is another 3-dimensional sample space, each isosurface would look like
an ellipsoid(but,still is a 2-dimensional surface) in 3-dimensional space
</p>
</div>
</div>

<div id="orge8a81b5" class="outline-4">
<h4 id="orge8a81b5">Linear Classifier&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></h4>
<div class="outline-text-4" id="text-orge8a81b5">
<p>
<span class="underline">linear classifier</span>:  The decision boundary is a hyperplane.
Usually uses a linear predictor function.  [Sometimes no predictor fn.]
</p>
</div>
</div>
<div id="org8c38985" class="outline-4">
<h4 id="org8c38985">Overfitting&#xa0;&#xa0;&#xa0;<span class="tag"><span class="def">def</span></span></h4>
<div class="outline-text-4" id="text-org8c38985">
<p>
<span class="underline">overfitting</span>:  When sinuous decision boundary fits sample data so well that it
doesn't classify future items well.
</p>
</div>
</div>
</div>

<div id="org488ea0e" class="outline-3">
<h3 id="org488ea0e">Math Review</h3>
<div class="outline-text-3" id="text-org488ea0e">
</div>
<div id="org5f292a4" class="outline-4">
<h4 id="org5f292a4">Vectors:</h4>
<div class="outline-text-4" id="text-org5f292a4">
<p>
[I will write vectors in matrix notation.]
</p>

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-04-30_23-16-31.png" alt="screenshot_2017-04-30_23-16-31.png" />
</p>
</div>
<pre class="example">
    -   -
    |x_1|
    |x_2|                        T
x = |x_3| = [x_1 x_2 x_3 x_4 x_5]
    |x_4|
    |x_5|
    -   -
</pre>
<p>
Think of x as a point in 5-dimensional space.
</p>
</div>
</div>

<div id="org5c2c446" class="outline-4">
<h4 id="org5c2c446">Conventions of symbol</h4>
<div class="outline-text-4" id="text-org5c2c446">
<pre class="example">
Conventions (often, but not always):
uppercase roman = matrix or random variable   X
lowercase roman = vector                      x
Greek = scalar                                alpha
Other scalars:                                n = # of samples
                                              d = # of features (per sample)
                                                = dimension of sample points
                                              i j k = indices
function (often scalar)                       f( ), s( ), ...

</pre>
</div>
</div>
<div id="org7f8257f" class="outline-4">
<h4 id="org7f8257f">Inner product: Linear fn</h4>
<div class="outline-text-4" id="text-org7f8257f">
<p>
<span class="underline">inner_product</span> (aka <span class="underline">dot_product</span>):
</p>
<pre class="example">
x . y = x_1 y_1 + x_2 y_2 + ... + x_d y_d

                                T
_Matrix notation_ also written x  y

Clearly,  f(x) = w . x + alpha  is a _linear_function_ in x.
</pre>
</div>
</div>

<div id="orgd3e4153" class="outline-4">
<h4 id="orgd3e4153">Inner product: Euclidean norm, Normalize</h4>
<div class="outline-text-4" id="text-orgd3e4153">
<p>
<span class="underline">Euclidean_norm</span>:
</p>

<pre class="example">
|x| = sqrt(x . x) = sqrt(x_1^2 + x_2^2 + ... + x_d^2) |

|x| is the length (aka Euclidean length) of a vector x.

                   x
Given a vector x, --- is a _unit_ vector (length 1).
                  |x|
                                           x
"_Normalize_ a vector x":  replace x with ---.
                                          |x|
</pre>
</div>
</div>

<div id="org327eb6b" class="outline-4">
<h4 id="org327eb6b">Inner product: Compute angle</h4>
<div class="outline-text-4" id="text-org327eb6b">
<pre class="example">
Use dot products to compute angles:

      x
     /                       x . y     x     y
    /           cos theta = ------- = --- . ---
   / theta                  |x| |y|   |x|   |y|
  ----------&gt;                        \___/ \___/
                                  length 1 length 1

      x   acute                ^     right       x        obtuse
     /     +ve                 |       0          \         -ve
    /                          |_                  \
   /                           | |                  \
  ----------&gt;                  +-----------&gt;         ---------------&gt;
  cos theta &gt; 0                cos theta = 0         cos theta &lt; 0

</pre>
</div>
</div>
<div id="org543cc07" class="outline-4">
<h4 id="org543cc07">Predictor fn: Hyperplane,</h4>
<div class="outline-text-4" id="text-org543cc07">
<p>
Given a linear predictor function f(x) = w . x + alpha,
the decision boundary is like
</p>

<pre class="example">

Predictor fn = 0;
f(x) = 0;
w . x + alpha = 0;
H = {x : w . x = - alpha};
</pre>

<p>
The set H is called a <span class="underline">hyperplane</span>.    (a line in 2D, a plane in 3D.)
</p>

<p>
[I want you to understand what a hyperplane is.  In 2D, it's a line.  In 3D,
it's a plane.  Now take that concept and generalize it to higher dimensions.
In d dimensions, a hyperplane is a flat, infinite thing with dimension d - 1.
A hyperplane divides the d-dimensional space into two halves.]
</p>
</div>
</div>

<div id="org42e75fa" class="outline-4">
<h4 id="org42e75fa">Hyperplane: Normal vector</h4>
<div class="outline-text-4" id="text-org42e75fa">
<pre class="example">

Theorem:
Let xy be a vector that lies on H.  Then w . (y - x) = 0.
Proof:
x and y lie on H.  Thus w . (x - y) = - alpha - (- alpha) = 0.

w is called the _normal_vector_ of H,
because (as the theorem shows) w is normal (perpendicular) to H.
(I.e. w is normal to every pair of points in H.)
</pre>


<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_13-38-48.png" alt="screenshot_2017-05-01_13-38-48.png" />
</p>
</div>
</div>
</div>

<div id="org1ece4c7" class="outline-4">
<h4 id="org1ece4c7">Normal vector: Signed Distance</h4>
<div class="outline-text-4" id="text-org1ece4c7">
<p>
If w is a unit vector, then w . x + alpha is the <span class="underline">signed_distance</span> from x to H.
I.e. it's the distance, but positive on one side of H; negative on other side.
这个很好理解，对于一个平面和一个点，我如何求这个点到这个平面的距离呢？我首先求这个平面的垂直向量，然后连线这个点与原点，这条线会穿过这个平面形成一个交点。这个点与交点之间的线段在这个垂直向量上的映射就是这个点到这个平面的距离。
</p>

<pre class="example">
或者这样理解，函数 w.x = y
当 w.x = y 在 2 维时，w.x=常量 是一条线;
当 w.x = y 在 3 维时，w.x=常量 是一个面;
按照'上加下减，左加右减'的原则
w.x=0 形成的平面 向上平移 1 变成 w.x=1
w.x=0 形成的平面 向下平移 1 变成 w.x=-1

</pre>

<p>
所以 signed distance of x' to H（因为通过升维法,H 始终可以表示为 w.x=0）, 就是 H 经过平移
alpha 距离得到的平面 H' 恰好包含 x'
</p>

<p>
Moreover, the distance from H to the origin is alpha.  [How do we know that?]
</p>

<p>
Hence alpha = 0 if and only if H passes through origin.
</p>

<p>
[w does not have to be a unit vector for the classifier to work.
If w not unit vector, w . x + alpha is a multiple of signed distance.
If you want to fix that, you can <span class="underline">rescale</span> the equation
by computing |w| and dividing both w and alpha by 1 / |w|.]
</p>
</div>
</div>

<div id="orgf5536cc" class="outline-4">
<h4 id="orgf5536cc">Signed Distance: Weight</h4>
<div class="outline-text-4" id="text-orgf5536cc">
<p>
The coefficients in w, plus alpha, are called <span class="underline">weights</span> or sometimes
<span class="underline">regression_coefficients</span>.
</p>

<p>
[That's why I named the vector w; "w" stands for "weights".]
</p>

<p>
The input data is <span class="underline">linearly_separable</span> if there exists a hyperplane that
separates all the samples in class O from all the samples NOT in class O.
</p>

<p>
[At the beginning of this lecture, I showed you one plot that's linearly
separable and two that are not.]
</p>

<p>
[We will investigate some linear classifiers that only work for linearly
separable data, then we'll move on to more sophisticated linear classifiers
that do a decent job with non-separable data.  Obviously, if your data is not
linearly separable, a linear classifier cannot do a <b>perfect</b> job.  But we're
still happy if we can find a classifier that usually predicts correctly.]
</p>
</div>
</div>
</div>

<div id="org5ca24c6" class="outline-3">
<h3 id="org5ca24c6">A Stupid Classifier</h3>
<div class="outline-text-3" id="text-org5ca24c6">
</div>
<div id="org6dbb873" class="outline-4">
<h4 id="org6dbb873">Centroid method</h4>
<div class="outline-text-4" id="text-org6dbb873">
<p>
compute mean mu_C of all vectors in class O and
        mean mu_X of all vectors NOT in O.
</p>

<p>
We use the predictor function
</p>


<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_14-48-06.png" alt="screenshot_2017-05-01_14-48-06.png" />
</p>
</div>

<pre class="example">
                                            mu_C + mu_X
f(x) = (mu_C - mu_X) . x - (mu_C - mu_X) . (-----------)
                                                 2

       \___________/                       \___________/
       normal vector                 midpoint between mu_C, mu_X

</pre>

<p>
so that decision boundary is the hyperplane that bisects line segment
w/endpoints mu_C, mu_X.
</p>

<p>
[Better yet, we can adjust the right hand side to minimize the number of
 misclassified points.  Same normal vector, but different position.]
</p>

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_16-23-46.png" alt="screenshot_2017-05-01_16-23-46.png" />
</p>
</div>

<p>
[In this example, there's clearly a better linear classifier that classifies
 every sample correctly.
 Note that this is hardly the worst example I could have given.
 If you're in the mood for an easy puzzle, pull out a sheet of paper and think
 of an example, with lots of samples, where the centroid method misclassifies
 every sample but one.]
 Only better for Gaussian distribution
 [Nevertheless, there are cases where this method works well, like when all your
 positive examples come from one Gaussian distribution, and all your negative
 examples come from another.]
</p>
</div>
</div>
</div>

<div id="org93af924" class="outline-3">
<h3 id="org93af924">Perceptron Algorithm</h3>
<div class="outline-text-3" id="text-org93af924">
<p>
(Frank Rosenblatt, 1957)
</p>
<hr />
<p>
Slow, but correct for linearly separable samples.
Uses a <span class="underline">numerical_optimization</span> algorithm, namely, <span class="underline">gradient_descent</span>.
</p>
</div>
<div id="org56ad49b" class="outline-4">
<h4 id="org56ad49b">Key questions of Numerical Optimization</h4>
<div class="outline-text-4" id="text-org56ad49b">
<p>
How many of you know what numerical optimization is?
How many of you know what gradient descent is?
How many of you know what Lagrange multipliers are?
How many of you know what linear programming is?
How many of you know what the simplex algorithm for linear programming is?
How many of you know what convex programming is?
</p>

<p>
We're going to learn what all these things are.  As machine learning people,
we will be heavy users of all the optimization methods.  Unfortunately,
I won't have time to teach you <b>algorithms</b> for all these optimization
problems, but we'll learn a few, and I'll give you some hints how the other
algorithms work.
</p>
</div>
</div>
<div id="orgec1e83f" class="outline-4">
<h4 id="orgec1e83f">Represent a sample: a row of features</h4>
<div class="outline-text-4" id="text-orgec1e83f">
<p>
一行就是一个 sample，一列就是一种 feature。
</p>

<pre class="example">
        feature1  feature2 feature3 feature4 feature5
        +--------+--------+--------+--------+--------+
sample1 |        |        |        |        |        |
  X_1   |        |        |        |        |        |
        +--------+--------+--------+--------+--------+
sample2 |        |        |        |        |        |
  X_2   |        |        |        |        |        |
        +--------+--------+--------+--------+--------+
sample3 |        |        |        |        |        |
  X_3   |        |        |        |        |        |
        +--------+--------+--------+--------+--------+
sample4 |        |        |        |        |        |
  X_4   |        |        |        |        |        |
        +--------+--------+--------+--------+--------+
sample5 |        |        |        |        |        |
  X_5   |        |        |        |        |        |
        +--------+--------+--------+--------+--------+
</pre>



<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_17-39-37.png" alt="screenshot_2017-05-01_17-39-37.png" />
</p>
</div>

<p>
[The reason I'm using capital X here is because we typically store these
 vectors as <span class="underline">rows of a matrix X</span>.  So the subscript picks out a <span class="underline">row</span> of X,
 representing a <span class="underline">specific sample</span>.  When I want to pick out <span class="underline">one feature</span> from
 a sample, I'll add <span class="underline">a second subscript</span> after the first one.]
</p>
</div>
</div>

<div id="org1ecba94" class="outline-4">
<h4 id="org1ecba94">Represent Label</h4>
<div class="outline-text-4" id="text-org1ecba94">

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_17-40-23.png" alt="screenshot_2017-05-01_17-40-23.png" />
</p>
</div>

<p>
For simplicity, consider only decision boundaries that pass through the origin.
(We'll fix this later.)                                <code>~~~~~~~~~~~~~~~~~~~~~</code>
</p>
</div>
</div>

<div id="org5a48fdb" class="outline-4">
<h4 id="org5a48fdb">Goal:  find weights w</h4>
<div class="outline-text-4" id="text-org5a48fdb">
<p>
That is, if sample 'i' is in class 'O', then we want a positive signed
distantce to that point from the hyperplane; else, we want a negative one.
</p>


<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_17-06-24.png" alt="screenshot_2017-05-01_17-06-24.png" />
</p>
</div>
</div>
</div>

<div id="org360bcb5" class="outline-4">
<h4 id="org360bcb5">Get constraint for optimization</h4>
<div class="outline-text-4" id="text-org360bcb5">
<p>
A trick make 2 inequation to 1 inequation.
</p>

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_17-41-45.png" alt="screenshot_2017-05-01_17-41-45.png" />
</p>
</div>

<p>
constraint 是把一个 machine learning problem 转化为 optimization problem 问题的前提
</p>
</div>
</div>

<div id="org1efeacc" class="outline-4">
<h4 id="org1efeacc">Define Loss function</h4>
<div class="outline-text-4" id="text-org1efeacc">
<p>
Define the <span class="underline">loss_function</span>
</p>

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-01_17-42-11.png" alt="screenshot_2017-05-01_17-42-11.png" />
</p>
</div>

<ul class="org-ul">
<li>yi is our prediction;</li>
<li>z  is correct classification.</li>
</ul>

<p>
注意 if y_i z &lt; 0 那么 -y_i z &gt; 0 的， 所以这个 loss fn 是越接近于 0
越好，如果是正值且很大说明不好，分的不是很理想。
</p>

<p>
[Here, z is the classifier's prediction, and y_i is the correct answer.]
</p>

<p>
Idea:  if z has the same sign as y_i, the loss function is zero (happiness).
But if z has the wrong sign, the loss function is positive.
</p>

<p>
[For each sample, you want to get the loss function down to zero, or as close
to zero as possible.  It's called the "loss function" because the bigger it
is, the bigger a loser you are.]
</p>
</div>
</div>

<div id="org1bd0703" class="outline-4">
<h4 id="org1bd0703">Define risk function</h4>
<div class="outline-text-4" id="text-org1bd0703">
<p>
risk 是标准浮动的空间, 简单说 R(w) = sum(L(测，真))
loss 是测试结果和现实情况之间的误差度量，如果结果都在 hyperplane 同一边，L(测，真) = 0;
如果结果分在 hyperplane 的两边，L(测，真) = -yi*z. = -(X_i.w)*z
</p>

<p>
Idea:
We define a risk_function R that is <span class="underline">positive</span> if some constraints are <span class="underline">violated</span>.
</p>

<p>
Then we use <span class="underline">optimization</span> to choose w that <span class="underline">minimizes R</span>.
</p>

<p>
Define <span class="underline">risk_function</span> (aka <span class="underline">objective_function</span> or <span class="underline">cost_function</span>)
</p>

<p>
<img src="Machine Learning/screenshot_2017-05-01_17-42-41.png" alt="screenshot_2017-05-01_17-42-41.png" />
可以看到，constraint fn 与 risk fn 在这里会师了。
R(w) = SUM(constraint violated)
Min( R(w) ) = Min( SUM(constraint violated))  &lt;- this is our target
</p>

<p>
<img src="Machine Learning/screenshot_2017-05-01_18-03-54.png" alt="screenshot_2017-05-01_18-03-54.png" />
Plot of risk R(w). Every point in the dark green flat spot is a minimum.
We’ll look at this more next lecture
</p>

<p>
If w classifies all the points X_1, ..., X_n correctly, then R(w) = 0.
Otherwise, R(w) is positive, and we want to find a better value of w.
</p>

<pre class="example">
--------------------------------------------------------------------------
| Goal:  Solve this _optimization_problem_:  find w that minimizes R(w). |
--------------------------------------------------------------------------
</pre>
</div>
</div>

<div id="org4b5ff26" class="outline-4">
<h4 id="org4b5ff26">Summarize</h4>
<div class="outline-text-4" id="text-org4b5ff26">
<p>
Through the semester what you're often going to do is take some complex
machine learning problem and reduce it to an optimazation problem which
you can throw a well known mathmatical tech.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 五</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 日 05:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
