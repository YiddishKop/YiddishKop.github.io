<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 æ—¥ 05:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 19 Heuristics for Avoiding Bad Local Minima</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 19 Heuristics for Avoiding Bad Local Minima</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org82fde07">lec 19 Heuristics for Avoiding Bad Local Minima (continued)</a>
<ul>
<li><a href="#org04049ef">Heuristics to Avoid Overfitting</a></li>
</ul>
</li>
<li><a href="#orgfbe3a0b">lec 19.2 CONVOLUTIONAL NEURAL NETWORKS (ConvNets)</a></li>
</ul>
</div>
</div>
<div id="org82fde07" class="outline-2">
<h2 id="org82fde07">lec 19 Heuristics for Avoiding Bad Local Minima (continued)</h2>
<div class="outline-text-2" id="text-org82fde07">
<hr />
<ul class="org-ul">
<li>Train several nets; pick best.
[If you train 10 neural nets on the same data, but each starting with
 different random weights, it's unlikely that any two of them will end up
 the same.  Some might fall into bad local minima, but probably not all.]</li>

<li><span class="underline">Layerwise_pretraining</span>.
[We train one layer of the neural net at a time, starting from the input
 layer.  The goal is to get the first layer to develop useful features, then
 for the second layer to develop even more useful features, and so on.
 This is a complicated topic on the forefront of research that I don't have
 time to do justice to.  If you're interested, do a search.]</li>
</ul>

<p>
Heuristics for Faster Training
</p>
<hr />
<p>
[One of the biggest disadvantages of neural nets is that they take a long, long
 time to train compared to most other classification methods we've studied.
 Here are some ideas for how to speed them up.  Unfortunately, you often have
 to experiment with techniques and parameters for a while to figure out which
 ones will help with your particular application.]
</p>

<ul class="org-ul">
<li>(1), (2), (3), (4) above.</li>

<li>Stochastic gradient descent faster than batch on large, redundant data sets.
[For example, if you have many different examples of the number "9", they
 contain some redundant information, and stochastic gradient descent learns
 the redundant information quickly.]
[Show 3-weight neural net and the convergence for batch and stochastic
 gradient descent (nnet2d.pdf, batchnnet.pdf, stochasticnnet.pdf).]
One <span class="underline">epoch</span> presents every training example once.  Training usually takes
many epochs, but if data is huge it can take less than one.</li>

<li><span class="underline">Normalizing</span> the data:</li>
</ul>
<p>
--- First <span class="underline">center</span> each feature so mean is zero.
--- Then scale each feature so variance is constant (~1 is good).
  [The first step seems to make it easier for hidden units to get into a good
   operating region of the sigmoid.  The second step makes the objective
   function better conditioned, so gradient descent converges faster.]
  [Show example of slow convergence of steepest descent on an objective
   function with an ill-conditioned Hessian (illcondition.jpg).]
  [Show 2D example of data normalization (normalize.jpg).]
</p>

<ul class="org-ul">
<li>Centering the hidden units helps too.
Replace sigmoids with 2 s(gamma) - 1 or with tanh gamma.
[These functions range from -1 to 1 instead of from 0 to 1.]
[If you do this, don't forget that you also need to change s' in backprop.
 Also, good output target values change to roughly -0.7 and 0.7.]</li>

<li>Use different learning rate for each layer of weights:
earlier layers have smaller gradients, need larger learning rate.
[Show second derivatives at different layers (curvaturelayers.pdf).]</li>

<li><span class="underline">Emphasizing_schemes</span>:
[Networks learn fastest from the most unexpected examples.  They learn
 slowest from the most redundant examples.  So we try to emphasize the
 uncommon examples.]</li>
</ul>
<p>
--- Shuffle so successive examples are never/rarely same class.
--- Present examples from rare classes more often, or w/bigger epsilon.
--- Warning:  can backfire on bad outliers.

</p>
<ul class="org-ul">
<li>Second-order optimization</li>
</ul>
<p>
--- No Newton's method; Hessian too expensive.
--- Nonlinear conjugate gradient; for small nets + small data + regression.
    Batch descent only!  -&gt;  Too slow with redundant data.
--- Stochastic Levenberg Marquardt; approximates a diagonal Hessian.
    [The authors claim convergence is typically three times faster than
     well-tuned stochastic gradient descent.  The algorithm is complicated and
     I don't have time to cover it.]
</p>
</div>

<div id="org04049ef" class="outline-3">
<h3 id="org04049ef">Heuristics to Avoid Overfitting</h3>
<div class="outline-text-3" id="text-org04049ef">
<hr />
<ul class="org-ul">
<li>Ensemble of neural nets.  Random initial weights; bagging.
[We saw how well ensemble learning works for decision trees.
 It works really well for neural nets too.  The combination of random initial
 weights and bagging helps ensure that each neural net comes out different,
 even though they're trained on the same data.  Obviously, this is slow.]</li>

<li>L_2 regularization, aka <span class="underline">weight_decay</span>.
Add lambda |w|^2 to the cost/loss fn, where w is vector of all weights.
[Thus w includes all the weights in matrices V and W, rewritten as a vector.]
[We do this for the same reason we do it in ridge regression:
 penalizing large weights reduces overfitting by reducing the variance.]
Effect:  -dJ/dw_i has extra term -2 lambda w_i
         Weight decays by factor 1 - 2 lambda if not reinforced by training.
[Show examples of 2D classification without and with weight decay
 (weightdecayoff.pdf, weightdecayon.pdf).  "softmax + logistic loss".
 Observe that second example comes close to the Bayes rule.]
[One of the tricky parts of neural nets is deciding how many hidden units
 there should be.  If there's too few, you can't learn very well, but if
 there's too many, they tend to overfit.  L_2 regularization and weight decay
 make it safer to have too many hidden units, so it's less critical to find
 just the right number.]</li>

<li><span class="underline">Dropout</span> emulates an ensemble in one network.  (Faster training too.)
[Show figure of neural net with dropout (dropout.pdf).]
[During training, we temporarily disable a random subset of the units, along
 with all the edges in and out of those units.  It seems to work well to
 disable each hidden unit with probability 0.5, and to disable input units
 with a smaller probability.  We do stochastic gradient descent and we
 frequently change which random subset of units is disabled.  The authors
 claim that their method generalizes better than L_2 regularization.
 It gives some of the advantages of an ensemble, but it's faster to train.]</li>
</ul>
</div>
</div>
</div>

<div id="orgfbe3a0b" class="outline-2">
<h2 id="orgfbe3a0b">lec 19.2 CONVOLUTIONAL NEURAL NETWORKS (ConvNets)</h2>
<div class="outline-text-2" id="text-orgfbe3a0b">
<p>
<code>===========================</code>
[Convolutional neural nets have caused a big resurgence of interest in neural
 nets in the last few years.  Often you'll hear the buzzword <span class="underline">deep_learning</span>,
 which refers to neural nets with many layers.]
</p>

<p>
Vision:  inputs are large images.  200 x 200 image = 40,000 pixels.
If we connect them all to 40,000 hidden units -&gt; 1.6 billion connections.
Neural nets are often <span class="underline">overparametrized</span>:  too many weights, too little data.
[As a rule of thumb, you should have more data than you have weights to
 estimate.  If you don't follow that rule, you usually overfit very very badly.
 With images, it's impossible to get enough data to correctly train billions of
 weights.  We could shrink the images, but then we're throwing away useful
 data.  Another problem with having billions of weights is that the network
 becomes very slow to train or even to use.]

[Researchers have addressed these problems by taking inspiration from the
 neurology of the visual system.  Remember that early in the semester, I told
 you that you can get better performance on the handwriting recognition task
 by using edge detectors.  Edge detectors have two interesting properties.
 First, each edge detector looks at just one small part of the image.  Second,
 the edge detection computation is the same no matter which part of the image
 you apply it to.  So let's apply these two properties to neural net design.]
</p>

<p>
ConvNet ideas:
</p>
<pre class="example">
(1)  Local connectivity:  A hidden unit (in early layer) connects only to small
     patch of units in previous layer.
     [This improves the overparametrization problem, and speeds up the network
      considerably.]
(2)  Shared weights:  Groups of hidden units share same set of input weights,
     called a _mask_ aka _filter_ aka _kernel_.  We learn several masks.
     [Each mask operates on every patch of image.]
     Masks x patches = hidden units (in first hidden layer).
     If one patch learns to detect edges, *every* patch has an edge detector.
     ConvNets automatically exploit repeated structure in images, audio.
     _Convolution_:  the same linear transformation applied to different parts
     of the input by shifting.
     [Shared weights improve the overparametrization problem even more, because
      shared weights means fewer weights.  It's a sort of regularization.
     [But shared weights have another big advantage.  Suppose that gradient
      descent starts to develop an edge detector.  That edge detector is being
      trained on *every* part of every image, not just on one spot.  And that's
      good, because edges appear at different locations in different images.
      The location no longer matters; the edge detector can learn from edges in
      every part of the image.]
</pre>

<p>
[In a neural net, you can think of hidden units as features that we learn, as
 opposed to features that you code up yourself.  Convolutional neural nets
 take them to the next level by learning features from multiple patches
 simultaneously and then applying those features everywhere, not just in the
 patches where they were originally learned.]
[By the way, although local connectivity is inspired by our visual systems,
 shared weights obviously don't happen in biology.]
</p>

<p>
[Go to slides on computing in the visual cortex and ConvNets (cnn.pdf).]
</p>

<p>
[Neurologists can stick needles into individual neurons in animal brains.
 After a few hours the neuron dies, but until then they can record its action
 potentials.  In this way, biologists quickly learned how some of the neurons
 in the retina, called <span class="underline">retinal_ganglion_cells</span>, respond to light.  They have
 interesting <span class="underline">receptive_fields</span>, illustrated in the slides, which show that
 each ganglion cell receives excitatory stimulation from receptors in a small
 patch of the retina but inhibitory stimulation from other receptors around
 it.]
</p>

<p>
[The signals from these cells propagate to the V1 visual cortex in the
 occipital lobe at the back of your skull.  These cells proved much harder to
 understand.  David Hubel and Torsten Wiesel of the Johns Hopkins University
 put probes into the V1 visual cortex of cats, but they had a very hard time
 getting any neurons to fire there.  However, a lucky accident unlocked the
 secret and ultimately won them the 1981 Nobel Prize in Physiology.]
</p>

<p>
[Show YouTube movie:  <a href="https://www.youtube.com/watch?v=IOHayh06LJ4">https://www.youtube.com/watch?v=IOHayh06LJ4</a> ]
</p>

<p>
[The glass slide happened to be at the particular orientation the neuron was
 sensitive to.  The neuron doesn't respond to other orientations; just that
 one.  So they were pretty lucky to catch that.]

[The <span class="underline">simple_cells</span> act as line detectors and/or edge detectors by taking
 a linear combination of inputs from retinal ganglion cells.]
[The <span class="underline">complex_cells</span> act as location-independent line detectors by taking
 inputs from many simple cells.]
</p>

<p>
[Later researchers showed that local connectivity runs through the V1 cortex
 by projecting certain images onto the retina and using radioactive tracers in
 the cortex to mark which neurons had been firing.  Those images show that the
 neural mapping from the retina to V1 is "retinatopic", i.e., local.]
</p>

<p>
[Unfortunately, as we go deeper into the visual system, layers V2 and V3 and
 so on, we know less and less about what processing the visual cortex does.]
</p>

<p>
[ConvNets were first popularized by the success of Yann LeCun's "LeNet 5"
 handwritten digit recognition software.  LeNet 5 has six hidden layers!
 Layers 1 and 3 are <span class="underline">convolutional_layers</span> in which groups of units share
 weights.  Layers 2 and 4 are <span class="underline">pooling_layers</span> that make the image smaller.
 These are just hardcoded max-functions with no weights and nothing to train.
 Layers 5 and 6 are just regular layers of hidden units with no shared weights.
 A great deal of experimentation went into figuring out the number of layers
 and their sizes.  At its peak, LeNet 5 was responsible for reading the zip
 codes on 10% of US Mail, and another Yann LeCun system was deployed in ATMs
 and check reading machines and was reading 10 to 20% of all the checks in the
 US by the late 90's.]
</p>

<p>
[When ConvNets were first applied to image analysis, researchers found that
 some of the learned masks are edge detectors or line detectors, similar to
 the ones that Hubel and Wiesel discovered!  This created a lot of excitement
 in both the computer learning community and the neuroscience community.
 The fact that a neural net can naturally learn the same features as the
 mammalian visual cortex is impressive.]
</p>

<p>
[I told you last week that neural nets research was popular in the 60's, but
 the 1969 book "Perceptrons" killed interest in them throughout the 70's.  They
 came back in the 80's, but interest was partly killed off a second time in the
 00's by...guess what?  By support vector machines.  SVMs work well for a lot
 of tasks, they're faster to train, and they more or less have only one
 hyperparameter, whereas neural nets take a lot of work to tune.]
[Neural nets are now in their third wave of popularity.  The single biggest
 factor in bringing them back is probably big data.  Thanks to the internet,
 we now have absolutely huge collections of images to train neural nets with,
 and researchers have discovered that neural nets often give better performance
 than competing algorithms when you have huge amounts of data to train them
 with.  In particular, convolutional neural nets are now learning better
 features than hand-tuned features.  That's a recent change.]
</p>

<p>
[One event that brought attention back to neural nets was the ImageNet Image
 Classification Challenge in 2012.  The winner of that competition was a neural
 net, and it won by a huge margin, about 10%.  It's called AlexNet, and it's
 surprisingly similarly to LeNet 5, in terms of how its layers are structured.
 However, there are some new innovations that led to their prize-winning
 performance, besides the fact that the training set had 1.4 million images:
 they used ReLUs, GPUs for training, and dropout.]
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 äº”</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 æ—¥ 05:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
