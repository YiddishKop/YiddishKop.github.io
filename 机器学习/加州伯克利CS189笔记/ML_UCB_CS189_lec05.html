<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 05:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 05 Hierarchical of ML and Optimization Problem</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 05 Hierarchical of ML and Optimization Problem</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5fbc58a">lec 05 Hierarchical of ML and Optimization Problem</a>
<ul>
<li><a href="#org49c5041">ML ABSTRACTIONS    [some meta comments on machine learning]</a></li>
<li><a href="#orgc1fb4bb">OPTIMIZATION PROBLEMS</a></li>
<li><a href="#orgf4e4a40">Unconstrained Optimization</a>
<ul>
<li><a href="#org02da76c">Algs for smooth f:</a></li>
<li><a href="#org243d202">Algs for nonsmooth f:</a></li>
</ul>
</li>
<li><a href="#org2a4b912">Constrained Optimization (smooth equality constraints)</a>
<ul>
<li><a href="#org7c584ef">Linear Program</a></li>
<li><a href="#org40c7eea">Quadratic program</a></li>
<li><a href="#org362e134">Convex Program (EE 127/227A/227B)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="org5fbc58a" class="outline-2">
<h2 id="org5fbc58a">lec 05 Hierarchical of ML and Optimization Problem</h2>
<div class="outline-text-2" id="text-org5fbc58a">
</div>
<div id="org49c5041" class="outline-3">
<h3 id="org49c5041">ML ABSTRACTIONS    [some meta comments on machine learning]</h3>
<div class="outline-text-3" id="text-org49c5041">
<p>
<code>=============</code>
[When you write a large computer program, you break it down into subroutines
 and modules.  Many of you know from experience that you need to have the
 discipline to impose strong abstraction barriers between different modules, or
 your program will become so complex you can no longer manage nor maintain it.]
</p>

<p>
[When you learn a new subject, it helps to have mental abstraction barriers,
 too, so you know when you can replace one approach with a different approach.
 I want to give you four levels of abstraction that can help you think about
 machine learning.  It's important to make mental distinctions between these
 four things, and the code you write should have modules that reflect these
 distinctions as well.]
</p>

<pre class="example">
-----------------------------------------------------------------------------
| APPLICATION/DATA                                                          |
|                                                                           |
| data labeled (classified) or not?                                         |
| yes: labels categorical (classification) or quantitative (regression)?    |
| no:  _similarity_ (clustering) or positioning (dimensionality reduction)? |
|---------------------------------------------------------------------------|
| MODEL                           [what kinds of hypotheses are permitted?] |
|                                                                           |
| e.g.:                                                                     |
| - predictor fns:  linear, polynomial, logistic, neural net, ...           |
| - nearest neighbors, decision trees(have no predictor fn)                 |
| - features                                                                |
| - low vs. high capacity (affects overfitting, underfitting, inference)    |
|---------------------------------------------------------------------------|
| OPTIMIZATION PROBLEM                                                      | a molde can be optimized by many OPTIMIZATION PROBLEM
|                                                                           | a OPTIMIZATION PROBLEM always can be expressed with
| - variables, objective fn, constraints                                    | 3 key-terms: var, obj, contr
| e.g., unconstrained, convex program, least squares, PCA                   |
|---------------------------------------------------------------------------|
| OPTIMIZATION ALGORITHM                                                    | an OPTIMIZATION PROBLEM can be solved by many
|                                                                           | OPTIMIZATION ALGORITHMS
| e.g., gradient descent, simplex, SVD                                      | (简单理解为求 极大/小值)
-----------------------------------------------------------------------------

</pre>

<p>
[In this course, we focus primarily on the middle two levels. As a data
 scientist, you might be given an <span class="underline">application</span>, and your challenge is to
 <span class="underline">turn it into an optimization problem</span> that we know how to solve. We'll talk
 a bit about optimization algorithms, but usually you'll use an optimization
 code that's faster and more robust than what you would write.
</p>

<p>
[The second level, the model, has a huge effect on the success of your
 learning algorithm.  Sometimes you can get a big improvement by tailoring the
 model or its features to fit the structure of your specific data.  The model
 also has a big effect on whether you overfit or underfit.  And if you want
 a model that you can interpret so you can do <span class="underline">inference</span>, the model has to
 be regular, not too complex.  Lastly, you have to pick a model that leads to
 an optimization problem that can be solved.  Some optimization problems are
 just too hard.]
</p>

<p>
[It's important to understand that when you change something in one level of
 this diagram, you probably have to change all the levels underneath it.
 If you switch from a linear classifier to a neural net, your optimization
 problem changes, and your optimization algorithm probably changes too.]
</p>

<p>
[Not all machine learning methods fit this four-level decomposition.
 Nevertheless, for everything you learn in this class, think about where it
 fits in this hierarchy.  If you don't distinguish which math is part of the
 model and which math is part of the optimization algorithm, this course will
 be very confusing for you.]
</p>
</div>
</div>

<div id="orgc1fb4bb" class="outline-3">
<h3 id="orgc1fb4bb">OPTIMIZATION PROBLEMS</h3>
<div class="outline-text-3" id="text-orgc1fb4bb">
<p>
<code>===================</code>
[I want to familiarize you with some types of optimization problems that can be
 solved reliably and efficiently, and the names of some of the optimization
 algorithms used to solve them.  An important skill for you to develop is to be
 able to go from an application to a well-defined optimization problem.]
</p>
</div>
</div>

<div id="orgf4e4a40" class="outline-3">
<h3 id="orgf4e4a40">Unconstrained Optimization</h3>
<div class="outline-text-3" id="text-orgf4e4a40">
<hr />
<p>
根据，AMATH301 课程提示：
unconstrained problem include:
</p>
<ul class="org-ul">
<li>Derivative based method
<ul class="org-ul">
<li>GD</li>
<li>fminsearch</li>
</ul></li>
<li>Derivative-free based method
<ul class="org-ul">
<li>golden section</li>
<li>successive parabolic interpolation</li>
</ul></li>
</ul>
<p>
2 kinds of methods to solve it.
</p>



<p>
Goal:  Find w that minimizes (or maximizes) a continuous fn f(w).
</p>

<p>
f is <span class="underline">smooth</span> if its gradient is continuous too.
</p>

<p>
A <span class="underline">global_minimum</span> of f is a value w such that f(w) &lt;= f(v) for every v.
A <span class="underline">local_minimum</span>   " "  " "   "   "   "    "    "      "
                   for every v in a tiny ball centered at w.
                   [In other words, you cannot walk downhill from w.]
</p>

<p>
这样的算法有 GD，SGD，simulated anealing, genetic algo
但是，GD/SGD 用于 continuous-fn,  后两者用于 不连续函数
</p>

<pre class="example">
 ^                  ---
 |--              --   --         ----          -
 |  --           -       --     --    --       -
 |    -         -          -   -        --   --
 |     -       -            - -           ---
 |      --   --              -             x
 |        ---                ^            /
 |         ^x---------- local minima ----/
-O---------+--------------------------------------&gt;
 |         |
    global minimum
</pre>

<p>
Usually, finding a local minimum is easy;
         finding the global minimum is hard. [or impossible]
</p>

<p>
Exception:  A function is <span class="underline">convex</span> if for every x, y in R^d,
the line connecting (x, f(x)) to (y, f(y)) does not go below f(x).
</p>

<pre class="example">
  ^                                                  -
f |-                                               --
  | o============================================o-
  |  --                                      ---
  |    ----                              ----
  |        -----                     ----
  |             ------          -----
  |                   ----------
 -O-o--------------------------------------------o-----&gt;
  | x                                            y
</pre>

<p>
E.g. perceptron risk fn is convex and nonsmooth.
不是 smooth 的，因为 perceptron 的 loss-fn 大概长这样子，因为是条件函数:
</p>

<pre class="example">
\                                 / 0        ; if yi 与 w.Xi 符号相同
 \                        L(x) = +
  \                               \ -yi w.Xi ; if yi 与 w.Xi 符号相反
   +-------------------
</pre>

<p>
[When you sum together convex functions, you always get a convex function.
 The perceptron risk function is a sum of convex loss functions.]
</p>

<p>
A [continuous] convex function [on a closed, convex domain] has either
</p>
<ul class="org-ul">
<li>no minimum (goes to -infinity), or</li>
<li>just one local minimum, or</li>
<li>a connected set of local minima that are all global minima with equal f.</li>
</ul>
<p>
[The perceptron risk function has the latter.]
[In the last two cases, if you walk downhill, you eventually converge to
 a global minimum.]

[However, there are many applications where you don't have a convex objective
 function, and your machine learning algorithm has to settle for finding a
 local minimum.  For example, neural nets try to optimize an objective function
 that has <b>lots</b> of local minima; they almost never find a global minimum.]
</p>
</div>

<div id="org02da76c" class="outline-4">
<h4 id="org02da76c">Algs for smooth f:</h4>
<div class="outline-text-4" id="text-org02da76c">
<ul class="org-ul">
<li>Steepest descent:
= blind [with learning rate]              repeat:  w &lt;- w - epsilon grad f(w)
= with line search:
  x Secant method
  x Newton-Raphson (may need Hessian matrix of f)
= stochastic (blind)   [trains on one sample per iteration, or a small batch]</li>
<li>Nonlinear conjugate gradient              [uses the same line search methods]
<ul class="org-ul">
<li>不是朝着 grad 反方向移动，通常认为这个方向未必是最好的方向。</li>
</ul></li>
<li>Newton's method (needs Hessian matrix)</li>
</ul>
</div>
</div>

<div id="org243d202" class="outline-4">
<h4 id="org243d202">Algs for nonsmooth f:</h4>
<div class="outline-text-4" id="text-org243d202">
<ul class="org-ul">
<li>Steepest descent
= blind
= with direct line search (e.g. golden section search)
  direct line search 也很适用于那些你只有数据，但是没有函数的情况。这时候用 direct line
  search 也同样可以得到一个 minimum。
  direct line search 并不是用 grad，因为 grad 在 nonsmooth-fn 中的结果并不十分可信。</li>
</ul>

<p>
These algs find a local minimum.
</p>

<p>
计算 global-minimum 耗费时间太多了。当然，如果函数是凸函数，那么
local-minimum is global-minimum
</p>

<p>
<span class="underline">line_search</span>:  finds a local minimum along the search direction by solving
                an optimization problem in 1D.
</p>
<p>
<img src="Machine Learning/screenshot_2017-05-03_17-20-09.png" alt="screenshot_2017-05-03_17-20-09.png" />
[...instead of using a blind step size like the perceptron algorithm does.
 Solving a 1D problem is much easier than solving a higher-dimensional one.]
</p>

<p>
Why line search fail, when fn is non-continuous?
</p>

<p>
Why GD can not used in saddle?
</p>

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-03_17-32-28.png" alt="screenshot_2017-05-03_17-32-28.png" />
</p>
</div>

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-03_17-32-50.png" alt="screenshot_2017-05-03_17-32-50.png" />
</p>
</div>

<div class="figure">
<p><img src="Machine Learning/screenshot_2017-05-03_17-35-42.png" alt="screenshot_2017-05-03_17-35-42.png" />
</p>
</div>

<p>
[Neural nets are <span class="underline">unconstrained optimization problems</span> with many, many local
 minima.  They sometimes benefit from the more sophisticated optimization
 algorithms, but when the input data set is very large, researchers often
 favor the dumb, blind, stochastic versions of gradient descent.]
</p>

<p>
[If you're optimizing over a d-dimensional space, the Hessian matrix is
 a d-by-d matrix and it's usually dense, so most methods that use the Hessian
 are computationally infeasible when d is large.]
</p>
</div>
</div>
</div>

<div id="org2a4b912" class="outline-3">
<h3 id="org2a4b912">Constrained Optimization (smooth equality constraints)</h3>
<div class="outline-text-3" id="text-org2a4b912">
<hr />
<p>
Goal:  Find w that minimizes (maximizes) f(w)
       subject to g(w) = 0              [&lt;- observe that this is an isosurface]
</p>

<p>
where g is a smooth fn
(may be vector, encoding multiple constraints)
</p>

<p>
Alg:  Use <span class="underline">Lagrange_multipliers</span>.
</p>
</div>

<div id="org7c584ef" class="outline-4">
<h4 id="org7c584ef">Linear Program</h4>
<div class="outline-text-4" id="text-org7c584ef">
<hr />
<p>
Linear objective fn + linear <b>inequality</b> constraints.
</p>

<p>
Goal:  Find w that maximizes (or minimizes) c . w
       subject to A w &lt;= b
</p>

<p>
where A is n-by-d matrix, b in R^n, expressing n <span class="underline">linear_constraints</span>:
A_i w &lt;= b_i,        i in [1, n]
</p>

<pre class="example">
              |    /        \            /
^           --+---O----------\----------/-------  &lt;-- active constraint
 \            |  /.optimum....\        /
  \ c         | /..............\      /
   \          |/................\    /
    \         /.....feasible.....\  /
     \       /|......region.......\/
            / |.................../\
           /  |................../  \
         -/---+-----------------/----\----
         /    |                /      \
        /
       /  &lt;-- active constraint
</pre>

<p>
The set of points that satisfy all constraints is a convex <span class="underline">polytope</span> called
the <span class="underline">feasible_region</span> F.  The <span class="underline">optimum</span> is the point in F that is furthest in
the direction c.  [What does convex mean?]  A point set P is <span class="underline">convex</span> if
for every p, q in P, the line segment with endpoints p, q lies entirely in P.
</p>

<p>
[A polytope is just a polyhedron, generalized to higher dimensions.]
</p>

<p>
We can always find a global optimum that is a vertex of the polytope.
</p>

<p>
The optimum achieves equality for some constraints (but not most), called the
<span class="underline">active_constraints</span> of the optimum.
</p>

<p>
[In the figure above, there are two active constraints.  In an SVM, active
 constraints correspond to the samples that touch or violate the slab.]
</p>

<p>
[Sometimes, there is more than one optimal point.  For example, in the figure
 above, if c pointed straight up, every point on the top horizontal edge would
 be optimal.  The set of optimal points is always convex.]
</p>

<p>
Example:  EVERY <span class="underline">feasible_point</span> (w, alpha) gives a linear classifier:
</p>

<pre class="example">
----------------------------------------------------------------
| Find w, alpha that maximizes 0                               |
| subject to y_i (w . X_i + alpha) &gt;= 1    for all i in [1, n] |
----------------------------------------------------------------
</pre>

<pre class="example">
IMPORTANT:
The data are linearly separable iff the feasible region is not the empty set.
        ^
        |----- Also true for maximum margin classifier (quadratic program)
</pre>

<p>
Algs for linear programming:
</p>
<ul class="org-ul">
<li>Simplex (George Dantzig, 1947)
[Indisputibly one of the most important algorithms of the 20th century.]
[Walks along edges of polytope from vertex to vertex until it finds optimum.]</li>
<li>Interior point methods</li>
</ul>

<p>
[Linear programming is very different from unconstrained optimization; it has
 a much more combinatorial flavor.  If you knew which constraints would be the
 active constraints once you found the solution, it would be easy; the hard
 part is figuring out which constraints should be the active ones.  There are
 exponentially many possibilities, so you can't afford to try them all.
 So linear programming algorithms tend to have a very discrete, computer
 science feeling to them, like graph algorithms, whereas unconstrained
 optimization algorithms tend to have a continuous, numerical optimization
 feeling.]
</p>

<p>
[Linear programs crop up everywhere in engineering and science, but they're
 usually in disguise.  An extremely useful talent you should develop is to
 recognize when a problem is a linear program.]
</p>

<p>
[A linear program solver can find a linear classifier, but it can't find the
 maximum margin classifier.  We need something more powerful.]
</p>
</div>
</div>

<div id="org40c7eea" class="outline-4">
<h4 id="org40c7eea">Quadratic program</h4>
<div class="outline-text-4" id="text-org40c7eea">
<hr />
<p>
Quadratic, convex objective fn + linear inequality constraints.
</p>

<pre class="example">
Goal:  Find w that minimizes f(w) = w^T Q w + c^T w
       subject to A w &lt;= b

       where Q is a symmetric, positive definite matrix.

A matrix is _positive_definite_ if w^T Q w &gt; 0 for all w != 0.
</pre>

<p>
Only one local minimum!
</p>

<p>
[If Q is indefinite, so f is not convex, then the minimum is not unique and
 quadratic programming is NP-hard.]
</p>

<p>
Example:  Find maximum margin classifier.
</p>
<p>
<img src="Machine Learning/screenshot_2017-05-03_19-06-46.png" alt="screenshot_2017-05-03_19-06-46.png" />
[Show circular contour; draw polygon on top; show constrained minimum.
 In an SVM, we are looking for the point in this polygon that's closest to
 the origin.  Example with one active constraint; example with two.]
</p>

<p>
Algs for quadratic programming:
</p>
<ul class="org-ul">
<li>Simplex-like</li>
<li>Sequential minimal optimization (SMO, used in LIBSVM)</li>
<li>Coordinate descent (used in LIBLINEAR)</li>
</ul>

<p>
[One clever idea used in SMO is that they do a line search that uses the
 Hessian, but it's cheap to compute because they don't walk in the direction of
 steepest descent; instead they walk along just one or a few coordinate axes at
 a time.]
</p>
</div>
</div>

<div id="org362e134" class="outline-4">
<h4 id="org362e134">Convex Program (EE 127/227A/227B)</h4>
<div class="outline-text-4" id="text-org362e134">
<hr />
<p>
Convex objective fn + convex inequality constraints.
</p>

<p>
[What I've given you here is, roughly, a sliding scale of optimization problems
 of increasing complexity, difficulty, and computation time.  But even convex
 programs are relatively easy to solve.  When you're trying to address the
 needs of real-world applications, it's not uncommon to devise an optimization
 problem with crazy inequalities and an objective function that's nowhere near
 convex.  These are sometimes very, very hard to solve.]
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 五</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 日 05:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
