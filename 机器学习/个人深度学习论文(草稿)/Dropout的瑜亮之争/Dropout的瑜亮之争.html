<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 05:36 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Dropout的瑜亮之争</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddishkop" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">Dropout的瑜亮之争</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org7826efc">一 导言</a></li>
<li><a href="#orgc877b82">二 相关技术</a>
<ul>
<li><a href="#orgaa0846a">2.1 DropConnect(ICML 2013)</a></li>
<li><a href="#org28f4fba">2.2 Dense-Sparse-Dense training(ICLR 2017)</a></li>
<li><a href="#orgb0e7f42">2.3 Adaptive dropout</a></li>
</ul>
</li>
<li><a href="#org03661df">三 实验设计</a>
<ul>
<li><a href="#org4d6a2cf">测试数据集：Cifar10</a></li>
<li><a href="#org32937f8">预处理</a></li>
<li><a href="#org1132bd4">架构</a></li>
<li><a href="#orgd1b13ec">描述</a></li>
<li><a href="#org7803d78">源代码链接</a></li>
<li><a href="#org0a69773">使用方法</a></li>
</ul>
</li>
<li><a href="#orgc3d5840">四 实验结果</a>
<ul>
<li><a href="#org8c92400">实验一</a></li>
<li><a href="#orgc40291f">实验二</a></li>
<li><a href="#org3eaa4f6">实验三</a></li>
</ul>
</li>
<li><a href="#org2d14eea">五 结论</a></li>
<li><a href="#orgd91f9b1">六 附录</a></li>
<li><a href="#orgb182f69">七 索引</a></li>
</ul>
</div>
</div>


<div id="org7826efc" class="outline-2">
<h2 id="org7826efc">一 导言</h2>
<div class="outline-text-2" id="text-org7826efc">
<p>
类神经网络是由一系列的 <code>connection</code> 和 <code>node</code> 组成，某一层的 <code>node</code> 数量越多，与它相关的 <code>connection</code> 的数量通常也会随着增加。当网络叠深、变复杂的时候，参数的数量也会大量增加，这时候就需要考虑 <code>over-fitting</code> 的问题。
</p>

<p>
传统的机器学习常会使用 <code>regularization term</code> 来维护 <code>validation</code> 和 <code>testing</code> 的品质。在深度学习中，除了 <code>regularization</code> ， <code>dropout</code> 和 <code>drop-connect</code> 也是常见的架构。
</p>

<p>
 <code>dropout</code> <sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>的概念，在于训练时以一定机率(drop-rate)把网络中的某些 <code>node</code> 的数值舍弃，即归零。 <code>drop-connect</code> 也使用相似的方法，但它舍弃的是 <code>connection</code> 而非=node= 。上述两个方法，都有随机的成份在，亦即当 <code>drop-rate</code> 定好之后，每个
<code>node</code> 或 <code>connection</code> ，都有可能被舍弃，不论它们的数值大小、正负。
</p>

<p>
针对这一个部分，另一个架构： <code>Dense-Sparse</code> <sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>给了不一样的观点。
<code>connection</code> 的绝对值大小，其实某种程度上说明了它的重要性，代表该层的 <code>node</code> 对下一层某个 <code>node</code> 的影响程度。既然我为了避免 <code>over-fitting</code> ，需要减少
<code>connection</code> 的数量，那么何不直接去掉 <b>影响程度较低</b> 的那些 <code>connection</code> 呢？比起 <b>随机丢弃</b> ，丢弃较不重要的部分可能会是一个更好的作法。经过实验之后，
<code>Dense-Sparse</code> 确实能够在减少 <code>connection</code> 的状态下，训练集错误率不至于上升而且
<code>validation</code> 的表现更好一些。
</p>

<p>
既然 <code>connection</code> 不需要随机丢弃，那么 <code>node</code> 是不是也能用类似的方法，取代=
 随机= 舍弃呢？此外，除了探讨 <code>dropout</code> 和 <code>drop-connect</code> 的随机性之外，我也想尝试一些 <code>dropout</code> 的其他方法，看看能不能让表现更好, 比如:
</p>

<ol class="org-ol">
<li>是否只有 <code>fully-connect</code> 层适合使用 <code>dropout</code> ？ <code>over-fitting</code> 和 <code>early-stopping</code> 呢 ？</li>
<li>如 果让 <code>drop-rate</code> 随着 <code>epoch</code> 增加而增加，会不会又有更好的表现？</li>
<li>既然 <code>drop-rate</code> 只是一个数值，能否用另一个类神经网络或函数，来"学"出某一层的
<code>drop-rate</code> ？比起自己调整，让网络变成 <code>end-to-end</code> 训练会不会更好？</li>
</ol>
</div>
</div>

<div id="orgc877b82" class="outline-2">
<h2 id="orgc877b82">二 相关技术</h2>
<div class="outline-text-2" id="text-orgc877b82">
</div>
<div id="orgaa0846a" class="outline-3">
<h3 id="orgaa0846a">2.1 DropConnect(ICML 2013)<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup></h3>
<div class="outline-text-3" id="text-orgaa0846a">
<p>
<a href="http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/">http://fastml.com/regularizing-neural-networks-with-dropout-and-with-dropconnect/</a>
</p>

<p>
经过上面介绍可知Dropout是随机丢弃Node，使得随机node输出为零，而DropConnect则是随机丢弃link使得某node的输入的某些权值为零，示意如下：
</p>


<div class="figure">
<p><img src="一、Introduction/screenshot_2018-08-09_02-09-02.png" alt="screenshot_2018-08-09_02-09-02.png" />
</p>
</div>


<p>
Dropout的公式：
</p>

<p>
\(r = m\dot{ a }(Wv)\)
</p>

<p>
Drop-Connect公式：
</p>

<p>
\(r = a((M \dot { W })v)\)
</p>


<p>
其中
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">参数</th>
<th scope="col" class="org-left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">m</td>
<td class="org-left">该node要不要被drop值为0或1; W0为bias</td>
</tr>

<tr>
<td class="org-left">W</td>
<td class="org-left">该layer的weight</td>
</tr>

<tr>
<td class="org-left">x</td>
<td class="org-left">上一层全部node所结合成的vector</td>
</tr>

<tr>
<td class="org-left">M</td>
<td class="org-left">为一个mask决定哪些weight该被drop</td>
</tr>

<tr>
<td class="org-left">a</td>
<td class="org-left">为activation function.</td>
</tr>
</tbody>
</table>


<p>
在training方面与Dropout很相似，流程如下：
</p>


<div class="figure">
<p><img src="一、Introduction/screenshot_2018-08-09_02-10-51.png" alt="screenshot_2018-08-09_02-10-51.png" />
</p>
</div>


<p>
因为Drop Connect建议只使用在Fully Connected Layer，若是整个Neural Network含有
Convolution Layer 或是Pooling Layer的话，该层应该放在Extract feature的地方。
</p>

<p>
在Inference方面，Dropout是把剩余node的weight全部scale up一个系数，例如：假设Keep
Probability为0.8，亦即有20%的node的输出会归零，他scale up的方式就是剩余node的输出会在除以0.8，这样可以保持整层layer的输出期望值与未dropout前一样。
</p>

<p>
而Drop Connect则是对每个input权重采用高斯抽样，过程如下：
</p>


<div class="figure">
<p><img src="一、Introduction/screenshot_2018-08-09_02-11-18.png" alt="screenshot_2018-08-09_02-11-18.png" />
</p>
</div>

<p>
从inference的算法可以轻易看出，每次inference需要对每个权重进行sample，故Drop
Connect速度会慢一些。
</p>
</div>
</div>

<div id="org28f4fba" class="outline-3">
<h3 id="org28f4fba">2.2 Dense-Sparse-Dense training(ICLR 2017)<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup></h3>
<div class="outline-text-3" id="text-org28f4fba">
<p>
<a href="https://arxiv.org/pdf/1607.04381.pdf">https://arxiv.org/pdf/1607.04381.pdf</a>
</p>

<p>
其架构如下：
</p>


<div class="figure">
<p><img src="二，相关工作/screenshot_2018-08-09_02-16-55.png" alt="screenshot_2018-08-09_02-16-55.png" />
</p>
</div>


<p>
它分为三个步骤
</p>

<ol class="org-ol">
<li>Dense: 这一步骤跟原始neural network 一样，你原本怎么train你的network，就怎么
train，直至收敛.</li>
<li>Sparse: 这一步骤是把每一层的weight取绝对值，把小的prune掉，这是因为作者认为值较小的weight会导致model over-fitting，接下来一样把model train到收敛.</li>
<li>Re Dense: 把在Sparse step prune掉的weight补回来，继续把model train到收敛。</li>
</ol>

<p>
详细算法如下：
</p>


<div class="figure">
<p><img src="二，相关工作/screenshot_2018-08-09_02-17-39.png" alt="screenshot_2018-08-09_02-17-39.png" />
</p>
</div>


<p>
作者在Paper中，以google net作实验，把weight画出图如下:
</p>


<div class="figure">
<p><img src="二，相关工作/screenshot_2018-08-09_02-18-01.png" alt="screenshot_2018-08-09_02-18-01.png" />
</p>
</div>

<p>
我的想法也是由此而来，如果在training的时候，train了一定的epoch数但还没converge前，就渐渐prune掉weight值较小的weight是不是可以类似regularization的效果，或是在
converge后，Sparse step照着normal distribution去掉weight而不是只拿掉小的，会达到一样的效果.
</p>
</div>
</div>

<div id="orgb0e7f42" class="outline-3">
<h3 id="orgb0e7f42">2.3 Adaptive dropout<sup><a id="fnr.5" class="footref" href="#fn.5">5</a></sup></h3>
<div class="outline-text-3" id="text-orgb0e7f42">
<p>
<a href="https://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks.pdf">https://papers.nips.cc/paper/5032-adaptive-dropout-for-training-deep-neural-networks.pdf</a>
</p>

<p>
这篇是在说要不要用某些方法学出dropout_rate，原本的dropout公式如下:
</p>


\begin{equation}
a_j=m_jg(\sum_{i}^{\mathit{i}<{j}}w_{j,i}a_i)
\end{equation}

<p>
其中:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">参数</th>
<th scope="col" class="org-left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">w</td>
<td class="org-left">weight，</td>
</tr>

<tr>
<td class="org-left">ai</td>
<td class="org-left">上一层某个node的output，</td>
</tr>

<tr>
<td class="org-left">g</td>
<td class="org-left">activation function，</td>
</tr>

<tr>
<td class="org-left">mj</td>
<td class="org-left">这个node该不该drop，</td>
</tr>
</tbody>
</table>


<p>
作者把 <code>mj = 1</code> 的probability写成如下:
</p>

\begin{equation}
P(m_j=1|{a_i:\mathit{i}<{j}})=f(\sum_i^{\mathit{i}<{j}}\pi_{j,i}a_i)
\end{equation}


<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">参数</th>
<th scope="col" class="org-left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">ai</td>
<td class="org-left">dropout公式中的ai，</td>
</tr>

<tr>
<td class="org-left">f</td>
<td class="org-left">sigmoid function</td>
</tr>

<tr>
<td class="org-left">pi</td>
<td class="org-left">要learn的权重</td>
</tr>
</tbody>
</table>

<p>
所以可以把dropout公式改写如下:
</p>

\begin{equation}
\mathbb{E}[a_j]=f(\displaystyle\sum_i^{\mathit{i}<{j}}\pi_{j,i}a_i)
g(\displaystyle\sum_{i}^{\mathit{i}<{ j }}w_{j,i}A_I)
\end{equation}
</div>
</div>
</div>

<div id="org03661df" class="outline-2">
<h2 id="org03661df">三 实验设计</h2>
<div class="outline-text-2" id="text-org03661df">
</div>
<div id="org4d6a2cf" class="outline-3">
<h3 id="org4d6a2cf">测试数据集：Cifar10</h3>
</div>
<div id="org32937f8" class="outline-3">
<h3 id="org32937f8">预处理</h3>
<div class="outline-text-3" id="text-org32937f8">
<p>
先取20000笔data做为一个epoch的data,并将他随机调亮、翻转、裁剪,在每train完一个
epoch后,他会将原本20000笔data再随机调亮、翻转、裁剪,因此每一个epoch的资料并不完全一样,而他的training loss也是根据一个一个batch去算,因此实验结果的横轴指的是step(train了几个batch),并非epoch。
</p>
</div>
</div>
<div id="org1132bd4" class="outline-3">
<h3 id="org1132bd4">架构</h3>
<div class="outline-text-3" id="text-org1132bd4">
<ul class="org-ul">
<li>两层convolution layer,两层fully connected layer</li>
<li>第一层convolution layer为kernel 5*5 filter 64</li>
<li>第二层convolution layer为kernel 5*5 filter 64</li>
<li>第三层fully connected layer为压平后转成384维</li>
<li>第四层fully connected layer为384转成192维</li>
<li>最后一层即是192维转成10维的output</li>
</ul>
</div>
</div>
<div id="orgd1b13ec" class="outline-3">
<h3 id="orgd1b13ec">描述</h3>
<div class="outline-text-3" id="text-orgd1b13ec">
<p>
我是基于tensorflow tutorial code去做修改,其中实验结果表一是在fully connect的
layer上加dropout 实验结果表二是在train到converge后,我把weight小的丢弃再train
到收敛,也就是Related work第二项DSD中的前两个步骤。在这个实验中我是在除了第一层外的其他层都做pruning,因为在第一层的convolution layer因为只有3个channel,随意拿掉任一weight会太敏感,所以不适合拿掉weight。
</p>
</div>
</div>

<div id="org7803d78" class="outline-3">
<h3 id="org7803d78">源代码链接</h3>
<div class="outline-text-3" id="text-org7803d78">
<p>
<a href="https://github.com/YiddishKop/ml_src_dropout_compare">https://github.com/YiddishKop/ml_src_dropout_compare</a>
</p>
</div>
</div>

<div id="org0a69773" class="outline-3">
<h3 id="org0a69773">使用方法</h3>
<div class="outline-text-3" id="text-org0a69773">
<p>
实验一＆二：
</p>
<div class="org-src-container">
<pre class="src src-shell">python cifar10_train_DSD.py $<span class="org-highlight-numbers-number">1</span> $<span class="org-highlight-numbers-number">2</span> $<span class="org-highlight-numbers-number">3</span> $<span class="org-highlight-numbers-number">4</span>
</pre>
</div>
<ul class="org-ul">
<li><code>$1</code> :你要存放model的资料夹(可以使用tensorboard view详细情况)</li>
<li><code>$2</code> :你想要存放record的地方</li>
<li><code>$3</code> : Dropout ratio(Fully Connected Layer的Keep probability)</li>
<li><code>$4</code> : Sparse ratio(在Sparse step的sparsity)</li>
</ul>
<p>
实验三：
</p>
<div class="org-src-container">
<pre class="src src-shell">python cifar10_train_combine.py $<span class="org-highlight-numbers-number">1</span> $<span class="org-highlight-numbers-number">2</span> $<span class="org-highlight-numbers-number">3</span> $<span class="org-highlight-numbers-number">4</span> $<span class="org-highlight-numbers-number">5</span>
</pre>
</div>
<ul class="org-ul">
<li><code>$1</code> :你要存放model的资料夹(可以使用tensorboard view详细情况)</li>
<li><code>$2</code> :你想要存放record的地方</li>
<li><code>$3</code> : Dropout ratio(Fully Connected Layer的Keep probability)</li>
<li><code>$4</code> : Sparse ratio(在Sparse step的sparsity)</li>
<li><code>$5</code> : step(你想在第几个step开始做sparse step)</li>
</ul>
</div>
</div>
</div>

<div id="orgc3d5840" class="outline-2">
<h2 id="orgc3d5840">四 实验结果</h2>
<div class="outline-text-2" id="text-orgc3d5840">
</div>
<div id="org8c92400" class="outline-3">
<h3 id="org8c92400">实验一</h3>
<div class="outline-text-3" id="text-org8c92400">
<p>
实际探讨 <code>dropout</code> 的表现,并尝试不同的 <code>keep_prob</code> 对 <code>performance</code> 的影响
</p>


<div class="figure">
<p><img src="四、Experiment Result/screenshot_2018-08-09_02-28-11.png" alt="screenshot_2018-08-09_02-28-11.png" />
</p>
<p><span class="figure-number">Figure 7: </span>图一与图二 \({1. 横轴为step,也就是已经train了几个batch\\ 2. 不同的颜色代表不同的keep_probability,亦即fc-layer保留多少比率的node\\ 3. 图一纵轴的accuracy为10000笔testing(validation) data的accuracy\\ 4. 图二纵轴的loss为batch loss(cross entropy)}\)</p>
</div>
</div>
</div>

<div id="orgc40291f" class="outline-3">
<h3 id="orgc40291f">实验二</h3>
<div class="outline-text-3" id="text-orgc40291f">
<p>
这部分我实现了 <code>Dense-Sparse</code> 的架构。之所以放弃 <code>Re-Dense</code> 的步骤,是因为它与这次探讨的内容,即 <code>Dropout/Drop-Connect</code> 较不相关,单纯只是藉由反覆的训练以提升准确率。在这个实验中,我探讨了 <code>Dense-Sparse</code> 的可行性,以及 <code>Dense</code> 转 =Sparse=的过程中 <b>舍弃的比例</b> 对准确率的影响。
</p>


<div id="org5863036" class="figure">
<p><img src="四、Experiment Result/screenshot_2018-08-09_02-29-03.png" alt="screenshot_2018-08-09_02-29-03.png" />
</p>
<p><span class="figure-number">Figure 8: </span>图三：\({1. 蓝色的线是指 Dense 阶段最后达到的 Validation Accuracy\\ 2. 绿色的线是指 Dense-Sparse 完整训练完之后,所能达到的 Validation Accuracy\\ 3. 横轴的数字为 keep_probability ,亦即每个 layer 保留多少比率的 weights\\ 4. baseline 是没用 Dense-Sparse 框架时的表现,即正常、简单地训练这个 CNN\\ 5. 此实验只套用 Dense-Sparse 的框架,没有使用 Dropout}\)</p>
</div>
</div>
</div>

<div id="org3eaa4f6" class="outline-3">
<h3 id="org3eaa4f6">实验三</h3>
<div class="outline-text-3" id="text-org3eaa4f6">
<p>
前一部分,我已经发现 <code>Dense-Sparse</code> 确实有可能表现得更好,而且参数量还可以大量减少。但是,原本的架构中,在 <code>Dense</code> 阶段需要把模型训练到完全收敛(Validation的表现不再上生),才能进入 <code>Sparse</code> 的阶段。如果提早进入 <code>Sparse</code> ,会不会有不一样的结果,还是会更差？ 我尝试提前让模型进入 <code>Sparse</code> 的阶段,并观察最后模型的表现。(原本的架构,
<code>Dense</code> 要训练约90000个 <code>batch</code> 才收敛,我试了只训练22500/45000/67500个=batch= 就进入 <code>Sparse</code> 的版本)
</p>



<div class="figure">
<p><img src="四、Experiment Result/screenshot_2018-08-09_02-29-58.png" alt="screenshot_2018-08-09_02-29-58.png" />
</p>
<p><span class="figure-number">Figure 9: </span>图四: \({1. 横轴代表不同的keep_probability,亦即fc-layer保留多少比率的node\\ 2. 不同的颜色代表不同的版本,即 Dense 阶段训练了22500/45000/67500个 batch\\ 3. 纵轴的accuracy为10000笔testing(validation) data的accuracy\\ 4. baseline 是没用 Dense-Sparse 框架时的表现,即正常、简单地训练这个 CNN}\)</p>
</div>
</div>
</div>
</div>

<div id="org2d14eea" class="outline-2">
<h2 id="org2d14eea">五 结论</h2>
<div class="outline-text-2" id="text-org2d14eea">
<p>
第一个实验,我发现 <code>Dropout</code> 并不一定能提升 <code>accuracy</code> 。我认为,这和资料集以及模型复杂度有关系, <code>Cifar10</code> 的 <code>train</code> 和 <code>validation</code> 的数据集分布差异并不大,且
<code>tutorial</code> 的模型参数并不多,本来就不至于有 <code>over-fitting</code> 的问题, <code>dropout</code> 的帮助自然不大。我有使用比较复杂的模型(较深层的CNN,或增加CNN每个layer的单元数量)但
dropout的表现变化不大。
</p>

<p>
而在改良 <code>Dense-Sparse-Dense</code> 的实验中,因 <code>Re-Dense</code> 和目标无关,我实现的是
<code>Dense-Sparse</code> 的部分：先把网络训练到收敛,然后把网络中较低的 <code>Weight</code> 归零,之后再继续训练到收敛。在这个尝试中, <code>Validation</code> 的表现相当好, <code>Sparse</code> 的模型参数数量较原本的模型少了许多,训练集的损失还能降到差不多的地步,说明了Dense-Sparse 的可行性和潜力。
</p>

<p>
我猜测, <code>Dropout</code> 有可能使模型 <code>under-fit</code> ,就像我的实验,加了 <code>Dropout</code> 之后,表现普遍不好,但也说明了 <code>Dropout</code> 确实有避免 <code>over-fit</code> 的效果。但使用
<code>Dense-Sparse</code> 来缩减参数的话,就算我 <code>mask</code> 掉大部分的=weight= ,模型的表现都不至于变差。简言之, <code>Dropout</code> 的随机性是有其必要的,而 <b>固定Drop掉Weight小的部分</b> 充其量只能做为压缩模型的一种手段而已,无法有效解决 <code>over-fit</code> 的问题。
</p>
</div>
</div>

<div id="orgd91f9b1" class="outline-2">
<h2 id="orgd91f9b1">六 附录</h2>
<div class="outline-text-2" id="text-orgd91f9b1">
<p>
下图是我的模型中 <code>fully-connected layer</code> 在不同时间的 <code>weight</code> 分布图
</p>

<p>
主要在三个时间点纪录了 <code>weight</code> 的状态
</p>


<div id="org8784204" class="figure">
<p><img src="六、Appendix/screenshot_2018-08-09_02-31-13.png" alt="screenshot_2018-08-09_02-31-13.png" />
</p>
<p><span class="figure-number">Figure 10: </span>图一： <code>Dense</code> 阶段收敛时</p>
</div>




<div id="org85cb2f8" class="figure">
<p><img src="六、Appendix/screenshot_2018-08-09_02-31-27.png" alt="screenshot_2018-08-09_02-31-27.png" />
</p>
<p><span class="figure-number">Figure 11: </span>图二： <code>Sparse</code> 阶段开始时</p>
</div>




<div id="org34281e8" class="figure">
<p><img src="六、Appendix/screenshot_2018-08-09_02-31-42.png" alt="screenshot_2018-08-09_02-31-42.png" />
</p>
<p><span class="figure-number">Figure 12: </span>图三： <code>Sparse</code> 阶段再次收敛时</p>
</div>


<p>
小结：
</p>

<p>
<b>可以发现,如 <code>paper</code> 所示,数值较小的 <code>weight</code> 们被 <code>Drop</code> 掉的时候,相当于有许多的 <code>weight</code> 变为0,而随着模型继续训练, =weight = 的分布会更进一步往中间集中。</b>
</p>
</div>
</div>

<div id="orgb182f69" class="outline-2">
<h2 id="orgb182f69">七 索引</h2>
<div class="outline-text-2" id="text-orgb182f69">
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
N. Srivastava, G. Hinton, A Krizhevsky, I. Sutskever and R. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
S. Han, J. Pool, J. Tran and W.J. Dally. Learning both Weights and Connections for Efficient Neural Networks
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
L. Wan, M. Zeiler, S. Zhang,Y. LeCun and R. Fergus. Regularization of Neural Networks using DropConnect
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">
S. Han, J. Pool, J. Tran and W.J. Dally. DSD: Dense-Sparse-Dense training for deep neural networks
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5">5</a></sup> <div class="footpara"><p class="footpara">
L.J. Ba and B. Frey. Adaptive dropout for training deep neural networks.
</p></div></div>


</div>
</div></div>
</body>
</html>
