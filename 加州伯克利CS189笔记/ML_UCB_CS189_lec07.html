<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 æ—¥ 14:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 07 GAUSSIAN DISCRIMINANT ANALYSIS</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 07 GAUSSIAN DISCRIMINANT ANALYSIS</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5846dde">lec 07 GAUSSIAN DISCRIMINANT ANALYSIS</a></li>
<li><a href="#orgc383b73">lec 07.1 QUADRATIC DISCRIMINANT ANALYSIS (QDA)</a></li>
<li><a href="#orgc76ccef">lec 07.2 LINEAR DISCRIMINANT ANALYSIS (LDA)</a></li>
<li><a href="#org56681d1">lec 07.3 MAXIMUM LIKELIHOOD ESTIMATION OF PARAMETERS (Ronald Fisher, circa 1912)</a></li>
<li><a href="#orgcb4acbc">lec 07.4 LIKELIHOOD OF A GAUSSIAN</a></li>
</ul>
</div>
</div>
<div id="org5846dde" class="outline-2">
<h2 id="org5846dde">lec 07 GAUSSIAN DISCRIMINANT ANALYSIS</h2>
<div class="outline-text-2" id="text-org5846dde">
<p>
Fundamental assumption:  each class comes from normal distribution (Gaussian).
</p>

<pre class="example">
                                                             2
               2                   1                 |x - mu|
X ~ N(mu, sigma ) :  P(x) = ---------------- exp ( - --------- )
                            sqrt(2 pi) sigma         2 sigma^2
</pre>

<p>
For each class C, suppose we estimate mean mu_C, variance sigma_C^2, and
prior pi_C = P(Y = C).
</p>

<p>
Given x, Bayes rule r*(x) returns class C that maximizes P(X = x | Y = C) pi_C.
</p>

<p>
ln z is monotonically increasing for z &gt; 0, so it is equivalent to maximize
</p>

<pre class="example">
                                                2
                                      |x - mu_C|
Q (x) = ln ( sqrt(2 pi) P(x) pi ) = - ----------- - ln sigma  + ln pi
 C                             C      2 sigma_C^2           C        C
^                       ^
| quadratic in x.       | normal distribution, replaces P(X = x | Y = C)

</pre>

<p>
[In a 2-class problem, you can also incorporate an asymmetrical loss function
 the same way we incorporated the prior pi_C.  In a multi-class problem, it
 gets a bit more complicated, because the penalty for guessing wrong might
 depend not just on the true class, but also on the wrong guess.]
</p>
</div>
</div>
<div id="orgc383b73" class="outline-2">
<h2 id="orgc383b73">lec 07.1 QUADRATIC DISCRIMINANT ANALYSIS (QDA)</h2>
<div class="outline-text-2" id="text-orgc383b73">
<p>
<code>=============================</code>
Suppose only 2 classes C, D.  Then
</p>
<pre class="example">

          /   C      if Q (x) - Q (x) &gt; 0,
  r*(x) = |              C       D
          \   D      otherwise

Prediction fn is quadratic in x.  Bayes decision boundary is Q (x) - Q (x) = 0.
                                                              C       D
  - In 1D, B.d.b. may have 1 or 2 points.    [Solution to a quadratic equation]
  - In d-D, B.d.b. is a quadric.                [In 2D, that's a conic section]

</pre>
<p>
[Show 2D Gaussian diagrams (from last lecture).]
</p>

<p>
[The equations I wrote down above can apply to a one-dimensional feature space,
 or they could apply equally well to a multi-dimensional feature space with
 isotropic, spherical Gaussians.  In the latter case, x and mu are vectors, but
 the variance is a scalar.  Next lecture we'll look at anisotropic Gaussians
 where the variance is different along different directions.]
</p>

<p>
[QDA works very nicely with more than 2 classes.  You typically wind up with
 multiple decision boundaries that adjoin each other at joints.  The feature
 space gets partitioned into regions.  In two or more dimensions, it looks like
 a sort of Voronoi diagram.]
</p>

<p>
[Show multiplicatively weighted Voronoi diagram.]
</p>

<p>
[You might not be satisfied with just knowing how each point is classified.
 One of the great things about QDA is that you can also determine the
 probability that your classification is correct.  Let's work that out.]
</p>

<p>

To recover posterior probabilities in 2-class case, use Bayes:
</p>

<pre class="example">
                           P(X | Y = C) pi_C
  P(Y = C | X) = -------------------------------------
                 P(X | Y = C) pi_C + P(X | Y = D) pi_D

                                                   Q_C(x)
                                           recall e       = sqrt(2 pi) P(x) pi
                                                                              C
                           Q_C(x)
                          e                       1
  P(Y = C | X = x) = ----------------- = --------------------
                      Q_C(x)    Q_D(x)        Q_D(x) - Q_C(x)
                     e       + e         1 + e

                   = s(Q (x) - Q (x)), where
                        C       D
                  1
  s(gamma) = -----------    &lt;===  _logistic_fn_ aka _sigmoid_fn_
                  -gamma
             1 + e

[Show logistic fn.  s(0) = 1/2, s(inf) -&gt; 1, s(-inf) -&gt; 0, monoton increasing.]


</pre>
</div>
</div>

<div id="orgc76ccef" class="outline-2">
<h2 id="orgc76ccef">lec 07.2 LINEAR DISCRIMINANT ANALYSIS (LDA)</h2>
<div class="outline-text-2" id="text-orgc76ccef">
<p>
[less likely to overfit than QDA]
</p>

<p>
Fundamental assumption:  all the Gaussians have same variance sigma.
</p>

<p>
[The equations simplify nicely in this case.]
</p>

<pre class="example">
                                      2       2
                (mu_C - mu_D) . x   mu_C  - mu_D
Q (x) - Q (x) = ----------------- - ------------- + ln pi  - ln pi
 C       D           sigma^2          2 sigma^2          C        D
                \_______________/ \_______________________________/
                      w . x      +             alpha

</pre>

<p>
[The quadratic terms in Q_C and Q_D cancelled each other out!]
</p>

<p>
Now it's a linear classifier!  Choose C that maximizes <span class="underline">linear_discriminant_fn</span>
</p>

<pre class="example">
                2
mu_C . x      mu_C
-------- - --------- + ln pi                     [works for any # of classes]
sigma^2    2 sigma^2        C

</pre>

<p>
In 2-class case:  decision boundary is w . x + alpha = 0
                  Bayes posterior is P(Y = C | X = x) = s(w . x + alpha)
</p>

<p>
[The effect of "w . x + alpha" is to scale and translate the logistic fn
 in x-space.  It's a linear transformation.]
[Show two 1D Gaussians + logistic fn.]
[Show Voronoi diagram.]
</p>

<pre class="example">
                                                             mu_C + mu_D
If pi  = pi  = 1/2   ===&gt;   (mu  - mu ) . x - (mu  - mu ) . (-----------) = 0
     C     D                   C     D           C     D          2

</pre>

<p>
This is the centroid method!
</p>

<p>

</p>
</div>
</div>
<div id="org56681d1" class="outline-2">
<h2 id="org56681d1">lec 07.3 MAXIMUM LIKELIHOOD ESTIMATION OF PARAMETERS (Ronald Fisher, circa 1912)</h2>
<div class="outline-text-2" id="text-org56681d1">
<p>
<code>=========================================</code>
[Before I talk about fitting Gaussians, I want to start with a simpler example.
 It's easier to understand maximum likelihood if we consider a discrete
 distribution first; then a continuous distribution later.]
</p>

<p>
Let's flip biased coins!  Heads with probability p; tails w/prob. 1 - p.
</p>

<p>
10 flips, 8 heads, 2 tails.  What is most likely value of p?
</p>

<p>
Binomial distribution:  X ~ B(n, p)
</p>

<pre class="example">

             /n\  x        n - x
  P[X = x] = | | p  (1 - p)
             \x/

Our example:  n = 10,

                 8        2      def
  P[X = 8] = 45 p  (1 - p)        =  L(p)

</pre>
<p>
Probability of 8 heads in 10 flips:
written as a fn L(p) of distribution parameter(s), this is the <span class="underline">likelihood_fn</span>.
</p>

<p>
<span class="underline">Maximum_likelihood_estimation</span> (MLE):  A method of estimating the parameters
of a statistical model by picking the params that maximize the likelihood fn.
</p>

<p>
[Let's phrase it as an optimization problem.]
</p>

<pre class="example">
--------------------------------
|  Find p that minimizes L(p)  |
--------------------------------
</pre>

<p>
[Show graph of L(p).]
</p>

<pre class="example">
Solve this example by setting derivative = 0:

  dL        7        2       8
  -- = 360 p  (1 - p)  - 90 p  (1 - p) = 0
  dp

  ===&gt;    4 (1 - p) - p = 0    ===&gt;    p = 0.8

[It shouldn't seem surprising that a coin that comes up heads 80% of the time
 is the coin most likely to produce 8 heads in 10 flips.]

       d^2L
Note:  ---- = -18.8744 &lt; 0   at p = 0.8, confirming it's a maximum.
       dp^2

</pre>

<p>

</p>
</div>
</div>
<div id="orgcb4acbc" class="outline-2">
<h2 id="orgcb4acbc">lec 07.4 LIKELIHOOD OF A GAUSSIAN</h2>
<div class="outline-text-2" id="text-orgcb4acbc">
<p>
<code>======================</code>
Given samples x_1, x_2, ..., x_n (scalars or vectors), find best-fit Gaussian.
</p>

<p>
[Let's do this with a normal distribution instead of a binomial distribution.
 If you generate a random point from a normal distribution, what is the
 probability that it will be exactly at the mean of the Gaussian?]
</p>

<p>
[Zero.  So it might seem like we have a bit of a problem here.  With a
 continuous distribution, the probability of generating any particular point
 is zero.  But we're just going to ignore that and do "likelihood" anyway.]
</p>

<p>
Likelihood of generating these samples is
</p>

<p>
L(mu, sigma; x_1, ..., x_n) = P(x_1) P(x_2) ... P(x_n)
</p>

<p>
The <span class="underline">log_likelihood</span> l(.) is the ln of the likelihood L(.).
Maximizing likelihood  &lt;==&gt;  maximizing log-likelihood.
</p>

<pre class="example">
l(mu, sigma; x_1, ..., x_n) = ln P(x_1) + ln P(x_2) + ... + ln P(x_n)

                             dl
Want to set grad   l = 0,  ------- = 0
                mu         d sigma
                    2
            |x - mu|
ln P(x) = - --------- - ln sqrt(2 pi) - ln sigma  [ln of normal distribution]
            2 sigma^2

            n  x_i - mu                 ^    1  n            [The ^hats^ mean
grad   l = sum -------- = 0    ====&gt;    mu = - sum x_i        "estimated"]
    mu     i=1 sigma^2                       n i=1
                        2        2
  dl       n  |x_i - mu|  - sigma                    ^  2   1  n           2
------- = sum ------------------ = 0      ====&gt;    sigma  = - sum |x  - mu|
d sigma   i=1      sigma^3                                  n i=1   i

                                      ^                      ^

</pre>
<p>
We don't know mu exactly, so substitute mu for mu to compute sigma.
</p>

<p>
In short, we use mean &amp; variance of samples in class C to estimate
mean &amp; variance of Gaussian for class C.
</p>

<p>
For QDA:  estimate mean &amp; variance of each class as above
        &amp; estimate the priors (for each class C):
</p>

<pre class="example">
   ^      n_C
  pi  = -------                              [this is the coin flip parameter!]
    C   sum n_D  &lt;==  sum of samples in all classes
         D

For LDA:  same mean &amp; priors; one variance for all classes:

    ^  2   1                          2
  sigma  = - sum    sum     |x  - mu |
           n  C  {i:y  = C}   i     C

</pre>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 äº”</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 æ—¥ 14:58</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
