<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 æ—¥ 14:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 08 EIGENVECTORS</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 08 EIGENVECTORS</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgbdb8628">lec 08 EIGENVECTORS</a>
<ul>
<li><a href="#orga84326c">Ellipsoids</a></li>
<li><a href="#orge7c4843">Building a Quadratic w/Specified Eigenvectors/values</a></li>
</ul>
</li>
<li><a href="#org385d217">lec 08.2 ANISOTROPIC GAUSSIANS</a></li>
</ul>
</div>
</div>
<div id="orgbdb8628" class="outline-2">
<h2 id="orgbdb8628">lec 08 EIGENVECTORS</h2>
<div class="outline-text-2" id="text-orgbdb8628">
<p>
<code>==========</code>
[I don't know if you were properly taught about eigenvectors here at Berkeley,
 but I sure don't like the way they're taught in most linear algebra books.
 So I'll start with a review.  You all know the definition of an eigenvector:]
</p>

<p>
Given matrix A, if Av = lambda v for some vector v != 0, scalar lambda, then
v is an <span class="underline">eigenvector</span> of A and lambda is the associated <span class="underline">eigenvalue</span> of A.
</p>

<p>
[But what does that mean?  It means that v is a magical vector that, after
 being multiplied by A, still points in the <b>same*direction</b>, or in exactly
 the <b>opposite*direction</b>.]
</p>

<pre class="example">
Eigenvalue 2:                                                             3   /
                                                                         A v /
          ^                   ^                   ^                   ^     /
          |                   |                   |                   |    /
          |                   |                   |   / 2             |   /
          |                   |                   |  / A v            |  /
          |  v                | / Av              | /                 | /
          |/                  |/                  |/                  |/
  &lt;-------+-------&gt;   &lt;-------+-------&gt;   &lt;-------+-------&gt;   &lt;-------+-------&gt;
          |                   |                   |                   |
          v                   v                   v                   v

Eigenvalue -1/2:

          ^                   ^                   ^                   ^
          |                   |                   |                   |
          |               \   |                   |                   |
          |             Aw \  |                   |               3   |
          |                 \ |                   |              A w  |
          |                  \|                   |                  \|
  &lt;-------+-------&gt;   &lt;-------+-------&gt;   &lt;-------+-------&gt;   &lt;-------+-------&gt;
          |\                  |                   |\                  |
          | \                 |                   | \  2              |
          |  \                |                   |   A w             |
          |   \               |                   |                   |
          |    \              |                   |                   |
          v     \ w           v                   v                   v
                 \
                  \

</pre>

<p>
[For most matrices, most vectors don't have this property.  So the ones that do
 are special, and we call them eigenvectors.]
[Clearly, when you scale an eigenvector, it's still an eigenvector.  Only the
 direction matters, not the length.  Let's look at a few consequences.]
</p>

<p>
Theorem:  if v is eigenvector of A w/eigenvalue lambda,
        then v is eigenvector of A^k w/eigenvalue lambda^k     [will use later]
</p>

<pre class="example">
         2                          2
Proof:  A  v = A (lambda v) = lambda  v, etc.

Theorem:  moreover, if A is invertible,
          then v is eigenvector of A^-1 w/eigenvalue 1 / lambda^k

         -1       1     -1        1                [Look at the figures above,
Proof:  A   v = ------ A   Av = ------ v            but go from right to left.]
                lambda          lambda

</pre>

<p>
[Stated simply:  When you invert a matrix, the eigenvectors don't change, but
 the eigenvalues get inverted.  When you square a matrix, the eigenvectors
 don't change, but the eigenvalues get squared.]
</p>

<p>
[Those theorems are pretty obvious.  The next theorem is not obvious at all.
 But it's going to be very useful for understanding the effect of a symmetric
 matrix on a vector that is <b>not</b> an eigenvector.]
</p>

<pre class="example">
_Spectral_Theorem_:  every symmetric n-by-n matrix has n eigenvectors that are
                                                 T
                     mutually orthogonal, i.e., v  v  = 0   for all i != j
                                                 i  j
</pre>

<p>
[This takes about a page of math to prove.
 One minor detail is that a matrix can have more than n eigenvector directions.
 If two eigenvectors happen to have the same eigenvalue, then every linear
 combination of those eigenvectors is also an eigenvector.  Then you have
 infinitely many eigenvector directions, but they all span the same plane.
 So you just arbitrarily pick two vectors in that plane that are orthogonal to
 each other.  By contrast, the set of eigenvalues is always uniquely determined
 by a matrix, including the multiplicity of eigenvalues.]
</p>

<p>
We can use them as a basis for R^n.
</p>

<pre class="example">
[Now we can ask, what happens to a vector that *isn't* an eigenvector when
 you apply a symmetric matrix to it?  Express that vector as a linear
 combination of eigenvectors, and look at each eigenvector separately.]      #
                                                                         3  # /
                                                                        A x# /
          ^              Ax # ^                   ^                   ^    #/
          |                 # |                   |                   |   #/
          |               \  #|                   |   /               |  #/
          |                \ #|                   |  /     2          | #/
          |                 \ # /                 | /  ## A x         | #
          |/                 \#/                  |/ ##              \|#
  &lt;-------+-------&gt;   &lt;-------+-------&gt;   &lt;-------+##-----&gt;   &lt;-------+-------&gt;
          |#                  |                   |\                  |
          | ##                |                   | \                 |
          |  \#  x = v + w    |                   |                   |
          |   \#              |                   |                   |
          |    \##            |                   |                   |
          v     \ #           v                   v                   v
                 \ #
                  \

</pre>

<p>
[Every time we apply A to this vector, it changes direction.  We can understand
 it by writing it as a sum of components that don't change direction.]
</p>

<p>
Write x as linear combo of eigenvectors:
</p>

<pre class="example">

x = alpha v + beta w

 k                k                k
A x = alpha lambda  v + beta lambda  w
                  v                w


</pre>
<p>

</p>
</div>
<div id="orga84326c" class="outline-3">
<h3 id="orga84326c">Ellipsoids</h3>
<div class="outline-text-3" id="text-orga84326c">
<hr />
<p>
[Now, let's look what happens to a quadratic function when we apply a symmetric
 matrix to the space, with these two eigenvectors and eigenvalues.]
</p>

<pre class="example">
          T
  f(x) = x x                  &lt;== quadratic; isotropic; isosurfaces are spheres
  g(x) = f(Ax)                &lt;== A symmetric

          T  2                                                    2
       = x  A  x              &lt;== _quadratic_form_ of the matrix A
                                  anisotropic; isosurfaces are ellipsoids

[Show isocontours for f(x) = |x|^2 and g(x) = |Ax|^2, where A = [ 3/4   5/4 ]
                                                                [ 5/4   3/4 ]
 Draw the stretch direction (1, 1) &amp; the shrink direction (1, -1).]

</pre>

<p>
[Here's how to think of this:  we stretched the plane on the right along the
 direction with eigenvalue 2, and shrunk the plane along the direction with
 eigenvalue -1/2; then we drew the circular isocontours, like on the left; then
 we undid the stretching and let the plane spring back to its original shape.
 So the circle turned into an ellipse when the plane sprang back.]
</p>

<p>
[Looking at the quadratic form is one of the best ways to visually understand
 symmetric matrices and their eigenvectors and eigenvalues.]
</p>

<pre class="example">
g(x) = 1 is an ellipsoid with axes v_1, v_2, ..., v_n and
                              radii 1/lambda_1, 1/lambda_2, ... 1/lambda_n

because if v  has length 1/lambda , g(v ) = f(Av ) = f(lambda  v ) = 1
            i                    i     i        i            i  i

        ==&gt;  v_i  lies on the ellipsoid
</pre>

<p>
[The reason the radii are the reciprocals of the eigenvalues is that we're
 stretching the plane by the eigenvalues, then drawing the spheres, then
 letting the plane spring back to its original shape.  When the plane springs
 back, each axis of the spheres gets scaled by 1/eigenvalue.]
</p>

<p>
bigger eigenvalue   &lt;==&gt;   steeper slope   &lt;==&gt;   shorter ellipsoid radius
                         [ ^ really bigger curvature; slope varies along axis]
</p>

<p>
Alternate interpretation:  ellipsoids are spheres in <span class="underline">distance_metric</span> A^2
</p>

<pre class="example">
Call M = A^2 a _metric_tensor_ because
the distance between samples x &amp; z in stretched space is

                                    T
  d(x, z) = |Ax - Az| = sqrt{(x - z)  M (x - z)}
</pre>

<p>
[This is the Euclidean distance in the stretched space, but let's think of it
 as an alternative metric for measuring distances in the original space.  It's
 a kind of distance from x to z that's different from the Euclidean distance.]
</p>

<p>
[I'm calling M a "tensor" because that's standard usage in Riemannian geometry,
 but don't worry about what "tensor" means.  For our purposes, it's a matrix.]
</p>

<p>
Ellipsoids are "spheres" in this metric:  {x : d(x, center) = isovalue}
</p>

<pre class="example">
A square matrix B is _positive_definite_     if w^T B w &gt; 0  for all w != 0.
                                             &lt;==&gt;   all eigenvalues positive
                     _positive_semidefinite_ if w^T B w &gt;= 0 for all w.
                                             &lt;==&gt;   all eigenvalues nonnegative
                     _indefinite_            if +ve eigenvalue &amp; -ve eigenvalue
                     invertible              if no zero eigenvalue
</pre>
<p>

</p>

<p>
[Show figures of ellipses for +ve definite, +ve semidefinite, indefinite
 matrices, and inverse of +ve definite matrix; separate "whiteboard".
 Positive eigenvalues correspond to axes where the curvature goes up; negative
 eigenvalues correspond to axes where the curvature goes down.]
</p>

<p>
Metric tensors must be symmetric +ve definite (SPD).
</p>

<p>
[Remember that M = A^2, so M's eigenvalues are the squares of the eigenvalues
 of A, so the eigenvalues must be nonnegative and M is positive semidefinite.
 But if M has a zero eigenvalue, its distance function is not a "metric".
 To have a metric, you must have a strictly positive definite M.  If you have
 eigenvalues of zero, the isosurfaces are cylinders instead of ellipsoids.]
</p>

<pre class="example">
Special case:  M &amp; A are diagonal   &lt;==&gt;  eigenvectors are coordinate axes
                                    &lt;==&gt;  ellipsoids are _axis-aligned_
</pre>

<p>
[Draw axis-aligned isocontours for a diagonal metric.]
</p>
</div>
</div>

<div id="orge7c4843" class="outline-3">
<h3 id="orge7c4843">Building a Quadratic w/Specified Eigenvectors/values</h3>
<div class="outline-text-3" id="text-orge7c4843">
<hr />
<p>
[I, personally, find the process of going from eigenvectors and eigenvalues to
 a matrix and some ellipsoids to be more intuitive than the reverse.  So let's
 do that.  Suppose you know which ellipsoid axes you want to use, and you know
 what ellipsoid radius or stretch factor you want to use along each axis.]
</p>

<pre class="example">
Choose n mutually orthogonal *unit* n-vectors v_1, ..., v_n
  [so they specify an orthonormal coordinate system]
Let V = [v_1   v_2   ...   v_n]                               &lt;== n-by-n matrix
Observe:  V^T V = I           [off-diagonal 0's because vectors are orthogonal]
                              [diagonal 1's because unit vectors]
==&gt;   V^T = V^-1   ==&gt;   V V^T = I
V is _orthonormal_matrix_:  acts like rotation (or reflection)

Choose some inverse radii lambda_i:
             -                                      -
             | lambda_1      0       ...      0     |
Let Lambda = |    0       lambda_2            0     |          [diagonal matrix
             |   ...                                |           of eigenvalues]
             |    0          0       ...   lambda_n |
             -                                      -

                        T    n               T
Theorem:  A = V Lambda V  = sum lambda_i v  v   has chosen eigenvectors/values
                            i=1           i  i       [Clearly, A is symmetric]
               ^       ^                 \___/
          equivalent   |             outer product:  n-by-n matrix, rank 1
               v       |
Proof:  AV = V Lambda  |       &lt;== definition of eigenvectors! (in matrix form)
                       |

</pre>

<p>
 This is a <span class="underline">matrix_factorization</span> called the <span class="underline">eigendecomposition</span>.
Lambda is the <span class="underline">diagonalized</span> version of A.
V^T rotates ellipsoid to be axis-aligned.
</p>

<p>
This is also a recipe for building quadratics with axes v_i, radii 1 / lambda_i

Observe:  M = A^2 = V Lambda V^T V Lambda V^T = V Lambda^2 V^T
</p>

<p>
Given SPD metric tensor M, we can find a symmetric <span class="underline">square_root</span> A = M^{1/2}:
</p>
<ul class="org-ul">
<li>compute eigenvectors/values of M</li>
<li>take square roots of M's eigenvalues</li>
<li>reassemble matrix A</li>
</ul>

<p>
[The first step--breaking a matrix down to its eigenvectors and eigenvalues--is
 much harder than the last step--building up a new matrix from its eigenvectors
 and eigenvalues.  But I think that the latter process helps take a lot of the
 mystery out of eigenvectors.]
</p>
</div>
</div>
</div>

<div id="org385d217" class="outline-2">
<h2 id="org385d217">lec 08.2 ANISOTROPIC GAUSSIANS</h2>
<div class="outline-text-2" id="text-org385d217">
<p>
<code>===================</code>
</p>
<pre class="example">
X ~ N(mu, Sigma)

                   1                      1         T      -1
P(x) = -------------------------- exp ( - - (x - mu)  Sigma   (x - mu) )
       sqrt(2 pi)^d sqrt(|Sigma|)         2
                         ^ determinant of Sigma

</pre>

<p>
Sigma is the SPD <span class="underline">covariance_matrix</span>.
Sigma^-1 is the SPD <span class="underline">precision_matrix</span>; serves as metric tensor.
</p>

<p>
[Show example of paraboloid q(x) and bivariate Gaussian n(q(x)), w/univariate
 Gaussian n(.) between them.]
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 äº”</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 æ—¥ 14:57</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
