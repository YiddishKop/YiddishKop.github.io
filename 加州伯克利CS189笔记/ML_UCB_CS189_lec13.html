<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 14:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 13 RIDGE REGRESSION</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 13 RIDGE REGRESSION</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0713ddd">lec 13 RIDGE REGRESSION (aka Tikhonov regularization)</a>
<ul>
<li><a href="#org87be06e">Bayesian justification for ridge reg.</a></li>
</ul>
</li>
<li><a href="#orgb59dd47">lec 13.2 KERNELS</a>
<ul>
<li><a href="#org7bab473">Kernel Ridge Regression</a></li>
<li><a href="#org872cb61">The Kernel Trick (aka <span class="underline">kernelization</span>)</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="org0713ddd" class="outline-2">
<h2 id="org0713ddd">lec 13 RIDGE REGRESSION (aka Tikhonov regularization)</h2>
<div class="outline-text-2" id="text-org0713ddd">
<p>
<code>==============</code>
(1) + (A) + l_2 penalized mean loss (d).
</p>

<pre class="example">
--------------------------------------------------
|                               2              2 |
| Find w that minimizes |Xw - y|  + lambda |w'|  | = J(w)
--------------------------------------------------
</pre>
<p>
where w' is w with component alpha replaced by 0.
X has fictitious dimension but we DON'T penalize alpha.
</p>

<p>
Adds a penalty term to encourage small |w'|--called <span class="underline">shrinkage</span>.  Why?
</p>

<ul class="org-ul">
<li>Guarantees positive definite normal eq's; always unique solution.
[When samples lie on a common hyperplane in feature space.]  E.g. when d &gt; n.
[Show figure comparing +ve semidef bowl w/+ve definite bowl (ridgequad.png)]
[That was the original motivation, but the next has become more important...]</li>
<li><p>
Reduces overfitting by reducing variance.  Why?
Imagine:  275238 x_1^2 - 543845 x_1 x_2 + 385832 x_2^2 is best fit for
          well-spaced samples all with |y| &lt; 1.
          Small change in x  =&gt;  big change in y!
[Given that all the y values in the data are small, it's a sure sign of
 overfitting if tiny changes in x cause huge changes in y.]
So we penalize large weights.
[Show plot of isocontours for the two terms of J (ridgeterms.pdf)]
[In this plot, beta is the least-squares solution.  The red ellipses are the
 isocontours of |Xw - y|^2.  The isocontours of |w'| are circles centered at
 the origin (blue).  The solution lies where a red isocontour just touches
 a blue isocontour tangentially.  As lambda increases, the solution will
 occur at a more outer red isocontour and a more inner blue isocontour.]
</p>

<pre class="example">
Setting grad J = 0 gives normal eq'ns
    T                    T
  (X X + lambda I') w = X y
where I' is identity matrix w/bottom right set to zero.
                                         T
Algorithm:  Solve for w.  Return h(z) = w z.

Increasing lambda  =&gt;  more _regularization_; smaller |w'|
Given our data model y = Xv + e, where e is noise,
                               T   T              -1  T
variance of ridge reg. is Var(z  (X X + lambda I')   X e).
As lambda -&gt; infinity, variance -&gt; 0, but bias increases.
</pre></li>
</ul>
<p>
[Show graph of bias^2 &amp; variance as lambda increases (ridgebiasvar.pdf)]
[So, as usual for the bias-variance trade-off, the test error as a function of
 lambda is a U-shaped curve, and we find the bottom by validation.]
</p>

<p>
lambda is a hyperparameter; tune by (cross-)validation.
</p>

<p>
Ideally, features should be "normalized" to have same variance.
Alternative:  use asymmetric penalty by replacing I' w/other diagonal matrix.
</p>
</div>

<div id="org87be06e" class="outline-3">
<h3 id="org87be06e">Bayesian justification for ridge reg.</h3>
<div class="outline-text-3" id="text-org87be06e">
<hr />
<pre class="example">
Assign a prior probability on w':  a Gaussian centered at 0.

  Posterior prob ~ likelihood of w  x  prior P(w')        &lt;= Gaussian PDF
  Maximize log posterior = ln likelihood + ln P(w')
                         = - const |Xw - y|^2 - const |w'|^2 - const

This method (using likelihood, but maximizing posterior) is called
_maximum_a_posteriori_ (MAP).

</pre>
<p>

</p>
</div>
</div>
</div>
<div id="orgb59dd47" class="outline-2">
<h2 id="orgb59dd47">lec 13.2 KERNELS</h2>
<div class="outline-text-2" id="text-orgb59dd47">
<p>
<code>=====</code>
Recall: with d input features, degree-p polynomials blow up to O(d^p) features.
[When d is large, this gets computationally intractible really fast.
 As I said in Lecture 4, if you have 100 features per sample and you want to
 use degree-4 predictor functions, then each lifted feature vector has a length
 on the order of 100 million.]
Today we use magic to use those features without computing them!
</p>

<p>
Observation:  In many learning algs,
</p>
<ul class="org-ul">
<li>the weights can be written as a linear combo of input samples, &amp;</li>
<li>we can use inner products of Phi(x)'s only  =&gt;  don't need to compute Phi(x)!</li>
</ul>

<pre class="example">
             T     n                         n
Suppose w = X a = sum a  X    for some a in R .
                  i=1  i  i
</pre>

<p>
Substitute this identity into alg. and optimize n <span class="underline">dual_weights</span> a
(aka <span class="underline">dual_parameters</span>) instead of d+1 <span class="underline">primal_weights</span> w.
</p>
</div>

<div id="org7bab473" class="outline-3">
<h3 id="org7bab473">Kernel Ridge Regression</h3>
<div class="outline-text-3" id="text-org7bab473">
<hr />
<p>
<span class="underline">Center</span> X and y so their means are zero; e.g. X_i &lt;- X_i - mu_X
This lets us replace I' with I in normal equations:
</p>

<pre class="example">
  T                   T
(X X + lambda I) w = X y
</pre>

<p>
[When we center X and y, the expected value of the intercept w_{d+1} = alpha
 is zero.  The actual value won't usually be exactly zero, but it will be close
 enough that we won't do much harm by penalizing the intercept.]
</p>

<pre class="example">
            1      T     T        T                 1
  =&gt;  w = ------ (X y - X X w) = X a    where a = ------ (y - X w)
          lambda                                  lambda        ^      T
                                                                | w = X a
This shows that w is a linear combo of samples.  To compute a:

                    T                 T            -1
  lambda a = (y - XX a)   =&gt;   a = (XX  + lambda I)   y

a is the _dual_solution_; solves the _dual_form_ of ridge regression:
  ------------------------------------------------------
  |                          T      2            T  2 |
  | Find a that minimizes |XX a - y|  + lambda |X a|  |
  ------------------------------------------------------

</pre>

<p>
Regression fn is
</p>

<pre class="example">
          T     T       n       T
  h(z) = w z = a X z = sum a  (X  z)          &lt;= weighted sum of inner products
                       i=1  i   i
               T
Let k(x, z) = x z be _kernel_fn_.
[Later, we'll replace x and z with Phi(x) and Phi(z), and that's where the
 magic will happen.]

          T
Let K = XX  be n-by-n _kernel_matrix_.  Note K   = k(X , X ).
                                              ij      i   j
K is singular if n &gt; d.  [And sometimes otherwise.]
In that case, no solution if lambda = 0.  [Then the penalty term is necessary.]
</pre>
<p>

Summary of kernel ridge reg.:
</p>
<pre class="example">
  K   = k(X , X )   for all i, j             &lt;= O(n^2 d) time
   ij      i   j
  Solve (K + lambda I) a = y    for a        &lt;= O(n^3) time
  for each test pt z
            n
    h(z) = sum a  k(X , z)                   &lt;= O(nd) time
           i=1  i    i

Does not use X directly!  Only k.            [This will become important soon.]
</pre>

<p>
Dual:    solve n-by-n linear system
Primal:  solve d-by-d linear system
</p>

<p>
[So we prefer the dual form when d &gt; n.  If we add new features like polynomial
 terms, this d goes up, but the d in the kernel running time doesn't.]
</p>
</div>
</div>

<div id="org872cb61" class="outline-3">
<h3 id="org872cb61">The Kernel Trick (aka <span class="underline">kernelization</span>)</h3>
<div class="outline-text-3" id="text-org872cb61">
<hr />
<p>
[Here's the magic part.  We will see that we can compute a polynomial kernel
 that involves many monomial terms without actually computing those terms.]
</p>

<pre class="example">
                                                   T      p
The _polynomial_kernel_ of degree p is k(x, z) = (x z + 1)

            T      p         T
Theorem:  (x z + 1)  = Phi(x)  Phi(z) where Phi(x) contains every monomial in x
                                            of degree 0...p.
Example for d = 2, p = 2:

    T      2    2 2    2 2
  (x z + 1)  = x z  + x z  + 2 x z x z  + 2 x z  + 2 x z  + 1
                1 1    2 2      1 1 2 2      1 1      2 2

                 2    2     _          _        _
             = [x    x    \/2 x x    \/2 x    \/2 x    1]
                 1    2        1 2        1        2
                 2    2     _          _        _        T
               [z    z    \/2 z z    \/2 z    \/2 z    1]
                 1    2        1 2        1        2

                     T
             = Phi(x)  Phi(z)                 [This is how we're defining Phi.]


</pre>
<p>
[Notice the factors of sqrt(2).  If you try a higher polynomial degree p,
 you'll see a wider variety of these constants.  We have no control of the
 constants used in Phi(x), but they don't matter very much, because the primal
 weights w will scale themselves to compensate.  Even though we aren't directly
 computing the primal weights...they still implicitly exist.]
</p>

<pre class="example">
Key win:  compute Phi(x)^T Phi(z) in O(d) time instead of O(d^p),
          even though Phi(x) has length O(d^p).

Kernel ridge regression replaces X_i with Phi(X_i):
                      T
  Let k(x, z) = Phi(x)  Phi(z)
</pre>


<p>
[I think what we've done here is pretty mind-blowing:  we can now do polynomial
 regression with an exponentially long, high-order polynomial in less time than
 it would take even to compute the final polynomial.  The running time is
 sublinear, actually much smaller than linear, in the size of the Phi vectors.]
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 五</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 日 14:57</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
