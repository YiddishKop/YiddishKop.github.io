<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 æ—¥ 14:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 12 STATISTICAL JUSTIFICATIONS FOR REGRESSION</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 12 STATISTICAL JUSTIFICATIONS FOR REGRESSION</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf83b780">lec 12 STATISTICAL JUSTIFICATIONS FOR REGRESSION</a>
<ul>
<li><a href="#orgf1eed56">Least-Squares Regression from Maximum Likelihood</a></li>
<li><a href="#org09ee56f">Empirical Risk</a></li>
<li><a href="#org2c7694c">Logistic regression from maximum likelihood</a></li>
</ul>
</li>
<li><a href="#org03a6de5">lec 12.2 THE BIAS-VARIANCE DECOMPOSITION</a>
<ul>
<li><a href="#org236b36f">Example:  Least-Squares Linear Reg.</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="orgf83b780" class="outline-2">
<h2 id="orgf83b780">lec 12 STATISTICAL JUSTIFICATIONS FOR REGRESSION</h2>
<div class="outline-text-2" id="text-orgf83b780">
<p>
So far, I've talked about regression as a way to fit curves to points. Recall
how early in the semester I divided machine learning into 4 levels: the
application, the model, the optimization problem, and the optimization
algorithm. My last two lectures about regression were at the bottom two levels:
optimization. The cost functions that we optimize are somewhat arbitrary. Today
let's take a step back to the second level, the model. I will describe some
models, how they lead to those optimization problems, and how they contribute to
underfitting or overfitting.
</p>

<p>
Typical model of reality:
</p>
<pre class="example">
- samples come from unknown prob. distribution:  X_i ~ D
- y-values are sum of unknown, non-random surface + random noise:  for all X_i,

  y  = f(X ) + epsilon ,                    epsilon  ~ D', D' has mean zero
   i      i           i                            i
</pre>

<p>
[We are positing that reality is described by a function f.  We don't know f,
 but f is not a random variable; it represents a real relationship between
 x and y that we can estimate.  We add to that a random variable epsilon, which
 represents measurement errors and all the other sources of statistical error
 when we measure real-world phenomena.  Notice that the noise is independent of
 x.  That's a pretty big assumption, and often it does not apply in practice,
 but that's all we'll have time to deal with this semester.  Also notice that
 this model leaves out systematic errors, like when your measuring device adds
 one to every measurement, because we usually can't diagnose systematic errors
 from data alone.]
</p>

<pre class="example">
Goal of regression:  find h that estimates f.

Ideal approach:  choose h(x) = E [Y | X = x] = f(x) + E[epsilon] = f(x)
                                Y
</pre>

<p>
[If this expectation exists at all, it partly justifies our model of reality.
 We can retroactively define f to be this expectation.]
[Draw figure showing example f, distribution for a fixed x.]
</p>
</div>

<div id="orgf1eed56" class="outline-3">
<h3 id="orgf1eed56">Least-Squares Regression from Maximum Likelihood</h3>
<div class="outline-text-3" id="text-orgf1eed56">
<hr />
<pre class="example">
Suppose epsilon  ~ N(0, sigma^2);  then y  ~ N(f(X ), sigma^2)
               i                         i        i
Recall that log likelihood for normal dist. is
                         2
               |y_i - mu|                     &lt;=  mu = f(X )
  ln P(y ) = - ----------- - constant,                    i
        i       2 sigma^2

  ln (P(y ) P(y ) ... P(y )) = ln P(y ) + ln P(y ) + ... ln P(y )
         1     2         n           1          2              n

Takeaway:  Max likelihood =&gt; find f by least-squares
</pre>

<p>
[So if the noise is normally distributed, maximum likelihood justifies using
 the least-squares cost function.]
[However, I've told you in previous lectures that least-squares is very
 sensitive to outliers.  If the error is truly normally distributed, that's not
 a big deal, especially when you have a lot of samples.  But in the real world,
 the real distribution of outliers often isn't normal.  Outliers might come
 from wrongly measured measurements, data entry errors, anomalous events, or
 just not having a normal distribution.  When you have a heavy-tailed
 distribution, for example, least-squares isn't a good choice.]
</p>

<p>

</p>
</div>
</div>
<div id="org09ee56f" class="outline-3">
<h3 id="org09ee56f">Empirical Risk</h3>
<div class="outline-text-3" id="text-org09ee56f">
<hr />
<p>
The <span class="underline">risk</span> for hypothesis h is expected loss R(h) = E[L] over all x, y.
Discriminative model:  we don't know X's dist. D.  How can we minimize risk?
[If we have a generative model, we can estimate the probability distributions
 for X and Y and derive the expected loss.  That's what we did for Gaussian
 discriminant analysis.  But today I'm assuming we don't have a generative
 model, so we don't know those probabilities.  Instead, we're going to
 approximate the distribution in a very crude way:  we pretend that the samples
 <b>are</b> the distribution.]
</p>

<pre class="example">
_Empirical_distribution_:  a discrete probability distribution that IS the
                           sample set, with each sample equally likely
_Empirical_risk_:  expected loss under empirical distribution
                   ^      1  n
                   R(h) = - sum L(h(X_i), y_i)
                          n i=1
</pre>
<p>
[The hat on the R indicates it's only a cheap approximation of the true,
 unknown statistical risk we really want to optimize.  Often, this is the best
 we can do.  For many but not all distributions, it converges to the true risk
 in the limit as n -&gt; infinity.]
</p>

<p>
Takeaway:  this is why we [usually] minimize the sum of loss fns.
</p>
</div>
</div>

<div id="org2c7694c" class="outline-3">
<h3 id="org2c7694c">Logistic regression from maximum likelihood</h3>
<div class="outline-text-3" id="text-org2c7694c">
<hr />
<p>
If we accept the logistic regression fn, what cost fn should we use?
</p>

<p>
Given arbitrary sample x, write probability it is in (not in) the class:
(Fictitious dimension:  x ends w/1; w ends w/alpha)
</p>

<pre class="example">

P(y = 1 | x; w) = h(x; w)                              &lt;=  h(x; w) = s(w^T x)
P(y = 0 | x; w) = 1 - h(x; w)                              [s is logistic fn]


</pre>
<p>
Combine these 2 facts into 1:
</p>

<pre class="example">
                    y           1-y       [A bit of a hack, but it works
  P(y | x; w) = h(x)  (1 - h(x))           nicely for intermediate values of y]

Likelihood is
                       n
  L(w; x , ..., x ) = prod P(y  | X ; w)
        1        n    i=1     i    i

Log likelihood is
                    n
  l(w) = ln L(w) = sum ln P(y  | X ; w)
                   i=1       i    i
          n  /                                       \
       = sum | y  ln h(X ) + (1 - y ) ln (1 - h(X )) |
         i=1 \  i       i          i             i   /

...which is negated logistic cost fn J(w).
We want to maximize log likelihood  =&gt;  minimize J.
</pre>


<p>

</p>
</div>
</div>
</div>
<div id="org03a6de5" class="outline-2">
<h2 id="org03a6de5">lec 12.2 THE BIAS-VARIANCE DECOMPOSITION</h2>
<div class="outline-text-2" id="text-org03a6de5">
<p>
<code>=============================</code>
</p>
<pre class="example">
There are 2 sources of error in a hypothesis h:
_bias_:  error due to inability of hypothesis h to fit f perfectly
         e.g. fitting quadratic f with a linear h
_variance_:  error due to fitting random noise in data
             e.g. we fit linear f with a linear h, yet h != f.

Model:  generate samples X_1 ... X_n from some distribution D
        values y_i = f(X_i) + epsilon_i
        fit hypothesis h to X, y
</pre>

<p>
Now h is a random variable; i.e. its weights are random
</p>


<p>
Consider an arbitrary pt z in R^d (not necessarily a sample!) and
gamma = f(z) + epsilon.  [So z is <b>arbitrary</b>, whereas gamma is <b>random</b>.]
</p>

<p>
Note:  E[gamma] = f(z); Var(gamma) = Var(epsilon)
       [So the mean comes from f, and the variance comes from epsilon.]
</p>

<p>
Risk fn when loss is squared error:
</p>

<pre class="example">
R(h) = E[L(h(z), gamma)]
       ^
       | we take expectation over possible training sets X, y
                                           and values of gamma
</pre>
<p>
[Stop and take a close look at this.  Remember that the hypothesis h is
 a random variable.  We are taking a mean over the probability distribution of
 hypotheses.  That seems pretty weird if you've never seen it before.  But
 remember, the training data X and y come from probability distributions.  We
 use the training data to choose weights, so the weights that define h also
 come from some probability distribution.  It might be pretty hard to work out
 what that distribution is, but it exists.  This "E[.]" is integrating the loss
 over all possible values of the weights.]
</p>

<pre class="example">
= E[(h(z) - gamma)^2]
= E[h(z)^2] + E[gamma^2] - 2 E[gamma h(z)]
                             [Observe that gamma &amp; h(z) are independent]
= Var(h(z)) + E[h(z)]^2 + Var(gamma) + E[gamma]^2 - 2 E[gamma] E[h(z)]
= (E[h(z)] - E[gamma])^2 + Var(h(z)) + Var(gamma)
= E[h(z) - f(z)]^2 + Var(h(z)) + Var(epsilon)
  \______________/   \_______/   \__________/
       bias^2        variance    _irreducible_error_
      of method      of method                            [Show bvn.pdf]

</pre>

<p>
This is pointwise version.  Mean version:  let z ~ D be random variable; take
expectation of bias^2, variance over z.
</p>

<p>
[So you can decompose one test point's error into these three components, or
 you can decompose the error of the hypothesis over its entire range into three
 components, which tells you how big they'll be on a large test set.]
</p>

<ul class="org-ul">
<li>Underfitting = too much bias</li>
<li>Overfitting caused by too much variance</li>
<li>Training error reflects bias but not variance; test error reflects both
[which is why low training error can fool you when you've overfitted]</li>
<li>For many distributions, variance -&gt; 0 as n -&gt; infinity</li>
<li>If h can fit f exactly, for many distributions bias -&gt; 0 as n -&gt; infinity</li>
<li>If h cannot fit f well, bias is large at "most" points</li>
<li>Adding a good feature reduces bias; adding a bad feature rarely increases it</li>
<li>Adding a feature usually increases variance</li>
<li>Can't reduce irreducible error [hence its name]</li>
<li>Noise in test set affects only Var(epsilon);
noise in training set affects only bias &amp; Var(h)</li>
<li>For real-world data, f is rarely knowable (and noise model might be wrong)</li>
<li>But we can test learning algs by choosing f &amp; making synthetic data</li>
</ul>

<p>
[Show trade-off figures with spline, increasing degrees of freedom.]
</p>
</div>

<div id="org236b36f" class="outline-3">
<h3 id="org236b36f">Example:  Least-Squares Linear Reg.</h3>
<div class="outline-text-3" id="text-org236b36f">
<hr />
<p>
For simplicity, assume no fictitious dimension.
[This implies that our linear regression fn has to be zero at the origin.]
</p>

<pre class="example">
Model:  f(z) = v^T z   (reality is linear)
        [So we could fit f perfectly with a linear h if not for the noise in
         the training set.]
        Let e be noise n-vector, e ~ N(0, sigma^2)
        Training values:  y = Xv + e
        [X &amp; y are the inputs to linear regression.  We don't know v or e.]

</pre>

<p>
Lin. reg. computes weights
</p>

<pre class="example">
       +     +                +
  w = X y = X (Xv + e) = v + X e
                             \_/ noise in weights

                            T     T        T  +       T  +
BIAS is E[h(z) - f(z)] = E[w z - v z] = E[z  X  e] = z  X  E[e] = 0

Warning:  This does not mean h(z) - f(z) is everywhere 0!
          Sometimes +ve, sometimes -ve, mean over training sets is 0.
          [Those deviations from the mean are captured in the variance.]
</pre>

<p>
[Not all learning methods give you a bias of zero when a perfect fit is
 possible; here it's a benefit of the squared error loss fn.  With a different
 loss fn, we might have a nonzero bias even fitting a linear h to a linear f.]
</p>

<pre class="example">
                             T     T  +           T  +
VARIANCE is Var(h(z)) = Var(z v + z  X  e) = Var(z  X  e)
</pre>

<p>
[This is the dot product of a vector z^T X^+ with an isotropic, normally
 distributed vector e.  The dot product reduces it to a one-dimensional
 Gaussian along the direction z^T X^+, so its variance is just the variance
 of the 1D Gaussian times the squared length of the vector z^T X^+.]
</p>

<pre class="example">
       2 | T  +|2        2  T   T  -1  T    T  -1
= sigma  |z  X |  = sigma  z  (X X)   X X (X X)   z

       2  T   T  -1
= sigma  z  (X X)   z
</pre>

<p>
If we choose coordinate system so E[X] = 0,
then X^T X -&gt; n Cov(D) as n -&gt; infinity, so one can show that for z ~ D,
</p>

<p>
Var(h(z)) ~ sigma^2 d / n
</p>

<p>
[Where d is the dimension--the number of features per sample.]
</p>

<pre class="example">
Takeaways:  Bias can be zero when hypothesis function can fit the real one!
              [Nice property of squared error loss fn.]
            Variance portion of RSS (overfitting) decreases as 1 / n (samples),
                                                  increases as d (features).
</pre>

<p>
[I've used linear regression because it's a relatively simple example.  But
 this bias-variance trade-off applies to nearly all learning algorithms,
 including classification as well as regression.  Of course, for many learning
 algorithms the math gets a lot more complicated than this, if you can do it at
 all.]
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 äº”</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 æ—¥ 14:57</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
