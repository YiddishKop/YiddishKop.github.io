<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 æ—¥ 14:57 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>lec 10 REGRESSION aka Fitting Curves to Data</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">lec 10 REGRESSION aka Fitting Curves to Data</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orga0b6955">lec 10 REGRESSION aka Fitting Curves to Data</a></li>
<li><a href="#org9d18608">lec 10.2 LEAST-SQUARES LINEAR REGRESSION</a></li>
<li><a href="#orge6cce34">lec 10.3 LOGISTIC REGRESSION (David Cox, 1958)</a></li>
</ul>
</div>
</div>


<div id="orga0b6955" class="outline-2">
<h2 id="orga0b6955">lec 10 REGRESSION aka Fitting Curves to Data</h2>
<div class="outline-text-2" id="text-orga0b6955">
<p>
Classification:  given sample x, predict class (often binary)
Regression:      given sample x, predict a numerical value
</p>

<p>
[Classification gives a discrete prediction, whereas regression gives us
 a quantitative prediction, usually on a continuous scale.]
[We've already seen an example of regression in Gaussian discriminant analysis.
 QDA and LDA don't just give us a classifier; they also give us the probability
 that a particular class label is correct.  So QDA and LDA implicitly do
 regression on probability values.]
</p>

<ul class="org-ul">
<li>Choose form of regression fn h(x; p) with parameters p     (h = <span class="underline">hypothesis</span>)
<ul class="org-ul">
<li>like predictor fn in classification [e.g. linear, quadratic, logistic in x]</li>
</ul></li>
<li>Choose a cost fn (objective fn) to optimize
<ul class="org-ul">
<li>usually based on a loss fn; e.g. risk fn = expected loss</li>
</ul></li>
</ul>

<pre class="example">
Some regression fns:
(1)  linear:     h(x; w, alpha) = w^T x + alpha
(2)  polynomial
     [equivalent to linear regression with added polynomial features]
(3)  logistic:   h(x; w, alpha) = s(w^T x + alpha)
                                             1
     recall:     logistic fn s(gamma) = -----------
                                             -gamma
                                        1 + e
     [This choice is interesting.  You'll recall that LDA produces a posterior
      probability fn with this equation.  So this equation seems to be
      a natural form for modeling certain probabilities.  If we want to model
      probabilities, sometimes we use LDA; but alternatively, we could skip
      fitting Gaussians to samples, and instead just directly try to fit
      a logistic function to a set of probabilities.]

</pre>

<pre class="example">
Some loss fns:  let z be prediction h(x); y be true value
(A)  L(z, y) = (z - y)^2                        _squared_error_
(B)  L(z, y) = |z - y|                          _absolute_error_
(C)  L(z, y) = - y ln z - (1 - y) ln (1 - z)    _logistic_loss_:  y in [0, 1],
                                                                  z in (0, 1)
Some cost fns to minimize:
            1  n
(a)  J(h) = - sum L(h(X ), y )                  _mean_loss_
            n i=1      i    i                   [you can leave out the "1/n"]
             n
(b)  J(h) = max L(h(X ), y )                    _maximum_loss_
            i=1      i    i
             n
(c)  J(h) = sum omega_i L(h(X ), y )            _weighted_sum_ [some samples
            i=1              i    i             more important than others]
            1  n                           2
(d)  J(h) = - sum L(h(X ), y ) + lambda |w|     l  _penalized_/_regularized_
            n i=1      i    i                    2
            1  n
(e)  J(h) = - sum L(h(X ), y ) + lambda |w|     l  _penalized_/_regularized_
            n i=1      i    i              l_1   1

Least-squares linear regr.:  (1) + (A) + (a) \
Weighted least-squ. linear:  (1) + (A) + (c) | quadratic; minimize w/calculus
Ridge regression:            (1) + (A) + (d) /
Lasso:                       (1) + (A) + (e) \ minimize w/gradient descent
Logistic reg.:               (3) + (C) + (a) /
Least absolute deviations:   (1) + (B) + (a) \ minimize w/linear programming
Chebyshev criterion:         (1) + (B) + (b) /

</pre>

<p>
[I have given you several choices of regression fn form, several choices of
 loss fn, and several choices of objective fn.  These are interchangeable parts
 where you can snap one part out and replace it with a different one.  But the
 optimization algorithm and its speed depend crucially on which parts you pick.
 Let's see some examples.]
</p>
</div>
</div>

<div id="org9d18608" class="outline-2">
<h2 id="org9d18608">lec 10.2 LEAST-SQUARES LINEAR REGRESSION</h2>
<div class="outline-text-2" id="text-org9d18608">
<p>
linear regression fn (1) + squared loss fn (A) + cost fn (a).
</p>

<pre class="example">
-----------------------------------------------------------
|                               n                       2 |
| Find w, alpha that minimizes sum (X  . w + alpha - y )  |
|                              i=1   i                i   |
-----------------------------------------------------------
</pre>
<p>
[Show linear regression example (linregress.pdf)]
</p>

<pre class="example">
Convention:  X is n-by-d _design_matrix_ of samples
             # y is d-vector of dependent scalars.
  -                                             -                 -     -
  | X_11   X_12   ...   |  X_1j  |   ...   X_1d |                 | y_1 |
  | X_21   X_22         |  X_2j  |         X_2d |                 | y_2 |
  |  ...                |        |              |                 |     |
  | --------------------+--------+------------- |           T     | ... |
  | X_i1   X_i2         |  X_ij  |         X_id | &lt; sample X      |     |
  | -----------------------------+------------- |           i     |     |
  |  ...                |        |              |                 |     |
  | X_n1   X_n2   ...   |  X_nj  |         X_nd |                 | y_n |
  -                                             -                 -     -
                           ^                                        ^
                           feature column X                         y
                                           *j
Usually n &gt; d.

</pre>

<p>
Recall fictitious dimension trick [from Lecture 3]:  replace x . w + alpha with
</p>

<pre class="example">
                    -       -
                    |  w_1  |
  [ x_1  x_2  1 ] . |  w_2  |
                    | alpha |
                    -       -

Now X is an n-by-(d+1) matrix;   w is a (d+1)-vector.

  -----------------------------------
  |                               2 |
  | Find w that minimizes |Xw - y|  | = RSS(w), for _residual_sum_of_squares_
  -----------------------------------

</pre>

<p>
[We know X and y, but w is unknown; we want to solve for w.]
</p>

<pre class="example">
Optimize by calculus:

                   T T       T      T
minimize RSS(w) = w X Xw - 2y Xw + y y

                    T       T
    grad RSS    = 2X Xw - 2X y = 0

                    T      T
              ==&gt;  X Xw = X y                   &lt;== the _normal_equations_
                   \_/\_____/                       [w unknown; X &amp; y known]
        (d+1)-by-(d+1)  (d+1)-vectors
    T
If X X is singular, problem is underconstrained [because the samples all lie on
a common hyperplane.  Notice that X^T X is always positive semidefinite.]

</pre>
<p>

</p>
<pre class="example">
                                     T  -1  T
We use a linear solver to find w = (X X)   X y         [never actually invert!]
                                   \________/
                                   X^+, the _pseudoinverse_ of X, (d+1)-by-n

</pre>

<p>
[X is usually not square, so it can't have an inverse.  However, every X has
 a pseudoinverse X^+, and if X has rank d+1, then X^+ is a "left inverse".]
</p>

<pre class="example">
           +      T  -1  T
Observe:  X X = (X X)   X X = I   &lt;= (d+1)-by-(d+1)   [which explains its name]

                                        ^                  ^          +
Observe:  the predicted values of y are y  = h(x )   ==&gt;   y = Xw = XX y = Hy
                                         i      i
                      +
          where H = XX  is called the _hat_matrix_ because it puts the hat on y
                        n-by-n [and if n &gt; d+1, it is not the identity matrix!]

</pre>

<p>
Interpretation as a projection:
</p>
<pre class="example">
  ^
- y = Xw in R^n is a linear combination of columns of X (one per feature)
- For fixed X, varying w, Xw is subspace of R^n spanned by columns

               O y
       ________|___________   Figure in n-dimensional space (1 dim/sample)
      /       _|          /   NOT d-dimensional feature space (row space of X).
     /       | | ^       /
    /        ' O y = Xw /  &lt;== subspace spanned by X's columns
   /                   /       (at most d+1 dimensions)
  ---------------------

              ^                  ^
- Minimizing |y - y| finds point y nearest y on subspace
  ==&gt;  projects y onto subspace
       [the vertical line is the direction of projection and the error vector]
                                                              T
- Error is smallest when line is perpendicular to subspace:  X (Xw - y) = 0
                                                      ==&gt; the normal equations!
- Hat matrix H does the projecting.  [Also sometimes called projection matrix.]

</pre>

<p>
Advantages:
</p>
<ul class="org-ul">
<li>Easy to compute; just solve a linear system.</li>
<li>Unique, stable solution.  [Except when the problem is underconstrained.]</li>
</ul>
<p>
Disadvantages:
</p>
<ul class="org-ul">
<li>Very sensitive to outliers, because error is squared!</li>
<li>Fails if X^T X is singular.</li>
</ul>

<p>
[Least-squares linear regression was apparently first posed and solved in 1801
 by the great mathematician Carl Friedrich Gauss, who used least squares to
 predict the trajectory of the planetoid Ceres.  A paper he wrote on the topic
 is regarded as the birth of modern linear algebra.]
</p>

<p>

</p>
</div>
</div>
<div id="orge6cce34" class="outline-2">
<h2 id="orge6cce34">lec 10.3 LOGISTIC REGRESSION (David Cox, 1958)</h2>
<div class="outline-text-2" id="text-orge6cce34">
<p>
<code>=================</code>
logistic regression fn (3) + logistic loss fn (C) + cost fn (a).
Fits "probabilities" in range (0, 1).
</p>

<p>
Usually used for classification.  The input y_i's <b>can</b> be probabilities,
but in most applications they're all 0 or 1.
</p>

<p>
QDA, LDA:  generative models
logistic regression:  discriminative model
</p>

<p>
With X and w including the fictitious dimension; alpha is w's last component...
</p>

<pre class="example">
-------------------------------------------------------------
| Find w that maximizes                                     |
|                                                           |
|      n  /                                               \ |
| J = sum | y  ln s(X  . w) + (1 - y ) ln (1 - s(X  . w)) | |
|     i=1 \  i       i              i             i       / |
-------------------------------------------------------------
</pre>
<p>
[Note that we are maximizing, not minimizing, this function because
 I've flipped the sign of the logistic loss (C).]
[Show log loss plots (logloss0.pdf, logloss.7.pdf)]
</p>

<p>
-J(w) is convex!  [J is "concave".]  Solve by gradient ascent.
</p>

<p>
[To do gradient ascent, we'll need to compute some derivatives.]
</p>

<pre class="example">
                                              -gamma
                   d          1              e
  s'(gamma)  =  -------  -----------  =  --------------
                d gamma       -gamma           -gamma 2
                         1 + e           (1 + e      )

             =  s(gamma) (1 - s(gamma))          [Show s' plot (dlogistic.pdf)]

Let s  = s(X  . w)
     i      i
                    / y_i            1 - y_i          \
    grad  J  =  sum | --- grad s_i - ------- grad s_i |
        w           \ s_i            1 - s_i          /

                    / y_i   1 - y_i \
             =  sum | --- - ------- | s  (1 - s ) X
                    \ s_i   1 - s_i /  i       i   i

             =  sum (y  - s ) X
                      i    i   i
                                         n
Gradient ascent rule:  w &lt;- w + epsilon sum (y  - s(X  . w)) X
                                        i=1   i      i        i

Stochastic gradient ascent:  w &lt;- w + epsilon (y  - s(X  . w)) X
                                                i      i        i

</pre>
<p>
Works best if we shuffle samples in random order, process one by one.
For very large n, sometimes converges before we visit all samples!
</p>

<p>
[This looks a lot like the perceptron learning rule.  The only difference is
 that the "- s_i" part is new.]
</p>

<p>
Starting from w = 0 works well in practice.
</p>

<p>
[Show mwascom's logistic regression plot (problogistic.png) from
<a href="http://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression">http://stackoverflow.com/questions/28256058/plotting-decision-boundary-of-logistic-regression</a>]
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2017-06-22 äº”</p>
<p class="author">Author: yiddi</p>
<p class="date">Created: 2018-08-12 æ—¥ 14:57</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
