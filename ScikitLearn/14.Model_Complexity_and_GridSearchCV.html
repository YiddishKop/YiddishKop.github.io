<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 21:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>scikit-笔记13:模型复杂度与GridSearchCV</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">scikit-笔记13:模型复杂度与GridSearchCV</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orga93672b">1. Parameter selection, Validation, and Testing</a>
<ul>
<li><a href="#orgd83fe67">1.1. bias variance tradeoff</a></li>
<li><a href="#org9ca7987">1.2. Hyperparameters, Over-fitting, and Under-fitting</a></li>
</ul>
</li>
<li><a href="#orgbb14d70">2. Misc tools</a>
<ul>
<li><a href="#org1f62338">2.1. scikit-learn</a>
<ul>
<li><a href="#orgc4deff0">2.1.1. ML models by now</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt

</pre>
</div>

<div id="orga93672b" class="outline-2">
<h2 id="orga93672b"><span class="section-number-2">1</span> Parameter selection, Validation, and Testing</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="orgd83fe67" class="outline-3">
<h3 id="orgd83fe67"><span class="section-number-3">1.1</span> bias variance tradeoff</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Most models have parameters that influence how complex a model they can learn.
Remember using KNeighborsRegressor. If we change the number of neighbors we
consider, we get a smoother and smoother prediction:
</p>



<div class="figure">
<p><img src="figures/plot_kneigbors_regularization.png" alt="plot_kneigbors_regularization.png" />
</p>
</div>

<p>
In the above figure, we see fits for three different values of n_neighbors:
</p>

<ul class="org-ul">
<li>For n_neighbors=2, the data is overfit, the model is too flexible and can
adjust too much to the noise in the training data.</li>

<li>For n_neighbors=20, the model is not flexible enough, and can not model the
variation in the data appropriately.</li>

<li>For n_neighbors = 5, we have found a good mid-point. It fits the</li>
</ul>
<p>
data fairly well, and does not suffer from the overfit or underfit problems seen
in the figures on either side.
</p>


<p>
What we would like is <b>a way to quantitatively identify overfit and underfit</b>,
and optimize the hyperparameters (in this case, the polynomial degree d) in
order to determine the best algorithm.
</p>

<p>
We trade off between:
</p>

<ul class="org-ul">
<li>remembering too much about the particularities and noise of the training data</li>
<li>not modeling enough of the variability.</li>
</ul>

<p>
This is a trade-off that needs to be made in basically every machine learning
application and is a central concept, called <b>bias-variance-tradeoff</b> or
"overfitting vs underfitting".
</p>


<p>
<img src="figures/overfitting_underfitting_cartoon.png" alt="overfitting_underfitting_cartoon.png" />
​
</p>
</div>
</div>
<div id="org9ca7987" class="outline-3">
<h3 id="org9ca7987"><span class="section-number-3">1.2</span> Hyperparameters, Over-fitting, and Under-fitting</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Unfortunately, there is no general rule how to find the sweet spot, and so
machine learning practitioners have to find the best trade-off of
<b>model-complexity</b> and <b>generalization</b> by <b>trying</b> several <b>hyperparameter</b>
settings.
</p>

<p>
Hyperparameters are the internal knobs or tuning parameters of a machine
learning algorithm (in contrast to model parameters that the algorithm learns
from the training data &#x2013; for example, the weight coefficients of a linear
regression model); the number of <b>k in K-nearest neighbors</b> is such a
<b>hyperparameter</b>.
</p>

<p>
Most commonly this "<b>hyperparameter tuning</b>" is done using a <b>brute force
search</b>, for example over multiple values of n_neighbors:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> cross_val_score, KFold
<span class="org-keyword">from</span> sklearn.neighbors <span class="org-keyword">import</span> KNeighborsRegressor
<span class="org-comment-delimiter"># </span><span class="org-comment">generate toy dataset:</span>
<span class="org-variable-name">x</span> = np.linspace(-<span class="org-highlight-numbers-number">3</span>, <span class="org-highlight-numbers-number">3</span>, <span class="org-highlight-numbers-number">100</span>)
<span class="org-variable-name">rng</span> = np.random.RandomState(<span class="org-highlight-numbers-number">42</span>)
<span class="org-variable-name">y</span> = np.sin(<span class="org-highlight-numbers-number">4</span> * x) + x + rng.normal(size=<span class="org-builtin">len</span>(x))
<span class="org-variable-name">X</span> = x[:, np.newaxis]
<span class="org-variable-name">cv</span> = KFold(shuffle=<span class="org-constant">True</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">for each parameter setting do cross-validation:</span>
<span class="org-keyword">for</span> n_neighbors <span class="org-keyword">in</span> [<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">3</span>, <span class="org-highlight-numbers-number">5</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">20</span>]:
    <span class="org-variable-name">scores</span> = cross_val_score(KNeighborsRegressor(n_neighbors=n_neighbors), X, y, cv=cv)
    <span class="org-keyword">print</span>(<span class="org-string">"n_neighbors: %d, average score: %f"</span> % (n_neighbors, np.mean(scores)))

</pre>
</div>

<p>
There is a function in scikit-learn, called <b>validation_plot</b> to reproduce the
cartoon figure above. It plots one parameter, such as the number of neighbors,
against training and validation error (using cross-validation):
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> validation_curve
<span class="org-variable-name">n_neighbors</span> = [<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">3</span>, <span class="org-highlight-numbers-number">5</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">20</span>, <span class="org-highlight-numbers-number">50</span>]
<span class="org-variable-name">train_scores</span>, <span class="org-variable-name">test_scores</span> = validation_curve(KNeighborsRegressor(), X, y, param_name=<span class="org-string">"n_neighbors"</span>,
                                             param_range=n_neighbors, cv=cv)
plt.plot(n_neighbors, train_scores.mean(axis=<span class="org-highlight-numbers-number">1</span>), <span class="org-string">'b'</span>, label=<span class="org-string">"train accuracy"</span>)
plt.plot(n_neighbors, test_scores.mean(axis=<span class="org-highlight-numbers-number">1</span>), <span class="org-string">'g'</span>, label=<span class="org-string">"test accuracy"</span>)
plt.ylabel(<span class="org-string">'Accuracy'</span>)
plt.xlabel(<span class="org-string">'Number of neighbors'</span>)
plt.xlim([<span class="org-highlight-numbers-number">50</span>, <span class="org-highlight-numbers-number">0</span>])
plt.legend(loc=<span class="org-string">"best"</span>);

</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/25041Ejv.png" alt="25041Ejv.png" />
</p>
</div>

<p>
Note that many neighbors mean a "smooth" or "simple" model, so the plot uses a
reverted x axis.
</p>

<p>
If multiple parameters are important, like the parameters <code>C</code> and <code>gamma</code> in an
<code>SVM</code> (more about that later), all <b>possible combinations are tried</b>:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> cross_val_score, KFold
<span class="org-keyword">from</span> sklearn.svm <span class="org-keyword">import</span> SVR
<span class="org-comment-delimiter"># </span><span class="org-comment">each parameter setting do cross-validation:</span>
<span class="org-keyword">for</span> C <span class="org-keyword">in</span> [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">10</span>]:
    <span class="org-keyword">for</span> gamma <span class="org-keyword">in</span> [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>]:
        <span class="org-variable-name">scores</span> = cross_val_score(SVR(C=C, gamma=gamma), X, y, cv=cv)
        <span class="org-keyword">print</span>(<span class="org-string">"C: %f, gamma: %f, average score: %f"</span> % (C, gamma, np.mean(scores)))
</pre>
</div>

<p>
0 - e4827918-798a-4252-86d4-a7aa55d50830
</p>

<p>
As this is such a very common pattern, there is a built-in class for this in
scikit-learn, <code>GridSearchCV</code>. GridSearchCV takes a dictionary that describes the
parameters that should be tried and a model to train.
</p>

<p>
The grid of parameters is defined as a <b>dictionary</b>, where the keys are the
parameters and the values are the settings to be tested.
</p>

<blockquote>
<p>
. GridSearchCV illustration
.
.            ^ C
.            |      |      |      |      |
.            |      |      |      |      |
.       10   |-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|&#x2013;&#x2014;
.            |      |      |      |      |
.            |      |      |      |      |
.        1   |-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|&#x2013;&#x2014;
.            |      |      |      |      |
.            |      |      |      |      |
.      0.1   |-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|&#x2013;&#x2014;
.            |      |      |      |      |
.            |      |      |      |      |
.     0.01   |-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|&#x2013;&#x2014;
.            |      |      |      |      |
.            |      |      |      |      |
.    0.001   |-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|-&#x2013;&#x2014;|&#x2013;&#x2014;
.            |      |      |      |      |
.            |      |      |      |      |
.           -+-----------------------------&#x2013;&#x2014;&gt; gamma
.                0.001   0.01    0.1     1
</p>
</blockquote>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> GridSearchCV
<span class="org-variable-name">param_grid</span> = {<span class="org-string">'C'</span>: [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">10</span>], <span class="org-string">'gamma'</span>: [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>]}
<span class="org-variable-name">grid</span> = GridSearchCV(SVR(), param_grid=param_grid, cv=cv, verbose=<span class="org-highlight-numbers-number">3</span>)
</pre>
</div>

<p>
One of the great things about <code>GridSearchCV</code> is that it is a <b>meta-estimator</b>.
It takes an estimator like SVR above, and creates a new estimator, that behaves
exactly the same - in this case, like a regressor. So <b>we can call fit on it, to
train it</b>:
</p>

<div class="org-src-container">
<pre class="src src-ipython">grid.fit(X, y)
</pre>
</div>

<pre class="example">
GridSearchCV(cv=KFold(n_splits=3, random_state=None, shuffle=True),
error_score='raise',
estimator=SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',
kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False),
fit_params=None, iid=True, n_jobs=1,
param_grid={'C': [0.001, 0.01, 0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1, 1]},
pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
scoring=None, verbose=3)
</pre>

<p>
What fit does is a bit more involved then what we did above.
</p>

<ul class="org-ul">
<li>First, it runs the same loop with cross-validation, to find the best
parameter combination.</li>

<li>Once it has the best combination, it runs fit again on all data passed to
fit (without cross-validation), to built a single new model using the best
parameter setting.</li>
</ul>

<p>
Then, as with all models, we can use predict or score:
</p>

<div class="org-src-container">
<pre class="src src-ipython">grid.predict(X)
</pre>
</div>

<pre class="example">
array([-1.8 , -1.74, -1.71, -1.72, -1.77, -1.85, -1.97, -2.12, -2.3 ,
-2.49, -2.7 , -2.89, -3.07, -3.23, -3.35, -3.43, -3.45, -3.42,
-3.33, -3.19, -2.99, -2.75, -2.48, -2.19, -1.89, -1.59, -1.31,
-1.05, -0.84, -0.67, -0.55, -0.48, -0.47, -0.51, -0.59, -0.7 ,
-0.83, -0.98, -1.12, -1.25, -1.35, -1.42, -1.44, -1.42, -1.34,
-1.22, -1.05, -0.85, -0.61, -0.36, -0.1 ,  0.16,  0.4 ,  0.62,
0.8 ,  0.94,  1.04,  1.1 ,  1.11,  1.09,  1.03,  0.96,  0.87,
0.77,  0.69,  0.62,  0.57,  0.56,  0.58,  0.64,  0.74,  0.87,
1.02,  1.21,  1.4 ,  1.6 ,  1.8 ,  1.99,  2.16,  2.31,  2.42,
2.51,  2.56,  2.58,  2.56,  2.53,  2.47,  2.39,  2.31,  2.23,
2.15,  2.09,  2.04,  2.  ,  1.99,  2.  ,  2.02,  2.07,  2.12,  2.19])
</pre>

<p>
You can inspect the best parameters found by GridSearchCV in the best_params_
attribute, and the best score in the best_score_ attribute:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">print</span>(grid.best_score_)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">print</span>(grid.best_params_)
</pre>
</div>

<p>
But you can investigate the performance and much more for each set of parameter
values by accessing the <code>cv_results_</code> attributes. The <code>cv_results_</code> attribute is
a dictionary where each key is a string and each value is array. It can
therefore be used to make a pandas DataFrame.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-builtin">type</span>(grid.cv_results_)
</pre>
</div>

<pre class="example">
dict

</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">print</span>(grid.cv_results_.keys())
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-variable-name">cv_results</span> = pd.DataFrame(grid.cv_results_)
cv_results.head()

<span class="org-variable-name">cv_results_tiny</span> = cv_results[[<span class="org-string">'param_C'</span>, <span class="org-string">'param_gamma'</span>, <span class="org-string">'mean_test_score'</span>]]
cv_results_tiny.sort_values(by=<span class="org-string">'mean_test_score'</span>, ascending=<span class="org-constant">False</span>).head()
</pre>
</div>

<pre class="example">
param_C param_gamma  mean_test_score
19      10           1         0.761236
15       1           1         0.715955
18      10         0.1         0.676316
14       1         0.1         0.675168
17      10        0.01         0.643706
</pre>

<p>
&gt;&gt;&gt; why we need validation set: hypothesis teseting error.
There is a problem with using this score for evaluation, however. You might be
making what is called a multiple <b>hypothesis testing error</b>.
</p>

<p>
If you try very many parameter settings, some of them will work better just by
chance, and the score that you obtained might not reflect how your model would
perform on new unseen data. <b>Therefore, it is good to split off a separate
test-set before performing grid-search</b>. This pattern can be seen as a
<b>training-validation-test</b> split, and is common in machine learning:
</p>


<div class="figure">
<p><img src="figures/grid_search_cross_validation.png" alt="grid_search_cross_validation.png" />
</p>
</div>

<p>
We can do this very easily by splitting of some test data using
<code>train_test_split</code>, training <code>GridSearchCV</code> on the <b>training set</b>, and applying
the score method to the test set:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> train_test_split
<span class="org-variable-name">X_train</span>, <span class="org-variable-name">X_test</span>, <span class="org-variable-name">y_train</span>, <span class="org-variable-name">y_test</span> = train_test_split(X, y, random_state=<span class="org-highlight-numbers-number">1</span>)
<span class="org-variable-name">param_grid</span> = {<span class="org-string">'C'</span>: [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">10</span>], <span class="org-string">'gamma'</span>: [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>]}
<span class="org-variable-name">cv</span> = KFold(n_splits=<span class="org-highlight-numbers-number">10</span>, shuffle=<span class="org-constant">True</span>)
<span class="org-variable-name">grid</span> = GridSearchCV(SVR(), param_grid=param_grid, cv=cv)
grid.fit(X_train, y_train)
grid.score(X_test, y_test)
</pre>
</div>

<pre class="example">
0.7262035177984737

</pre>

<p>
We can also look at the parameters that were selected:
</p>

<div class="org-src-container">
<pre class="src src-ipython">grid.best_params_
</pre>
</div>

<pre class="example">
{'C': 10, 'gamma': 1}

</pre>

<p>
Some practitioners go for an easier scheme, splitting the data simply into three
parts, training, validation and testing. This is a possible alternative if your
training set is very large, or it is infeasible to train many models using
cross-validation because training a model takes very long. You can do this with
scikit-learn for example by splitting of a test-set and then applying
GridSearchCV with ShuffleSplit cross-validation with a single iteration: ​
</p>


<div class="figure">
<p><img src="figures/train_validation_test2.png" alt="train_validation_test2.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> train_test_split, ShuffleSplit
<span class="org-variable-name">X_train</span>, <span class="org-variable-name">X_test</span>, <span class="org-variable-name">y_train</span>, <span class="org-variable-name">y_test</span> = train_test_split(X, y, random_state=<span class="org-highlight-numbers-number">1</span>)
<span class="org-variable-name">param_grid</span> = {<span class="org-string">'C'</span>: [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">10</span>], <span class="org-string">'gamma'</span>: [<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">001</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">01</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>]}
<span class="org-variable-name">single_split_cv</span> = ShuffleSplit(n_splits=<span class="org-highlight-numbers-number">1</span>)
<span class="org-variable-name">grid</span> = GridSearchCV(SVR(), param_grid=param_grid, cv=single_split_cv, verbose=<span class="org-highlight-numbers-number">3</span>)
grid.fit(X_train, y_train)
grid.score(X_test, y_test)
</pre>
</div>

<pre class="example">
0.7262035177984737

</pre>

<p>
This is much faster, but might result in worse hyperparameters and therefore
worse results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">clf</span> = GridSearchCV(SVR(), param_grid=param_grid)
clf.fit(X_train, y_train)
clf.score(X_test, y_test)
</pre>
</div>

<pre class="example">
0.7262035177984737

</pre>

<p>
EXERCISE: Apply grid-search to find the best setting for the number of neighbors
in KNeighborsClassifier, and apply it to the digits dataset.
</p>
</div>
</div>
</div>

<div id="orgbb14d70" class="outline-2">
<h2 id="orgbb14d70"><span class="section-number-2">2</span> Misc tools</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="org1f62338" class="outline-3">
<h3 id="org1f62338"><span class="section-number-3">2.1</span> scikit-learn</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="orgc4deff0" class="outline-4">
<h4 id="orgc4deff0"><span class="section-number-4">2.1.1</span> ML models by now</h4>
<div class="outline-text-4" id="text-2-1-1">
<blockquote>
<ol class="org-ol">
<li>from sklearn.datasets import make_blobs</li>
<li>from sklearn.datasets import load_iris</li>
<li>from sklearn.model_selection import train_test_split</li>
<li>from sklearn.model_selection import cross_val_score</li>
<li>from sklearn.model_selection import KFold</li>
<li>from sklearn.model_selection import StratifiedKFold</li>
<li>from sklearn.model_selection import ShuffleSplit</li>
<li>from sklearn.model_selection import GridSearchCV     *</li>
<li>from sklearn.linear_model import LogisticRegression</li>
<li>from sklearn.linear_model import LinearRegression</li>
<li>from sklearn.neighbors import KNeighborsClassifier</li>
<li>from sklearn.neighbors import KNeighborsRegressor</li>
<li>from sklearn.preprocessing import StandardScaler</li>
<li>from sklearn.decomposition import PCA</li>
<li>from sklearn.metrics import confusion_matrix, accuracy_score</li>
<li>from sklearn.metrics import adjusted_rand_score</li>
<li>from sklearn.cluster import KMeans</li>
<li>from sklearn.cluster import KMeans</li>
<li>from sklearn.cluster import MeanShift</li>
<li>from sklearn.cluster import DBSCAN  # &lt;&lt;&lt; this algorithm has related sources in <a href="https://github.com/YiddishKop/org-notes/blob/master/ML/TaiDa_LiHongYi_ML/LiHongYi_ML_lec12_semisuper.org">LIHONGYI's lecture-12</a></li>
<li>from sklearn.cluster import AffinityPropagation</li>
<li>from sklearn.cluster import SpectralClustering</li>
<li>from sklearn.cluster import Ward</li>
<li>from sklearn.metrics import confusion_matrix</li>
<li>from sklearn.metrics import accuracy_score</li>
<li>from sklearn.metrics import adjusted_rand_score</li>
<li>from sklearn.feature_extraction import DictVectorizer</li>
<li>from sklearn.feature_extraction.text import CountVectorizer</li>
<li>from sklearn.feature_extraction.text import TfidfVectorizer</li>
<li>from sklearn.preprocessing import Imputer</li>
<li>from sklearn.dummy import DummyClassifier</li>
</ol>
</blockquote>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
