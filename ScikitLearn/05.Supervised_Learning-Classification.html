<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 21:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>scikit-笔记04:监督学习之分类问题</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">scikit-笔记04:监督学习之分类问题</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org288fc90">1. Supervised Learning Part 1 &#x2013; Classification</a>
<ul>
<li><a href="#orgb9843f6">1.1. preprocessing: generate, plot, split</a>
<ul>
<li><a href="#org8fb5993">1.1.1. generate two-kind-labeled-data points by <code>make_blobs</code></a></li>
<li><a href="#org3be17d3">1.1.2. plotting</a></li>
<li><a href="#org0409e6a">1.1.3. split datasets to: training and testing</a></li>
</ul>
</li>
<li><a href="#orgec9b8c2">1.2. The scikit-learn estimator API</a>
<ul>
<li><a href="#orgf23f856">1.2.1. instantiate the estimator object.</a></li>
<li><a href="#org5e04611">1.2.2. build the model by <code>estimator.fit(train_data, train_label)</code></a></li>
<li><a href="#orgc298bc3">1.2.3. using model to predict test_data by <code>estimator.predict(test_data)</code></a></li>
<li><a href="#org01a08d8">1.2.4. compare the standard label and predict label by <code>estimator.score(test_data,test_label)</code></a></li>
<li><a href="#org917c80f">1.2.5. method-1 : by <code>np.mean</code></a></li>
<li><a href="#org8d84852">1.2.6. method-2 : by <code>estimator.score(test_data,test_label)</code></a></li>
<li><a href="#orgf3f073d">1.2.7. draw this estimator as line by <code>plt.figures.plot(estimator,ALL_datasets)</code></a></li>
<li><a href="#org5942d49">1.2.8. get the parameters of this estimator by <code>estimator.&lt;attr&gt;_</code></a></li>
</ul>
</li>
<li><a href="#orgdde1d26">1.3. Another classifier: K Nearest Neighbors</a></li>
</ul>
</li>
<li><a href="#orgd41f7a6">2. Exercise</a></li>
<li><a href="#orgf58ec9a">3. Misc tools</a>
<ul>
<li><a href="#orgf06fc43">3.1. Scikit-learn</a>
<ul>
<li><a href="#orgf284813">3.1.1. sklearn.datasets.make_blobs</a></li>
<li><a href="#orgb541b2c">3.1.2. ML models by now</a></li>
</ul>
</li>
<li><a href="#org6c67c5d">3.2. Techs</a>
<ul>
<li><a href="#org2b6f6d8">3.2.1. do filtering by making a mask by other array</a></li>
<li><a href="#org2db74e8">3.2.2. common steps to generate data points and plot</a></li>
<li><a href="#org5e91fa7">3.2.3. method-1: by dataframe and groupby</a></li>
<li><a href="#org5b640e3">3.2.4. method-2: by ndarray and mask</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
</pre>
</div>

<div id="org288fc90" class="outline-2">
<h2 id="org288fc90"><span class="section-number-2">1</span> Supervised Learning Part 1 &#x2013; Classification</h2>
<div class="outline-text-2" id="text-1">
<p>
To visualize the workings of machine learning algorithms, it is often helpful to
study two-dimensional or one-dimensional data, that is data with only one or two
features. While in practice, datasets usually have many more features, it is
hard to plot high-dimensional data in on two-dimensional screens.
</p>

<p>
We will illustrate some very simple examples before we move on to more "real
world" data sets.
</p>
</div>
<div id="orgb9843f6" class="outline-3">
<h3 id="orgb9843f6"><span class="section-number-3">1.1</span> preprocessing: generate, plot, split</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="org8fb5993" class="outline-4">
<h4 id="org8fb5993"><span class="section-number-4">1.1.1</span> generate two-kind-labeled-data points by <code>make_blobs</code></h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
First, we will look at a <b>two class classification</b> problem in two dimensions.
We use the synthetic data generated by the make_blobs function.
</p>

<p>
<a href="#orgf284813">sklearn.datasets.make_blobs</a>
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-comment-delimiter"># </span><span class="org-comment">definition of make_blobs: produce multiple gaussians</span>
sklearn.datasets.make_blobs(n_samples=<span class="org-highlight-numbers-number">100</span>,
                            n_features=<span class="org-highlight-numbers-number">2</span>,
                            centers=<span class="org-highlight-numbers-number">3</span>,
                            cluster_std=<span class="org-highlight-numbers-number">1</span>.<span class="org-highlight-numbers-number">0</span>,
                            center_box=(-<span class="org-highlight-numbers-number">10</span>.<span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">10</span>.<span class="org-highlight-numbers-number">0</span>),
                            shuffle=<span class="org-constant">True</span>,
                            random_state=<span class="org-constant">None</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_blobs

<span class="org-variable-name">X</span>, <span class="org-variable-name">y</span> = make_blobs(centers=<span class="org-highlight-numbers-number">2</span>, random_state=<span class="org-highlight-numbers-number">0</span>)

<span class="org-keyword">print</span>(<span class="org-string">'X ~ n_samples x n_features:'</span>, X.shape)
<span class="org-keyword">print</span>(<span class="org-string">'y ~ n_samples:'</span>, y.shape)

<span class="org-keyword">print</span>(<span class="org-string">'\nFirst 5 samples:\n'</span>, X[:<span class="org-highlight-numbers-number">5</span>, :])
<span class="org-keyword">print</span>(<span class="org-string">'\nFirst 5 labels:'</span>, y[:<span class="org-highlight-numbers-number">5</span>])

</pre>
</div>
</div>
</div>

<div id="org3be17d3" class="outline-4">
<h4 id="org3be17d3"><span class="section-number-4">1.1.2</span> plotting</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
As the data is two-dimensional, we can plot each sample as a point in a
two-dimensional coordinate system, with the first feature being the x-axis and
the second feature being the y-axis.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-comment-delimiter"># </span><span class="org-comment">give different color to data with different label</span>
plt.scatter(X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'blue'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'0'</span>)
plt.scatter(X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'red'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'1'</span>, marker=<span class="org-string">'s'</span>)

plt.xlabel(<span class="org-string">'first feature'</span>)
plt.ylabel(<span class="org-string">'second feature'</span>)
plt.legend(loc=<span class="org-string">'upper right'</span>)
</pre>
</div>

<pre class="example">
&lt;matplotlib.legend.Legend at 0x7f9c530a16d8&gt;

</pre>

<div class="figure">
<p><img src="./obipy-resources/25041LzJ.png" alt="25041LzJ.png" />
</p>
</div>
</div>
</div>

<div id="org0409e6a" class="outline-4">
<h4 id="org0409e6a"><span class="section-number-4">1.1.3</span> split datasets to: training and testing</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
Classification is a supervised task, and since we are interested in its
performance on unseen data, we split our data into two parts:
</p>

<ul class="org-ul">
<li>a training set that the learning algorithm uses to fit the model</li>
<li>a test set to evaluate the generalization performance of the model</li>
</ul>

<p>
The <code>train_test_split</code> function from the <code>model_selection</code> module does that for us
&#x2013; we will use it to split a dataset into 75% training data and 25% test data.
</p>


<div class="figure">
<p><img src="figures/train_test_split_matrix.png" alt="train_test_split_matrix.png" />
</p>
</div>



<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> train_test_split

<span class="org-variable-name">X_train</span>, <span class="org-variable-name">X_test</span>, <span class="org-variable-name">y_train</span>, <span class="org-variable-name">y_test</span> = train_test_split(X, y,
                                                    test_size=<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">25</span>,
                                                    random_state=<span class="org-highlight-numbers-number">1234</span>,
                                                    stratify=y)

<span class="org-comment-delimiter"># </span><span class="org-comment">stratify : array-like or None (default is None)</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">If not None, data is split in a stratified fashion, using this as the class labels.</span>
</pre>
</div>
</div>
</div>
</div>

<div id="orgec9b8c2" class="outline-3">
<h3 id="orgec9b8c2"><span class="section-number-3">1.2</span> The scikit-learn estimator API</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="https://prnt.sc/jr49ti">https://prnt.sc/jr49ti</a>
</p>

<p>
Every algorithm is exposed in scikit-learn via an <code>Estimator object</code>. (All
models in scikit-learn have a very consistent interface). For instance, we first
import the logistic regression class.
</p>
</div>

<div id="orgf23f856" class="outline-4">
<h4 id="orgf23f856"><span class="section-number-4">1.2.1</span> instantiate the estimator object.</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
Estimator is an ML algorithm, like <code>linear/logistic regression</code>, <code>SVM</code> etc.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.linear_model <span class="org-keyword">import</span> LogisticRegression
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">classifier</span> = LogisticRegression()
X_train.shape
y_train.shape
</pre>
</div>

<pre class="example">
(75,)

</pre>
</div>
</div>

<div id="org5e04611" class="outline-4">
<h4 id="org5e04611"><span class="section-number-4">1.2.2</span> build the model by <code>estimator.fit(train_data, train_label)</code></h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
<code>estimator.fit(train_data, train_label)</code> will print the initial parameters of this model.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">parameters</th>
<th scope="col" class="org-left">description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">C=1.0,</td>
<td class="org-left">inverse of regularization strength</td>
</tr>

<tr>
<td class="org-left">class_weight=None,</td>
<td class="org-left">the weight of certain label, always used for handling unbalance data</td>
</tr>

<tr>
<td class="org-left">dual=False,</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">fit_intercept=True,</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">intercept_scaling=1,</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">max_iter=100,</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">multi_class='ovr',</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">n_jobs=1,</td>
<td class="org-left">number of CPU cores used</td>
</tr>

<tr>
<td class="org-left">penalty='l2',</td>
<td class="org-left">regularization</td>
</tr>

<tr>
<td class="org-left">random_state=None,</td>
<td class="org-left">used to shuffling dataset before predict</td>
</tr>

<tr>
<td class="org-left">solver='liblinear',</td>
<td class="org-left">optimization method, eg: SGD: stochastic  gendient descent, etc.</td>
</tr>

<tr>
<td class="org-left">tol=0.0001,</td>
<td class="org-left">tolerance to stop</td>
</tr>

<tr>
<td class="org-left">verbose=0,</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">warm_start=False</td>
<td class="org-left">can model chained together or not</td>
</tr>
</tbody>
</table>


<p>
To built the model from our data, that is to learn how to classify new points,
we call the fit function with the training data, and the corresponding training
labels (the desired output for the training data point):
</p>

<div class="org-src-container">
<pre class="src src-ipython">classifier.fit(X_train, y_train)
</pre>
</div>

<pre class="example">
LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
verbose=0, warm_start=False)
</pre>

<p>
(Some estimator methods such as fit return self by default. Thus, after
executing the code snippet above, you will see the default parameters of this
particular instance of LogisticRegression. Another way of retrieving the
estimator's ininitialization parameters is to execute <code>classifier.get_params()</code>,
which returns a parameter dictionary.)
</p>
</div>
</div>

<div id="orgc298bc3" class="outline-4">
<h4 id="orgc298bc3"><span class="section-number-4">1.2.3</span> using model to predict test_data by <code>estimator.predict(test_data)</code></h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
<code>estimator.predict(test_data)</code> will return the <code>predict_test_label</code> as ndarray.
</p>

<p>
We can then apply the model to unseen data and use the model to predict the
estimated outcome using the predict method:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">prediction</span> = classifier.predict(X_test)
prediction
</pre>
</div>

<pre class="example">
array([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,
1, 0])
</pre>
</div>
</div>

<div id="org01a08d8" class="outline-4">
<h4 id="org01a08d8"><span class="section-number-4">1.2.4</span> compare the standard label and predict label by <code>estimator.score(test_data,test_label)</code></h4>
</div>
<div id="org917c80f" class="outline-4">
<h4 id="org917c80f"><span class="section-number-4">1.2.5</span> method-1 : by <code>np.mean</code></h4>
<div class="outline-text-4" id="text-1-2-5">
<p>
We can compare these against the true labels:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">print</span>(prediction)
<span class="org-keyword">print</span>(y_test)

</pre>
</div>

<p>
We can evaluate our classifier quantitatively by measuring what fraction of
predictions is correct. This is called accuracy:
</p>

<div class="org-src-container">
<pre class="src src-ipython">np.mean(prediction == y_test) <span class="org-comment-delimiter"># </span><span class="org-comment">&lt;- classical usage of mean, keep in mind</span>
</pre>
</div>

<pre class="example">
0.83999999999999997

</pre>
</div>
</div>

<div id="org8d84852" class="outline-4">
<h4 id="org8d84852"><span class="section-number-4">1.2.6</span> method-2 : by <code>estimator.score(test_data,test_label)</code></h4>
<div class="outline-text-4" id="text-1-2-6">
<p>
There is also a convenience function , score, that all scikit-learn classifiers
have to compute this directly from the test data:
</p>

<div class="org-src-container">
<pre class="src src-ipython">classifier.score(X_test, y_test)
</pre>
</div>

<pre class="example">
0.83999999999999997

</pre>

<p>
It is often helpful to compare the generalization performance (on the test set)
to the performance on the training set:
</p>

<div class="org-src-container">
<pre class="src src-ipython">classifier.score(X_train, y_train)

</pre>
</div>

<pre class="example">
0.94666666666666666

</pre>
</div>
</div>

<div id="orgf3f073d" class="outline-4">
<h4 id="orgf3f073d"><span class="section-number-4">1.2.7</span> draw this estimator as line by <code>plt.figures.plot(estimator,ALL_datasets)</code></h4>
<div class="outline-text-4" id="text-1-2-7">
<p>
LogisticRegression is a so-called linear model, that means it will create a
decision that is linear in the input space. In 2d, this simply means it finds a
line to separate the blue from the red:
</p>

<p>
<code>plot_2d_separator</code> is a method defined in <code>./figures/plot_2d_separator.py</code>
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> figures <span class="org-keyword">import</span> plot_2d_separator

plt.scatter(X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'blue'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'0'</span>)
plt.scatter(X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'red'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'1'</span>, marker=<span class="org-string">'s'</span>)

plt.xlabel(<span class="org-string">"first feature"</span>)
plt.ylabel(<span class="org-string">"second feature"</span>)
plot_2d_separator(classifier, X)
plt.legend(loc=<span class="org-string">'upper right'</span>);

</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/25041dCz.png" alt="25041dCz.png" />
</p>
</div>
</div>
</div>

<div id="org5942d49" class="outline-4">
<h4 id="org5942d49"><span class="section-number-4">1.2.8</span> get the parameters of this estimator by <code>estimator.&lt;attr&gt;_</code></h4>
<div class="outline-text-4" id="text-1-2-8">
<p>
Estimated parameters: All the estimated model parameters are <b>attributes</b> of the
estimator object <b>ending by an underscore</b>. Here, these are the coefficients and
the offset of the line:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">print</span>(classifier.coef_)
<span class="org-keyword">print</span>(classifier.intercept_)

</pre>
</div>
</div>
</div>
</div>

<div id="orgdde1d26" class="outline-3">
<h3 id="orgdde1d26"><span class="section-number-3">1.3</span> Another classifier: K Nearest Neighbors</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Another popular and easy to understand classifier is K nearest neighbors (kNN).
It has one of the simplest learning strategies:
</p>

<blockquote>
<p>
given a new, unknown observation, look up in your reference database which ones
have the closest features and assign the predominant class.
</p>
</blockquote>

<p>
The interface is exactly the same as for LogisticRegression above.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.neighbors <span class="org-keyword">import</span> KNeighborsClassifier

</pre>
</div>

<p>
This time we set a parameter of the KNeighborsClassifier to tell it we only want
to look at one nearest neighbor:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">knn</span> = KNeighborsClassifier(n_neighbors=<span class="org-highlight-numbers-number">1</span>)

</pre>
</div>

<p>
We fit the model with out training data
</p>

<div class="org-src-container">
<pre class="src src-ipython">knn.fit(X_train, y_train)
plt.scatter(X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'blue'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'0'</span>)
plt.scatter(X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'red'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'1'</span>, marker=<span class="org-string">'s'</span>)

plt.xlabel(<span class="org-string">"first feature"</span>)
plt.ylabel(<span class="org-string">"second feature"</span>)
plot_2d_separator(knn, X)
plt.legend(loc=<span class="org-string">'upper right'</span>);
knn.score(X_test, y_test)

</pre>
</div>

<pre class="example">
1.0

</pre>

<div class="figure">
<p><img src="./obipy-resources/250412qU.png" alt="250412qU.png" />
</p>
</div>
</div>
</div>
</div>

<div id="orgd41f7a6" class="outline-2">
<h2 id="orgd41f7a6"><span class="section-number-2">2</span> Exercise</h2>
<div class="outline-text-2" id="text-2">
<blockquote>
<p>
EXERCISE: Apply the KNeighborsClassifier to the ``iris`` dataset. Play with
different values of the ``n_neighbors`` and observe how training and test score
change.
</p>
</blockquote>


<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> sklearn.datasets
</pre>
</div>
</div>
</div>
<div id="orgf58ec9a" class="outline-2">
<h2 id="orgf58ec9a"><span class="section-number-2">3</span> Misc tools</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="orgf06fc43" class="outline-3">
<h3 id="orgf06fc43"><span class="section-number-3">3.1</span> Scikit-learn</h3>
<div class="outline-text-3" id="text-3-1">
</div>
<div id="orgf284813" class="outline-4">
<h4 id="orgf284813"><span class="section-number-4">3.1.1</span> sklearn.datasets.make_blobs</h4>
<div class="outline-text-4" id="text-3-1-1">
<p>
Generate isotropic Gaussian blobs for clustering.
</p>

<p>
n_samples : int, optional (default=100)
The total number of points equally divided among clusters.
</p>

<p>
n_features : int, optional (default=2)
The number of features for each sample.
</p>

<p>
centers : int or array of shape [n_centers, n_features], optional (default=3)
The number of centers to generate, or the fixed center locations.
</p>

<p>
cluster_std : float or sequence of floats, optional (default=1.0)
The standard deviation of the clusters.
</p>

<p>
center_box : pair of floats (min, max), optional (default=(-10.0, 10.0))
The bounding box for each cluster center when centers are generated at random.
</p>

<p>
shuffle : boolean, optional (default=True)
Shuffle the samples.
</p>

<p>
random_state : int, RandomState instance or None, optional (default=None)
</p>

<div class="org-src-container">
<pre class="src src-ipython">sklearn.datasets.make_blobs(n_samples=<span class="org-highlight-numbers-number">100</span>,
                            n_features=<span class="org-highlight-numbers-number">2</span>,
                            centers=<span class="org-highlight-numbers-number">3</span>,
                            cluster_std=<span class="org-highlight-numbers-number">1</span>.<span class="org-highlight-numbers-number">0</span>,
                            center_box=(-<span class="org-highlight-numbers-number">10</span>.<span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">10</span>.<span class="org-highlight-numbers-number">0</span>),
                            shuffle=<span class="org-constant">True</span>,
                            random_state=<span class="org-constant">None</span>)
</pre>
</div>
</div>
</div>
<div id="orgb541b2c" class="outline-4">
<h4 id="orgb541b2c"><span class="section-number-4">3.1.2</span> ML models by now</h4>
<div class="outline-text-4" id="text-3-1-2">
<blockquote>
<ol class="org-ol">
<li>from sklearn.neighbors import KNeighborsClassifier</li>
<li>from sklearn.datasets import make_blobs</li>
<li>from sklearn.model_selection import train_test_split</li>
<li>from sklearn.linear_model import LogisticRegression</li>
</ol>
</blockquote>
</div>
</div>
</div>

<div id="org6c67c5d" class="outline-3">
<h3 id="org6c67c5d"><span class="section-number-3">3.2</span> Techs</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="org2b6f6d8" class="outline-4">
<h4 id="org2b6f6d8"><span class="section-number-4">3.2.1</span> do filtering by making a mask by other array</h4>
<div class="outline-text-4" id="text-3-2-1">
<blockquote>
<p>

</p>

<p>
. |        |      X_col0 |     X_col1 |                                  |        |     X_col1 |
. |--------<del>-------------</del>-------&#x2013;&#x2014;|                                  |---&#x2013;&#x2014;+-------&#x2013;&#x2014;|
. | X_row0 |  4.21850347 | 2.23419161 |
. | X_row1 |  0.90779887 | 0.45984362 |  X[ <b>mask</b>, 1]
. | X_row2 | -0.27652528 | 5.08127768 |  keep the <code>true row</code> and <code>col 1</code> | X_row2 | 5.08127768 |
. | X_row3 |  0.08848433 | 2.32299086 |            ^                     | X_row3 | 2.32299086 |
. | X_row4 |  3.24329731 | 1.21460627 |            |
.                                                  |
.                                         <del>--------</del>
.                                         |
.                                       <b>mask</b> = boolean of array
. | y |                                 | y |
. |&#x2014;|                                 |&#x2014;|
. | 1 |                                 | F |
. | 1 |     y == 0                      | F |
. | 0 | -------------------------&#x2013;&#x2014;&gt; | T |
. | 0 |                                 | T |
. | 1 |                                 | F |
</p>
</blockquote>

<div class="org-src-container">
<pre class="src src-ipython">
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_blobs
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-variable-name">X</span>, <span class="org-variable-name">y</span> = make_blobs(centers=<span class="org-highlight-numbers-number">2</span>, random_state=<span class="org-highlight-numbers-number">0</span>)

<span class="org-keyword">print</span>(<span class="org-string">'X ~ n_samples x n_features:'</span>, X.shape)
<span class="org-keyword">print</span>(<span class="org-string">'y ~ n_samples:'</span>, y.shape)

<span class="org-keyword">print</span>(<span class="org-string">'\nFirst 5 samples:\n'</span>, X[:<span class="org-highlight-numbers-number">5</span>, :])
<span class="org-keyword">print</span>(<span class="org-string">'\nFirst 5 labels:'</span>, y[:<span class="org-highlight-numbers-number">5</span>])

plt.scatter(X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'blue'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'0'</span>)
plt.scatter(X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'red'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'1'</span>, marker=<span class="org-string">'s'</span>)

plt.xlabel(<span class="org-string">'first feature'</span>)
plt.ylabel(<span class="org-string">'second feature'</span>)
plt.legend(loc=<span class="org-string">'upper right'</span>)
</pre>
</div>
</div>
</div>
<div id="org2db74e8" class="outline-4">
<h4 id="org2db74e8"><span class="section-number-4">3.2.2</span> common steps to generate data points and plot</h4>
</div>
<div id="org5e91fa7" class="outline-4">
<h4 id="org5e91fa7"><span class="section-number-4">3.2.3</span> method-1: by dataframe and groupby</h4>
<div class="outline-text-4" id="text-3-2-3">
<blockquote>
<p>

</p>

<p>
. sklearn.datasets              np                                  pd                   pd
. <code>make_blobs</code>      -----&#x2013;&#x2014;&gt; ndarray ----&#x2013;&#x2014;&gt; dict -----&#x2013;&#x2014;&gt; dataframe ----&#x2013;&#x2014;&gt; groupby
.                                                                and color dict
</p>

<p>
. for i,j in groupOfDf: plot (df,color=color_dict[i])
</p>
</blockquote>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.datasets.samples_generator <span class="org-keyword">import</span> make_blobs
<span class="org-keyword">from</span> matplotlib <span class="org-keyword">import</span> pyplot
<span class="org-keyword">from</span> pandas <span class="org-keyword">import</span> DataFrame
<span class="org-comment-delimiter"># </span><span class="org-comment">generate 2d classification dataset</span>
<span class="org-variable-name">X</span>, <span class="org-variable-name">y</span> = make_blobs(n_samples=<span class="org-highlight-numbers-number">100</span>, centers=<span class="org-highlight-numbers-number">3</span>, n_features=<span class="org-highlight-numbers-number">2</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">scatter plot, dots colored by class value</span>
<span class="org-variable-name">df</span> = DataFrame(<span class="org-builtin">dict</span>(x=X[:,<span class="org-highlight-numbers-number">0</span>], y=X[:,<span class="org-highlight-numbers-number">1</span>], label=y))
<span class="org-variable-name">colors</span> = {<span class="org-highlight-numbers-number">0</span>:<span class="org-string">'red'</span>, <span class="org-highlight-numbers-number">1</span>:<span class="org-string">'blue'</span>, <span class="org-highlight-numbers-number">2</span>:<span class="org-string">'green'</span>}
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = pyplot.subplots()
<span class="org-variable-name">grouped</span> = df.groupby(<span class="org-string">'label'</span>)
<span class="org-keyword">for</span> key, group <span class="org-keyword">in</span> grouped:
    group.plot(ax=ax, kind=<span class="org-string">'scatter'</span>, x=<span class="org-string">'x'</span>, y=<span class="org-string">'y'</span>, label=key, color=colors[key])
pyplot.show()
</pre>
</div>
</div>
</div>

<div id="org5b640e3" class="outline-4">
<h4 id="org5b640e3"><span class="section-number-4">3.2.4</span> method-2: by ndarray and mask</h4>
<div class="outline-text-4" id="text-3-2-4">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-comment-delimiter"># </span><span class="org-comment">give different color to data with different label</span>
plt.scatter(X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">0</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'blue'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'0'</span>)
plt.scatter(X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">0</span>], X[y == <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>],
            c=<span class="org-string">'red'</span>, s=<span class="org-highlight-numbers-number">40</span>, label=<span class="org-string">'1'</span>, marker=<span class="org-string">'s'</span>)

plt.xlabel(<span class="org-string">'first feature'</span>)
plt.ylabel(<span class="org-string">'second feature'</span>)
plt.legend(loc=<span class="org-string">'upper right'</span>)
</pre>
</div>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
