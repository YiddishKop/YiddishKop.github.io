<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 日 21:50 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>scikit-笔记16:深入理解线性模型</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="yiddi" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
 <link rel='stylesheet' href='css/site.css' type='text/css'/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href="index.html"> UP </a>
 |
 <a accesskey="H" href="/index.html"> HOME </a>
</div><div id="content">
<h1 class="title">scikit-笔记16:深入理解线性模型</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org2e0f590">1. Linear Regression</a>
<ul>
<li><a href="#org3ed134d">1.1. what is a regularization</a>
<ul>
<li><a href="#orgeef73db">1.1.1. difference due to regularization</a></li>
<li><a href="#org0f989a6">1.1.2. linear regression is bad due to no regularization</a></li>
</ul>
</li>
<li><a href="#org9ab13b2">1.2. Linear Regression (without Regularization)</a></li>
<li><a href="#org3c22591">1.3. Ridge Regression (L2 Regularization)</a></li>
<li><a href="#orga3962ca">1.4. Lasso (L1 Regularization)</a></li>
</ul>
</li>
<li><a href="#orgbadbbc7">2. Linear Classification</a>
<ul>
<li><a href="#org9cdf206">2.1. Bi-class linear classification</a>
<ul>
<li><a href="#org999b1fb">2.1.1. regression model vs. classification model</a></li>
<li><a href="#orgbecc4e6">2.1.2. The influence of C in LinearSVC</a></li>
<li><a href="#org5e42d90">2.1.3. l1 regularization vs. l2 regularization</a></li>
</ul>
</li>
<li><a href="#org2014d4e">2.2. Multi-class linear classification</a>
<ul>
<li><a href="#org8844394">2.2.1. shape of coef_ and intercept_</a></li>
<li><a href="#org56db450">2.2.2. how to plot these 3 lines</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org8179541">3. EXERCISE</a></li>
<li><a href="#org62e4d96">4. Misc tools</a>
<ul>
<li><a href="#orgd788aa1">4.1. scikit-learn</a>
<ul>
<li><a href="#orga220009">4.1.1. ML models by now</a></li>
<li><a href="#org3f605d8">4.1.2. make_regression</a></li>
<li><a href="#org62f60b0">4.1.3. learning_curve</a>
<ul>
<li><a href="#org0e47173">4.1.3.1. return</a></li>
</ul>
</li>
<li><a href="#orgf0ccbef">4.1.4. Ridge</a></li>
<li><a href="#org5b3e8f3">4.1.5. Lasso</a></li>
</ul>
</li>
<li><a href="#orgb868929">4.2. Linear algebra</a>
<ul>
<li><a href="#org263db41">4.2.1. SVD</a></li>
</ul>
</li>
<li><a href="#org732d095">4.3. scikit user guid</a>
<ul>
<li><a href="#org6540335">4.3.1. 5.4. Sample generators</a>
<ul>
<li><a href="#org17becb8">4.3.1.1. 5.4.1. Generators for classification and clustering</a></li>
<li><a href="#org7d6ec7b">4.3.1.2. 5.4.1.1. Single label</a></li>
<li><a href="#org7ad77dd">4.3.1.3. 5.4.1.2. Multilabel</a></li>
<li><a href="#org9346bad">4.3.1.4. 5.4.1.3. Biclustering</a></li>
<li><a href="#org4e59b51">4.3.1.5. 5.4.2. Generators for regression</a></li>
<li><a href="#org9f7c4f4">4.3.1.6. 5.4.3. Generators for manifold learning</a></li>
<li><a href="#org340aa9b">4.3.1.7. 5.4.4. Generators for decomposition</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3706135">4.4. Numpy</a>
<ul>
<li><a href="#org82c30d7">4.4.1. np.argsort</a></li>
</ul>
</li>
<li><a href="#org36d3428">4.5. Matplotlib</a>
<ul>
<li><a href="#org2559784">4.5.1. how to give a gradually changed color</a></li>
<li><a href="#org6d3330e">4.5.2. what are tickets</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
</pre>
</div>

<div id="org2e0f590" class="outline-2">
<h2 id="org2e0f590"><span class="section-number-2">1</span> Linear Regression</h2>
<div class="outline-text-2" id="text-1">
<p>
Linear models are useful when little data is available or for very large feature
spaces as in text classification. In addition, they form a good case study for
regularization.
</p>
</div>

<div id="org3ed134d" class="outline-3">
<h3 id="org3ed134d"><span class="section-number-3">1.1</span> what is a regularization</h3>
<div class="outline-text-3" id="text-1-1">
<p>
All linear models for regression learn a coefficient parameter <code>coef_</code> and an
offset <code>intercept_</code> to make predictions using a linear combination of features:
</p>

<p>
regression fn model:
</p>
<blockquote>
<p>
y_pred = x_test[0] * coef_[0] + &#x2026; + x_test[n_features-1] * coef_[n_features-1] + intercept_
</p>
</blockquote>
</div>

<div id="orgeef73db" class="outline-4">
<h4 id="orgeef73db"><span class="section-number-4">1.1.1</span> difference due to regularization</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
The <b>difference between the linear models for regression</b> is what kind of
<b>restrictions or penalties</b> are put on <code>coef_</code> as <b>regularization</b> , in addition
to fitting the training data well.
</p>
</div>
</div>

<div id="org0f989a6" class="outline-4">
<h4 id="org0f989a6"><span class="section-number-4">1.1.2</span> linear regression is bad due to no regularization</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
The most standard linear model is the 'ordinary least squares regression, often
simply called 'linear regression'. It doesn't put any additional restrictions on
<code>coef_</code>, so when the number of features is large, it becomes ill-posed and the
model overfits.
</p>

<p>
Let us generate a simple simulation, to see the behavior of these models.
</p>

<p>
<code>make_regression</code> can create a data set.
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_regression
<span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> train_test_split
<span class="org-variable-name">X</span>, <span class="org-variable-name">y</span>, <span class="org-variable-name">true_coefficient</span> = make_regression(n_samples=<span class="org-highlight-numbers-number">200</span>, n_features=<span class="org-highlight-numbers-number">30</span>, n_informative=<span class="org-highlight-numbers-number">10</span>, noise=<span class="org-highlight-numbers-number">100</span>, coef=<span class="org-constant">True</span>, random_state=<span class="org-highlight-numbers-number">5</span>)
<span class="org-variable-name">X_train</span>, <span class="org-variable-name">X_test</span>, <span class="org-variable-name">y_train</span>, <span class="org-variable-name">y_test</span> = train_test_split(X, y, random_state=<span class="org-highlight-numbers-number">5</span>, train_size=<span class="org-highlight-numbers-number">60</span>, test_size=<span class="org-highlight-numbers-number">140</span>)
<span class="org-keyword">print</span>(X_train.shape) <span class="org-comment-delimiter"># </span><span class="org-comment">(60,30)</span>
<span class="org-keyword">print</span>(y_train.shape) <span class="org-comment-delimiter"># </span><span class="org-comment">(60,)</span>
</pre>
</div>
</div>
</div>
</div>

<div id="org9ab13b2" class="outline-3">
<h3 id="org9ab13b2"><span class="section-number-3">1.2</span> Linear Regression (without Regularization)</h3>
<div class="outline-text-3" id="text-1-2">
<p>
\(\text{min}_{w, b} \sum_i || w^\mathsf{T}x_i + b  - y_i||^2\)
</p>


<p>
about <b>score</b>, which is an evaluation of the model:
</p>
<ul class="org-ul">
<li>every model has a built-in method, almost with <code>'model_name'.score(dataset,
  labelset)</code></li>
<li>many other evaluation method inside <code>sklearn.metrics</code>, almost with
<code>'scorer_name'.(predict_label, truelabel)</code>, like <code>r2_score</code>,
<code>adjusted_rand_score</code>, etc</li>
</ul>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.linear_model <span class="org-keyword">import</span> LinearRegression
<span class="org-variable-name">linear_regression</span> = LinearRegression().fit(X_train, y_train)
<span class="org-keyword">print</span>(<span class="org-string">"R^2 on training set: %f"</span> % linear_regression.score(X_train, y_train))
<span class="org-keyword">print</span>(<span class="org-string">"R^2 on test set: %f"</span> % linear_regression.score(X_test, y_test))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.metrics <span class="org-keyword">import</span> r2_score
<span class="org-keyword">print</span>(r2_score(np.dot(X, true_coefficient), y))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.figure(figsize=(<span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">5</span>))
<span class="org-keyword">print</span> (true_coefficient.shape)
<span class="org-variable-name">coefficient_sorting</span> = np.argsort(true_coefficient)[::-<span class="org-highlight-numbers-number">1</span>] <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- inverse the order of result of argsort</span>
plt.plot(true_coefficient[coefficient_sorting], <span class="org-string">"o"</span>, label=<span class="org-string">"true"</span>)
plt.plot(linear_regression.coef_[coefficient_sorting], <span class="org-string">"o"</span>, label=<span class="org-string">"linear regression"</span>)
plt.legend()
</pre>
</div>

<pre class="example">
&lt;matplotlib.legend.Legend at 0x7ff5069e0438&gt;

</pre>

<div class="figure">
<p><img src="./obipy-resources/3199P4I.png" alt="3199P4I.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.model_selection <span class="org-keyword">import</span> learning_curve
<span class="org-keyword">def</span> <span class="org-function-name">plot_learning_curve</span>(est, X, y):
    <span class="org-variable-name">training_set_size</span>, <span class="org-variable-name">train_scores</span>, <span class="org-variable-name">test_scores</span> = learning_curve(est, X, y, train_sizes=np.linspace(.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">20</span>))
    <span class="org-variable-name">estimator_name</span> = est.__class__.<span class="org-builtin">__name__</span>
    <span class="org-variable-name">line</span> = plt.plot(training_set_size, train_scores.mean(axis=<span class="org-highlight-numbers-number">1</span>), <span class="org-string">'--'</span>, label=<span class="org-string">"training scores "</span> + estimator_name)
    plt.plot(training_set_size, test_scores.mean(axis=<span class="org-highlight-numbers-number">1</span>), <span class="org-string">'-'</span>, label=<span class="org-string">"test scores "</span> + estimator_name, c=line[<span class="org-highlight-numbers-number">0</span>].get_color())
    plt.xlabel(<span class="org-string">'Training set size'</span>)
    plt.legend(loc=<span class="org-string">'best'</span>)
    plt.ylim(-<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>.<span class="org-highlight-numbers-number">1</span>)
plt.figure()
plot_learning_curve(LinearRegression(), X, y)
</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/3199pTJ.png" alt="3199pTJ.png" />
</p>
</div>
</div>
</div>

<div id="org3c22591" class="outline-3">
<h3 id="org3c22591"><span class="section-number-3">1.3</span> Ridge Regression (L2 Regularization)</h3>
<div class="outline-text-3" id="text-1-3">
<p>
<b><b>The Ridge estimator</b></b> is a simple regularization (called <b>l2 penalty</b>) of the
ordinary <b>LinearRegression</b>. In particular, it has the benefit of being not
computationally more expensive than the ordinary least square estimate. ​
</p>

<p>
\[ \text{min}_{w,b}  \sum_i || w^\mathsf{T}x_i + b  - y_i||^2  + \alpha ||w||_2^2\]
</p>

<p>
The amount of regularization is set via the <b>alpha parameter of the Ridge</b>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.linear_model <span class="org-keyword">import</span> Ridge
<span class="org-variable-name">ridge_models</span> = {}
<span class="org-variable-name">training_scores</span> = []
<span class="org-variable-name">test_scores</span> = []
<span class="org-keyword">for</span> alpha <span class="org-keyword">in</span> [<span class="org-highlight-numbers-number">100</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">1</span>, .<span class="org-highlight-numbers-number">01</span>]:
    <span class="org-variable-name">ridge</span> = Ridge(alpha=alpha).fit(X_train, y_train)
    training_scores.append(ridge.score(X_train, y_train))
    test_scores.append(ridge.score(X_test, y_test))
    <span class="org-variable-name">ridge_models</span>[alpha] = ridge
plt.figure()
plt.plot(training_scores, label=<span class="org-string">"training scores"</span>)
plt.plot(test_scores, label=<span class="org-string">"test scores"</span>)
plt.xticks(<span class="org-builtin">range</span>(<span class="org-highlight-numbers-number">4</span>), [<span class="org-highlight-numbers-number">100</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">1</span>, .<span class="org-highlight-numbers-number">01</span>])
plt.legend(loc=<span class="org-string">"best"</span>)
</pre>
</div>

<pre class="example">
&lt;matplotlib.legend.Legend at 0x7ff5069675c0&gt;

</pre>

<div class="figure">
<p><img src="./obipy-resources/31992dP.png" alt="31992dP.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.figure(figsize=(<span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">5</span>))
plt.plot(true_coefficient[coefficient_sorting], <span class="org-string">"o"</span>, label=<span class="org-string">"true"</span>, c=<span class="org-string">'b'</span>)
<span class="org-keyword">for</span> i, alpha <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-highlight-numbers-number">100</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">1</span>, .<span class="org-highlight-numbers-number">01</span>]):
    plt.plot(ridge_models[alpha].coef_[coefficient_sorting],
             <span class="org-string">"o"</span>,
             label=<span class="org-string">"alpha = %.2f"</span> % alpha,
             c=plt.cm.summer(i / <span class="org-highlight-numbers-number">3</span>.) <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- how to give a gradually changed color</span>
    )
plt.legend(loc=<span class="org-string">"best"</span>)
</pre>
</div>

<pre class="example">
&lt;matplotlib.legend.Legend at 0x7ff50680fc50&gt;

</pre>

<div class="figure">
<p><img src="./obipy-resources/3199d8h.png" alt="3199d8h.png" />
</p>
</div>

<p>
Tuning alpha is critical for performance.
</p>


<div class="org-src-container">
<pre class="src src-ipython">plt.figure()
plot_learning_curve(LinearRegression(), X, y)
plot_learning_curve(Ridge(alpha=<span class="org-highlight-numbers-number">10</span>), X, y)
</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/3199qGo.png" alt="3199qGo.png" />
</p>
</div>
</div>
</div>

<div id="orga3962ca" class="outline-3">
<h3 id="orga3962ca"><span class="section-number-3">1.4</span> Lasso (L1 Regularization)</h3>
<div class="outline-text-3" id="text-1-4">
<p>
<b><b>The Lasso estimator</b></b> is useful to impose sparsity on the coefficient. In
other words, it is to be prefered if we believe that many of the features are
not relevant. This is done via the so-called l1 penalty. ​
</p>

<p>
\(\text{min}_{w, b} \sum_i \frac{1}{2} || w^\mathsf{T}x_i + b  - y_i||^2  + \alpha ||w||_1\)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.linear_model <span class="org-keyword">import</span> Lasso
<span class="org-variable-name">lasso_models</span> = {}
<span class="org-variable-name">training_scores</span> = []
<span class="org-variable-name">test_scores</span> = []
<span class="org-keyword">for</span> alpha <span class="org-keyword">in</span> [<span class="org-highlight-numbers-number">30</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">1</span>, .<span class="org-highlight-numbers-number">01</span>]:
    <span class="org-variable-name">lasso</span> = Lasso(alpha=alpha).fit(X_train, y_train)
    training_scores.append(lasso.score(X_train, y_train))
    test_scores.append(lasso.score(X_test, y_test))
    <span class="org-variable-name">lasso_models</span>[alpha] = lasso
plt.figure()
plt.plot(training_scores, label=<span class="org-string">"training scores"</span>)
plt.plot(test_scores, label=<span class="org-string">"test scores"</span>)
plt.xticks(<span class="org-builtin">range</span>(<span class="org-highlight-numbers-number">4</span>), [<span class="org-highlight-numbers-number">30</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">1</span>, .<span class="org-highlight-numbers-number">01</span>])
plt.legend(loc=<span class="org-string">"best"</span>)
</pre>
</div>

<pre class="example">
&lt;matplotlib.legend.Legend at 0x7f9c30ff9550&gt;

</pre>

<div class="figure">
<p><img src="./obipy-resources/25041f_2.png" alt="25041f_2.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.figure(figsize=(<span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">5</span>))
plt.plot(true_coefficient[coefficient_sorting], <span class="org-string">"o"</span>, label=<span class="org-string">"true"</span>, c=<span class="org-string">'b'</span>)
<span class="org-keyword">for</span> i, alpha <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-highlight-numbers-number">30</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">1</span>, .<span class="org-highlight-numbers-number">01</span>]):
    plt.plot(lasso_models[alpha].coef_[coefficient_sorting], <span class="org-string">"o"</span>, label=<span class="org-string">"alpha = %.2f"</span> % alpha, c=plt.cm.summer(i / <span class="org-highlight-numbers-number">3</span>.))
plt.legend(loc=<span class="org-string">"best"</span>)
</pre>
</div>

<pre class="example">
&lt;matplotlib.legend.Legend at 0x7f9c30ed32e8&gt;

</pre>

<div class="figure">
<p><img src="./obipy-resources/25041RJG.png" alt="25041RJG.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.figure(figsize=(<span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">5</span>))
plot_learning_curve(LinearRegression(), X, y)
plot_learning_curve(Ridge(alpha=<span class="org-highlight-numbers-number">10</span>), X, y)
plot_learning_curve(Lasso(alpha=<span class="org-highlight-numbers-number">10</span>), X, y)
</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/25041eTM.png" alt="25041eTM.png" />
</p>
</div>

<p>
Instead of picking <b>Ridge</b> or <b>Lasso</b>, you can also use <b>ElasticNet</b>, which uses
both forms of <b>regularization</b> and provides a parameter to <b>assign a weighting
between them</b>. <b>ElasticNet</b> typically performs the best amongst these models.
</p>
</div>
</div>
</div>

<div id="orgbadbbc7" class="outline-2">
<h2 id="orgbadbbc7"><span class="section-number-2">2</span> Linear Classification</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="org9cdf206" class="outline-3">
<h3 id="org9cdf206"><span class="section-number-3">2.1</span> Bi-class linear classification</h3>
<div class="outline-text-3" id="text-2-1">
</div>
<div id="org999b1fb" class="outline-4">
<h4 id="org999b1fb"><span class="section-number-4">2.1.1</span> regression model vs. classification model</h4>
<div class="outline-text-4" id="text-2-1-1">
<p>
Regression fn model:
</p>
<hr />
<blockquote>
<p>
y_pred = x_test[0] * coef_[0] + &#x2026; + x_test[n_features-1] * coef_[n_features-1] + intercept_
</p>
</blockquote>

<p>
All linear models for classification learn a <b>coefficient parameter</b> <code>coef_</code> and
an <b>offset</b> <code>intercept_</code> to make predictions using a linear combination of
features:
</p>


<p>
Classification fn model:
</p>
<hr />
<blockquote>
<p>
y_pred = x_test[0] * coef_[0] + &#x2026; + x_test[n_features-1] * coef_[n_features-1] + intercept_ &gt; 0
</p>
</blockquote>

<p>
As you can see, this is very similar to regression, only that a <b>threshold</b> at
zero is applied.
</p>

<p>
Again, the difference between the linear models for classification what kind of
regularization is <b>put on coef_ and intercept_</b>, but there are also minor
differences in how the fit to the training set is measured (the so-called loss
function).
</p>

<p>
The two most common models for <b>linear classification</b> are the linear SVM as
implemented in <code>LinearSVC</code> and <code>LogisticRegression</code>.
</p>

<p>
A good intuition for regularization of linear classifiers is that with high
regularization, it is enough if most of the points are classified correctly. But
with less regularization, more importance is given to each individual data
point. This is illustrated using an linear SVM with different values of C below.
</p>
</div>
</div>

<div id="orgbecc4e6" class="outline-4">
<h4 id="orgbecc4e6"><span class="section-number-4">2.1.2</span> The influence of C in LinearSVC</h4>
<div class="outline-text-4" id="text-2-1-2">
<p>
In LinearSVC, the C parameter controls the regularization within the model.
</p>

<p>
Lower C entails more regularization and simpler models, whereas higher C entails
less regularization and more influence from individual data points.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> figures <span class="org-keyword">import</span> plot_linear_svc_regularization
plot_linear_svc_regularization()
</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/3199Eb0.png" alt="3199Eb0.png" />
</p>
</div>
</div>
</div>

<div id="org5e42d90" class="outline-4">
<h4 id="org5e42d90"><span class="section-number-4">2.1.3</span> l1 regularization vs. l2 regularization</h4>
<div class="outline-text-4" id="text-2-1-3">
<p>
Similar to the Ridge/Lasso separation, you can set the penalty parameter to:
</p>
<ul class="org-ul">
<li>'l1' to enforce <b>sparsity of the coefficients</b> (similar to Lasso)</li>
<li>'l2' to encourage <b>smaller coefficients</b> (similar to Ridge).</li>
</ul>

<p>
We can see,
</p>
<blockquote>
<p>
. xxx regularization apply on yyy , yyy will change to zzz
.  |                           |                        |
.  v                           v                        v
. l1                          coef_                  sparse coef_
. l2                          coef_                  small coef_
</p>
</blockquote>
</div>
</div>
</div>

<div id="org2014d4e" class="outline-3">
<h3 id="org2014d4e"><span class="section-number-3">2.2</span> Multi-class linear classification</h3>
<div class="outline-text-3" id="text-2-2">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_blobs
plt.figure()
<span class="org-variable-name">X</span>, <span class="org-variable-name">y</span> = make_blobs(random_state=<span class="org-highlight-numbers-number">42</span>)
<span class="org-keyword">print</span> (X.shape, y.shape) <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- two features</span>
<span class="org-keyword">print</span> (X, y) <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- two features</span>
plt.scatter(X[:, <span class="org-highlight-numbers-number">0</span>], X[:, <span class="org-highlight-numbers-number">1</span>], c=plt.cm.spectral(y / <span class="org-highlight-numbers-number">2</span>.));
</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/3199Q5P.png" alt="3199Q5P.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.svm <span class="org-keyword">import</span> LinearSVC
<span class="org-variable-name">linear_svm</span> = LinearSVC().fit(X, y)
<span class="org-keyword">print</span>(linear_svm.coef_.shape)
<span class="org-keyword">print</span>(linear_svm.intercept_.shape)
</pre>
</div>
</div>

<div id="org8844394" class="outline-4">
<h4 id="org8844394"><span class="section-number-4">2.2.1</span> shape of coef_ and intercept_</h4>
<div class="outline-text-4" id="text-2-2-1">
<blockquote>
<p>
you can see if you want to :
separte the 2 class, you should need 1 split line;
separte the 3 class, you should need 3 split line;
separte the 4 class, you should need 6 split line;
</p>
</blockquote>

<p>
So the shape of coef_ should be 3 in this case.
</p>
</div>
</div>

<div id="org56db450" class="outline-4">
<h4 id="org56db450"><span class="section-number-4">2.2.2</span> how to plot these 3 lines</h4>
<div class="outline-text-4" id="text-2-2-2">
<p>
You should note, why we use the code shown below, to plot split line.
As we said that fn model for classification is :
</p>

<blockquote>
<p>
y_pred = x_test[0] * coef_[0] + &#x2026; + x_test[n_features-1] * coef_[n_features-1] + intercept_ &gt; 0
</p>
</blockquote>

<p>
We get 2d coef_ for each line(totally we have 3 lines), for each line we should
make make one dimension of the 2d point(x1,x2), as x-axis and another as y-axis,
to plot the line for example:
</p>
<ul class="org-ul">
<li>x1 &#x2013;&gt; x-axis</li>
<li>x2 &#x2013;&gt; y-axis</li>
</ul>

<p>
In another word, this means that we should make point
</p>
<ul class="org-ul">
<li>x1 as 'x' the independent variable;</li>
<li>x2 as 'y' the dependent variable;</li>
</ul>

<p>
x_test[0] * coef_[0] + x_test[1] * coef_[1] + intercept_ = 0
</p>

<p>
x_test[1] = - (x_test[0] * coef_[0] + intercept_) / coef_[1]
</p>

<p>
This is the origin of code below:
</p>
<blockquote>
<p>
plt.plot(line, -(line * coef[0] + intercept) / coef[1])
</p>
</blockquote>


<div class="org-src-container">
<pre class="src src-ipython">plt.scatter(X[:, <span class="org-highlight-numbers-number">0</span>], X[:, <span class="org-highlight-numbers-number">1</span>], c=plt.cm.spectral(y / <span class="org-highlight-numbers-number">2</span>.))
<span class="org-variable-name">line</span> = np.linspace(-<span class="org-highlight-numbers-number">15</span>, <span class="org-highlight-numbers-number">15</span>)
<span class="org-keyword">for</span> coef, intercept <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(linear_svm.coef_, linear_svm.intercept_):
    plt.plot(line, -(line * coef[<span class="org-highlight-numbers-number">0</span>] + intercept) / coef[<span class="org-highlight-numbers-number">1</span>])
plt.ylim(-<span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">15</span>)
plt.xlim(-<span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">8</span>);
</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/25041Fye.png" alt="25041Fye.png" />
</p>
</div>

<p>
Points are classified in a <b>one-vs-rest</b> fashion (aka <b>one-vs-all</b>), where we
assign a test point to the class whose model has the <b>highest confidence</b> (in
the SVM case, <b>highest distance</b> to the separating hyperplane) for the test
point.
</p>
</div>
</div>
</div>
</div>

<div id="org8179541" class="outline-2">
<h2 id="org8179541"><span class="section-number-2">3</span> EXERCISE</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>Use LogisticRegression to classify the digits data set, and grid-search the C
parameter.</li>
<li><p>
How do you think the learning curves above change when you increase or
decrease alpha? Try changing the alpha parameter in ridge and lasso, and see
if your intuition was correct.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> load_digits
<span class="org-keyword">from</span> sklearn.linear_model <span class="org-keyword">import</span> LogisticRegression
<span class="org-variable-name">digits</span> = load_digits()
<span class="org-variable-name">X_digits</span>, <span class="org-variable-name">y_digits</span> = digits.data, digits.target

</pre>
</div></li>
</ul>

<p>
​
</p>
</div>
</div>

<div id="org62e4d96" class="outline-2">
<h2 id="org62e4d96"><span class="section-number-2">4</span> Misc tools</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="orgd788aa1" class="outline-3">
<h3 id="orgd788aa1"><span class="section-number-3">4.1</span> scikit-learn</h3>
<div class="outline-text-3" id="text-4-1">
</div>
<div id="orga220009" class="outline-4">
<h4 id="orga220009"><span class="section-number-4">4.1.1</span> ML models by now</h4>
<div class="outline-text-4" id="text-4-1-1">
<blockquote>
<ol class="org-ol">
<li>from sklearn.datasets import make_blobs</li>
<li>from sklearn.datasets import make_regression *</li>
<li>from sklearn.datasets import load_iris</li>
<li>from sklearn.datasets import load_digits</li>
<li>from sklearn.model_selection import train_test_split</li>
<li>from sklearn.model_selection import cross_val_score</li>
<li>from sklearn.model_selection import KFold</li>
<li>from sklearn.model_selection import StratifiedKFold</li>
<li>from sklearn.model_selection import ShuffleSplit</li>
<li>from sklearn.model_selection import GridSearchCV</li>
<li>from sklearn.model_selection import learning_curve *</li>
<li>from sklearn.linear_model import LogisticRegression</li>
<li>from sklearn.linear_model import LinearRegression</li>
<li>from sklearn.linear_model import Ridge  *</li>
<li>from sklearn.linear_model import Lasso  *</li>
<li>from sklearn.linear_model import ElasticNet  *</li>
<li>from sklearn.neighbors import KNeighborsClassifier</li>
<li>from sklearn.neighbors import KNeighborsRegressor</li>
<li>from sklearn.preprocessing import StandardScaler</li>
<li>from sklearn.decomposition import PCA</li>
<li>from sklearn.metrics import confusion_matrix, accuracy_score</li>
<li>from sklearn.metrics import adjusted_rand_score</li>
<li>from sklearn.metrics.scorer import SCORERS</li>
<li>from sklearn.metrics import r2_score *</li>
<li>from sklearn.cluster import KMeans</li>
<li>from sklearn.cluster import KMeans</li>
<li>from sklearn.cluster import MeanShift</li>
<li>from sklearn.cluster import DBSCAN  # &lt;&lt;&lt; this algorithm has related sources in <a href="https://github.com/YiddishKop/org-notes/blob/master/ML/TaiDa_LiHongYi_ML/LiHongYi_ML_lec12_semisuper.org">LIHONGYI's lecture-12</a></li>
<li>from sklearn.cluster import AffinityPropagation</li>
<li>from sklearn.cluster import SpectralClustering</li>
<li>from sklearn.cluster import Ward</li>
<li>from sklearn.metrics import confusion_matrix</li>
<li>from sklearn.metrics import accuracy_score</li>
<li>from sklearn.metrics import adjusted_rand_score</li>
<li>from sklearn.metrics import classification_report</li>
<li>from sklearn.feature_extraction import DictVectorizer</li>
<li>from sklearn.feature_extraction.text import CountVectorizer</li>
<li>from sklearn.feature_extraction.text import TfidfVectorizer</li>
<li>from sklearn.preprocessing import Imputer</li>
<li>from sklearn.dummy import DummyClassifier</li>
<li>from sklearn.pipeline import make_pipeline</li>
<li>from sklearn.svm import LinearSVC</li>
<li>from sklearn.svm import SVC</li>
</ol>
</blockquote>
</div>
</div>
<div id="org3f605d8" class="outline-4">
<h4 id="org3f605d8"><span class="section-number-4">4.1.2</span> make_regression</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
Generate a random regression problem.
</p>

<p>
The input set can either be well conditioned (by default) or have a low rank-fat
tail singular profile. See make_low_rank_matrix for more details.
</p>

<p>
The output is generated by applying a (potentially biased) random linear
regression model with n_informative nonzero regressors to the previously
generated input and some gaussian centered noise with some adjustable scale.
</p>


<div class="org-src-container">
<pre class="src src-ipython">make_regression(n_samples=<span class="org-highlight-numbers-number">100</span>,      <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- number of samples.</span>
                n_features=<span class="org-highlight-numbers-number">100</span>,     <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- number of features.</span>
                n_informative=<span class="org-highlight-numbers-number">10</span>,   <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- number of truely useful features</span>
                n_targets=<span class="org-highlight-numbers-number">1</span>,        <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- the dimension of y output</span>
                bias=<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">0</span>,           <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- bias term in underlying linear model</span>
                effective_rank=<span class="org-constant">None</span>,<span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- The approximate number of singular</span>
                                    <span class="org-comment-delimiter">#</span><span class="org-comment">vectors required to explain most of the</span>
                                    <span class="org-comment-delimiter">#</span><span class="org-comment">input data by linear combinations.</span>
                tail_strength=<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">5</span>,
                noise=<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">0</span>,          <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- The standard deviation of the gaussian</span>
                                    <span class="org-comment-delimiter">#</span><span class="org-comment">noise applied to the output.</span>
                shuffle=<span class="org-constant">True</span>,
                coef=<span class="org-constant">False</span>,         <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- whether return coef of this undelying</span>
                                    <span class="org-comment-delimiter">#</span><span class="org-comment">linear model</span>
                random_state=<span class="org-constant">None</span>)
</pre>
</div>
</div>
</div>

<div id="org62f60b0" class="outline-4">
<h4 id="org62f60b0"><span class="section-number-4">4.1.3</span> learning_curve</h4>
<div class="outline-text-4" id="text-4-1-3">
<p>
Learning curve.
</p>

<p>
Determines cross-validated training and test scores for different training set sizes.
</p>

<p>
A cross-validation generator splits the whole dataset k times in training and
test data. Subsets of the training set with varying sizes will be used to train
the estimator and a score for each training subset size and the test set will be
computed. Afterwards, the scores will be averaged over all k runs for each
training subset size.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">training_set_size</span>, <span class="org-variable-name">train_scores</span>, <span class="org-variable-name">test_scores</span> = learning_curve(
    est,  <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- the ML model used to predict</span>
    X,    <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- dataset passed to this model</span>
    y,    <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- labels passed to this model</span>
    train_sizes=np.linspace(.<span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">1</span>, <span class="org-highlight-numbers-number">20</span>) <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- commonly usage way: linspace(.1,1, num)</span>
)
</pre>
</div>

<p>
0 - ee240ab5-f4df-4c6b-8fa9-dc13d1590680
</p>
</div>

<div id="org0e47173" class="outline-5">
<h5 id="org0e47173"><span class="section-number-5">4.1.3.1</span> return</h5>
<div class="outline-text-5" id="text-4-1-3-1">
<p>
train_sizes_abs : array, shape = (n_unique_ticks,), dtype int
</p>

<p>
Numbers of training examples that has been used to generate the learning curve.
Note that the number of ticks might be less than n_ticks because duplicate
entries will be removed.
</p>

<p>
train_scores : array, shape (n_ticks, n_cv_folds)
</p>

<p>
Scores on training sets.
</p>

<p>
test_scores : array, shape (n_ticks, n_cv_folds)
</p>

<p>
Scores on test set.
</p>
</div>
</div>
</div>


<div id="orgf0ccbef" class="outline-4">
<h4 id="orgf0ccbef"><span class="section-number-4">4.1.4</span> Ridge</h4>
<div class="outline-text-4" id="text-4-1-4">
<p>
Ridge is another linear_model, and has the identical inteface with
linear_model on methods invocation
</p>

<ul class="org-ul">
<li>ridge = Ridge()</li>
<li>ridge.fit(X,y)</li>
<li>ridge.predict(X)</li>
<li>ridge.score(X,y) : <code>model_name.score(X,y)</code></li>
</ul>
</div>
</div>
<div id="org5b3e8f3" class="outline-4">
<h4 id="org5b3e8f3"><span class="section-number-4">4.1.5</span> Lasso</h4>
<div class="outline-text-4" id="text-4-1-5">
<p>
Lasso is another linear_model, and has the identical inteface with
linear_model on methods invocation
</p>

<ul class="org-ul">
<li>lasso = Lasso()</li>
<li>lasso.fit(X,y)</li>
<li>lasso.predict(X)</li>
<li>lasso.score(X,y) : <code>model_name.score(X,y)</code></li>
</ul>
</div>
</div>
</div>
<div id="orgb868929" class="outline-3">
<h3 id="orgb868929"><span class="section-number-3">4.2</span> Linear algebra</h3>
<div class="outline-text-3" id="text-4-2">
</div>
<div id="org263db41" class="outline-4">
<h4 id="org263db41"><span class="section-number-4">4.2.1</span> SVD</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
The singular value decomposition of a matrix A is the factorization of A into
the product of three matrices \(A = UDV^T\) where the columns of U and V are
orthonormal and the matrix D is diagonal with positive real entries.
</p>
</div>
</div>
</div>

<div id="org732d095" class="outline-3">
<h3 id="org732d095"><span class="section-number-3">4.3</span> scikit user guid</h3>
<div class="outline-text-3" id="text-4-3">
</div>
<div id="org6540335" class="outline-4">
<h4 id="org6540335"><span class="section-number-4">4.3.1</span> 5.4. Sample generators</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
<a href="http://scikit-learn.org/stable/datasets/index.html#sample-generators">http://scikit-learn.org/stable/datasets/index.html#sample-generators</a>
</p>

<p>
In addition, scikit-learn includes various random sample generators that can be
used to build artificial datasets of controlled size and complexity.
</p>
</div>

<div id="org17becb8" class="outline-5">
<h5 id="org17becb8"><span class="section-number-5">4.3.1.1</span> 5.4.1. Generators for classification and clustering</h5>
<div class="outline-text-5" id="text-4-3-1-1">
<p>
These generators produce a matrix of features and corresponding discrete targets.
</p>
</div>
</div>

<div id="org7d6ec7b" class="outline-5">
<h5 id="org7d6ec7b"><span class="section-number-5">4.3.1.2</span> 5.4.1.1. Single label</h5>
<div class="outline-text-5" id="text-4-3-1-2">
<p>
Both <code>make_blobs</code> and <code>make_classification</code> create multiclass datasets by
allocating each class one or more <b>normally-distributed</b> clusters of points.
</p>

<p>
<code>make_blobs</code> provides greater control regarding the <b>centers</b> and <b>standard
deviations</b> of each cluster, and is used to demonstrate clustering.
</p>

<p>
<code>make_classification</code> specialises in introducing noise by way of: <b>correlated</b>,
<b>redundant</b> and <b>uninformative</b> features; multiple Gaussian clusters per class;
and linear transformations of the feature space.
</p>

<p>
<code>make_gaussian_quantiles</code> divides a single Gaussian cluster into near-equal-size
classes separated by concentric hyperspheres.
</p>

<p>
<code>make_hastie_10_2</code> generates a similar binary, 10-dimensional problem.
</p>

<p>
../_images/sphx_glr_plot_random_dataset_0011.png
</p>

<p>
<code>make_circles</code> and <code>make_moons</code> generate 2d binary classification datasets that
are challenging to certain algorithms (e.g. centroid-based clustering or linear
classification), including optional Gaussian noise. They are useful for
visualisation. produces Gaussian data with a spherical decision boundary for
binary classification.
</p>
</div>
</div>

<div id="org7ad77dd" class="outline-5">
<h5 id="org7ad77dd"><span class="section-number-5">4.3.1.3</span> 5.4.1.2. Multilabel</h5>
<div class="outline-text-5" id="text-4-3-1-3">
<p>
<code>make_multilabel_classification</code> generates random samples with multiple labels,
reflecting a <b>bag of words</b> drawn from a mixture of topics. The number of topics
for each document is drawn from a <b>Poisson distribution</b>, and the topics
themselves are drawn from a fixed random distribution. Similarly, the number of
words is drawn from Poisson, with words drawn from a multinomial, where each
topic defines a probability distribution over words. Simplifications with
respect to true bag-of-words mixtures include:
</p>

<p>
Per-topic word distributions are independently drawn, where in reality all would
be affected by a sparse base distribution, and would be correlated. For a
document generated from multiple topics, all topics are weighted equally in
generating its bag of words. Documents without labels words at random, rather
than from a base distribution.
</p>

<p>
../_images/sphx_glr_plot_random_multilabel_dataset_0011.png
</p>
</div>
</div>

<div id="org9346bad" class="outline-5">
<h5 id="org9346bad"><span class="section-number-5">4.3.1.4</span> 5.4.1.3. Biclustering</h5>
<div class="outline-text-5" id="text-4-3-1-4">
<p>
make_biclusters(shape, n_clusters[, noise, …])	Generate an array with constant block diagonal structure for biclustering.
make_checkerboard(shape, n_clusters[, …])	Generate an array with block checkerboard structure for biclustering.
</p>
</div>
</div>

<div id="org4e59b51" class="outline-5">
<h5 id="org4e59b51"><span class="section-number-5">4.3.1.5</span> 5.4.2. Generators for regression</h5>
<div class="outline-text-5" id="text-4-3-1-5">
<p>
<code>make_regression</code> produces regression targets as an optionally-sparse random
linear combination of random features, with noise. Its informative features
may be uncorrelated, or low rank (<b>few features account for most of the
variance</b>).
</p>

<p>
Other regression generators generate functions deterministically from
randomized features.
</p>

<p>
<code>make_sparse_uncorrelated</code> produces a target as a linear combination of four
features with fixed coefficients.
</p>

<p>
Others encode explicitly non-linear relations:
</p>

<p>
<code>make_friedman1</code> is related by polynomial and sine transforms;
<code>make_friedman2</code> includes feature multiplication and reciprocation; and
<code>make_friedman3</code> is similar with an arctan transformation on the target.
</p>
</div>
</div>

<div id="org9f7c4f4" class="outline-5">
<h5 id="org9f7c4f4"><span class="section-number-5">4.3.1.6</span> 5.4.3. Generators for manifold learning</h5>
<div class="outline-text-5" id="text-4-3-1-6">
<p>
make_s_curve([n_samples, noise, random_state])	Generate an S curve dataset.
make_swiss_roll([n_samples, noise, random_state])	Generate a swiss roll dataset.
</p>
</div>
</div>
<div id="org340aa9b" class="outline-5">
<h5 id="org340aa9b"><span class="section-number-5">4.3.1.7</span> 5.4.4. Generators for decomposition</h5>
<div class="outline-text-5" id="text-4-3-1-7">
<p>
make_low_rank_matrix([n_samples, …])	Generate a mostly low rank matrix with bell-shaped singular values
make_sparse_coded_signal(n_samples, …[, …])	Generate a signal as a sparse combination of dictionary elements.
make_spd_matrix(n_dim[, random_state])	Generate a random symmetric, positive-definite matrix.
make_sparse_spd_matrix([dim, alpha, …])	Generate a sparse symmetric definite positive matrix.
</p>
</div>
</div>
</div>
</div>

<div id="org3706135" class="outline-3">
<h3 id="org3706135"><span class="section-number-3">4.4</span> Numpy</h3>
<div class="outline-text-3" id="text-4-4">
</div>
<div id="org82c30d7" class="outline-4">
<h4 id="org82c30d7"><span class="section-number-4">4.4.1</span> np.argsort</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
Returns the indices that would sort an array, for multiple dimension array, sort
and return each array('[]' denote an array) itself.
</p>

<p>
Perform an indirect sort along the given axis using the algorithm specified by
the kind keyword. It returns an array of indices of the same shape as a that
index data along the given axis in sorted order.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-variable-name">arr</span> = np.array([[<span class="org-highlight-numbers-number">4</span>,<span class="org-highlight-numbers-number">2</span>,<span class="org-highlight-numbers-number">1</span>,<span class="org-highlight-numbers-number">0</span>],[<span class="org-highlight-numbers-number">5</span>,<span class="org-highlight-numbers-number">9</span>,<span class="org-highlight-numbers-number">8</span>,<span class="org-highlight-numbers-number">7</span>]])
<span class="org-variable-name">argarr</span> = np.argsort(arr)
argarr
</pre>
</div>

<pre class="example">
array([[3, 2, 1, 0],
[0, 3, 2, 1]])
</pre>
</div>
</div>
</div>

<div id="org36d3428" class="outline-3">
<h3 id="org36d3428"><span class="section-number-3">4.5</span> Matplotlib</h3>
<div class="outline-text-3" id="text-4-5">
</div>
<div id="org2559784" class="outline-4">
<h4 id="org2559784"><span class="section-number-4">4.5.1</span> how to give a gradually changed color</h4>
<div class="outline-text-4" id="text-4-5-1">
<div class="org-src-container">
<pre class="src src-ipython">plt.figure(figsize=(<span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">5</span>))
plt.plot(true_coefficient[coefficient_sorting], <span class="org-string">"o"</span>, label=<span class="org-string">"true"</span>, c=<span class="org-string">'b'</span>)
<span class="org-keyword">for</span> i, alpha <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>([<span class="org-highlight-numbers-number">100</span>, <span class="org-highlight-numbers-number">10</span>, <span class="org-highlight-numbers-number">1</span>, .<span class="org-highlight-numbers-number">01</span>]):
    plt.plot(ridge_models[alpha].coef_[coefficient_sorting],
             <span class="org-string">"o"</span>,
             label=<span class="org-string">"alpha = %.2f"</span> % alpha,
             c=plt.cm.summer(i / <span class="org-highlight-numbers-number">3</span>.) <span class="org-comment-delimiter">#</span><span class="org-comment">&lt;- how to give a gradually changed color</span>
    )
plt.legend(loc=<span class="org-string">"best"</span>)
</pre>
</div>
</div>
</div>
<div id="org6d3330e" class="outline-4">
<h4 id="org6d3330e"><span class="section-number-4">4.5.2</span> what are tickets</h4>
<div class="outline-text-4" id="text-4-5-2">
<p>
specify the indexs of axes
</p>
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots()
ax.set_xticks([<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">15</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">68</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">97</span>])
ax.set_yticks([<span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">2</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">55</span>, <span class="org-highlight-numbers-number">0</span>.<span class="org-highlight-numbers-number">76</span>])
plt.show()
</pre>
</div>


<div class="figure">
<p><img src="./obipy-resources/31993Qu.png" alt="31993Qu.png" />
</p>
</div>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
